I0326 16:33:20.060289      21 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-194310689
I0326 16:33:20.063950      21 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0326 16:33:20.064235      21 e2e.go:124] Starting e2e run "33c5d069-26ee-4ed9-8532-3bb0bd124aa3" on Ginkgo node 1
{"msg":"Test Suite starting","total":275,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1585240396 - Will randomize all specs
Will run 275 of 4992 specs

Mar 26 16:33:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:33:20.157: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0326 16:33:20.173356      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
Mar 26 16:33:20.175: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 26 16:33:20.209: INFO: 5 / 5 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 26 16:33:20.209: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 26 16:33:20.209: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 26 16:33:20.217: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Mar 26 16:33:20.217: INFO: e2e test version: v1.18.0
Mar 26 16:33:20.218: INFO: kube-apiserver version: v1.18.0
Mar 26 16:33:20.218: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:33:20.222: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:33:20.231: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
Mar 26 16:33:20.395: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar 26 16:33:20.440: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-a285c99c-5429-415f-99e2-72cb057b15d8
STEP: Creating a pod to test consume secrets
Mar 26 16:33:20.732: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd" in namespace "projected-5118" to be "Succeeded or Failed"
Mar 26 16:33:20.746: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.473871ms
Mar 26 16:33:22.759: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026198848s
Mar 26 16:33:24.767: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034722289s
Mar 26 16:33:26.777: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045171241s
Mar 26 16:33:28.782: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050000145s
Mar 26 16:33:30.793: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060600267s
Mar 26 16:33:32.801: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.068461384s
Mar 26 16:33:34.807: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.074644083s
Mar 26 16:33:36.841: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.108336176s
STEP: Saw pod success
Mar 26 16:33:36.841: INFO: Pod "pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd" satisfied condition "Succeeded or Failed"
Mar 26 16:33:36.843: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 16:33:36.964: INFO: Waiting for pod pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd to disappear
Mar 26 16:33:36.971: INFO: Pod pod-projected-secrets-409bf851-7dae-4d75-a51d-5023f1f878bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:33:36.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5118" for this suite.

â€¢ [SLOW TEST:16.749 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":1,"skipped":24,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:33:36.981: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename containers
E0326 16:33:36.987027      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Mar 26 16:33:37.188: INFO: Waiting up to 5m0s for pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3" in namespace "containers-7157" to be "Succeeded or Failed"
Mar 26 16:33:37.201: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.034454ms
Mar 26 16:33:39.207: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018799119s
Mar 26 16:33:41.217: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029358536s
Mar 26 16:33:43.220: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032272634s
Mar 26 16:33:45.226: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03827603s
Mar 26 16:33:47.230: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042191803s
Mar 26 16:33:49.235: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047546795s
Mar 26 16:33:51.238: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.050467385s
STEP: Saw pod success
Mar 26 16:33:51.239: INFO: Pod "client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3" satisfied condition "Succeeded or Failed"
Mar 26 16:33:51.240: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3 container test-container: <nil>
STEP: delete the pod
Mar 26 16:33:51.270: INFO: Waiting for pod client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3 to disappear
Mar 26 16:33:51.272: INFO: Pod client-containers-1a32a69b-b4e8-42af-a21a-f0b1398395c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:33:51.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7157" for this suite.

â€¢ [SLOW TEST:14.301 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":275,"completed":2,"skipped":28,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:33:51.285: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
E0326 16:33:51.378751      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 26 16:33:51.554: INFO: namespace kubectl-1235
Mar 26 16:33:51.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-1235'
Mar 26 16:33:53.157: INFO: stderr: ""
Mar 26 16:33:53.157: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 16:33:54.186: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:33:54.186: INFO: Found 0 / 1
Mar 26 16:33:55.161: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:33:55.161: INFO: Found 1 / 1
Mar 26 16:33:55.161: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 16:33:55.163: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:33:55.163: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 16:33:55.163: INFO: wait on agnhost-master startup in kubectl-1235 
Mar 26 16:33:55.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs agnhost-master-s98n8 agnhost-master --namespace=kubectl-1235'
Mar 26 16:33:55.347: INFO: stderr: ""
Mar 26 16:33:55.347: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 26 16:33:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1235'
Mar 26 16:33:55.605: INFO: stderr: ""
Mar 26 16:33:55.605: INFO: stdout: "service/rm2 exposed\n"
Mar 26 16:33:55.644: INFO: Service rm2 in namespace kubectl-1235 found.
STEP: exposing service
Mar 26 16:33:57.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1235'
Mar 26 16:33:57.833: INFO: stderr: ""
Mar 26 16:33:57.833: INFO: stdout: "service/rm3 exposed\n"
Mar 26 16:33:57.845: INFO: Service rm3 in namespace kubectl-1235 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:33:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1235" for this suite.

â€¢ [SLOW TEST:8.576 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":275,"completed":3,"skipped":35,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:33:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2524
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-c808a921-8835-47c9-b94b-f91a1c8983eb
STEP: Creating configMap with name cm-test-opt-upd-33df2419-bd54-439b-b8ad-0ee6c0027c43
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c808a921-8835-47c9-b94b-f91a1c8983eb
STEP: Updating configmap cm-test-opt-upd-33df2419-bd54-439b-b8ad-0ee6c0027c43
STEP: Creating configMap with name cm-test-opt-create-4706e236-8f24-43b2-9b5a-d4d46dcf9a45
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:35:32.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2524" for this suite.

â€¢ [SLOW TEST:93.098 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":4,"skipped":62,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:35:32.965: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 16:35:33.139: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:35:43.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6183" for this suite.

â€¢ [SLOW TEST:10.104 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":275,"completed":5,"skipped":78,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:35:43.069: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:35:45.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-483" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":275,"completed":6,"skipped":79,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:35:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 16:35:45.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c" in namespace "downward-api-7151" to be "Succeeded or Failed"
Mar 26 16:35:45.424: INFO: Pod "downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.344467ms
Mar 26 16:35:47.428: INFO: Pod "downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015309046s
STEP: Saw pod success
Mar 26 16:35:47.428: INFO: Pod "downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c" satisfied condition "Succeeded or Failed"
Mar 26 16:35:47.432: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c container client-container: <nil>
STEP: delete the pod
Mar 26 16:35:47.452: INFO: Waiting for pod downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c to disappear
Mar 26 16:35:47.454: INFO: Pod downwardapi-volume-4e3db880-4c79-4c73-b489-a13aa1136f4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:35:47.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7151" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":7,"skipped":106,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:35:47.460: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:35:59.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4183" for this suite.

â€¢ [SLOW TEST:12.205 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a busybox command in a pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":275,"completed":8,"skipped":107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:35:59.670: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 26 16:36:14.971: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:16.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7347" for this suite.

â€¢ [SLOW TEST:16.372 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":275,"completed":9,"skipped":150,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:16.044: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 16:36:16.213: INFO: Waiting up to 5m0s for pod "pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d" in namespace "emptydir-4517" to be "Succeeded or Failed"
Mar 26 16:36:16.221: INFO: Pod "pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.617094ms
Mar 26 16:36:18.228: INFO: Pod "pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015128624s
Mar 26 16:36:20.272: INFO: Pod "pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058213543s
STEP: Saw pod success
Mar 26 16:36:20.272: INFO: Pod "pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d" satisfied condition "Succeeded or Failed"
Mar 26 16:36:20.274: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d container test-container: <nil>
STEP: delete the pod
Mar 26 16:36:20.326: INFO: Waiting for pod pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d to disappear
Mar 26 16:36:20.331: INFO: Pod pod-edb6d6d7-be9e-4f87-a7e6-38a5bcc5398d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:20.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4517" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":10,"skipped":168,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:20.340: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-f06b03ee-c69a-4ca4-82ec-dacc5f13297e
STEP: Creating a pod to test consume configMaps
Mar 26 16:36:20.528: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3" in namespace "projected-9710" to be "Succeeded or Failed"
Mar 26 16:36:20.538: INFO: Pod "pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.269266ms
Mar 26 16:36:22.570: INFO: Pod "pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042169272s
Mar 26 16:36:24.574: INFO: Pod "pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046174389s
STEP: Saw pod success
Mar 26 16:36:24.574: INFO: Pod "pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3" satisfied condition "Succeeded or Failed"
Mar 26 16:36:24.577: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 16:36:24.619: INFO: Waiting for pod pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3 to disappear
Mar 26 16:36:24.625: INFO: Pod pod-projected-configmaps-d4ddf984-4ac7-4df3-933e-106f0303ede3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:24.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9710" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":11,"skipped":172,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:24.638: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:37.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8311" for this suite.

â€¢ [SLOW TEST:13.354 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":275,"completed":12,"skipped":180,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 26 16:36:38.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-8597'
Mar 26 16:36:38.877: INFO: stderr: ""
Mar 26 16:36:38.877: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 16:36:39.890: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:36:39.890: INFO: Found 0 / 1
Mar 26 16:36:40.889: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:36:40.889: INFO: Found 1 / 1
Mar 26 16:36:40.889: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 26 16:36:40.893: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:36:40.893: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 16:36:40.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 patch pod agnhost-master-k9d8t --namespace=kubectl-8597 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 26 16:36:41.019: INFO: stderr: ""
Mar 26 16:36:41.019: INFO: stdout: "pod/agnhost-master-k9d8t patched\n"
STEP: checking annotations
Mar 26 16:36:41.021: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:36:41.022: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:41.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8597" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":275,"completed":13,"skipped":187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:41.030: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2806.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2806.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2806.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2806.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2806.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2806.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 16:36:57.323: INFO: DNS probes using dns-2806/dns-test-5d871c3d-0211-4743-9023-93145f91b83d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:36:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2806" for this suite.

â€¢ [SLOW TEST:16.350 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":275,"completed":14,"skipped":211,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:36:57.383: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Mar 26 16:37:01.674: INFO: Pod pod-hostip-831daf22-c53b-46b1-805d-02f0efa7c01b has hostIP: 10.107.34.127
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:01.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2728" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":275,"completed":15,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:01.693: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 16:37:01.875: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90" in namespace "downward-api-3118" to be "Succeeded or Failed"
Mar 26 16:37:01.887: INFO: Pod "downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90": Phase="Pending", Reason="", readiness=false. Elapsed: 11.508983ms
Mar 26 16:37:03.891: INFO: Pod "downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01597996s
STEP: Saw pod success
Mar 26 16:37:03.891: INFO: Pod "downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90" satisfied condition "Succeeded or Failed"
Mar 26 16:37:03.895: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90 container client-container: <nil>
STEP: delete the pod
Mar 26 16:37:03.916: INFO: Waiting for pod downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90 to disappear
Mar 26 16:37:03.926: INFO: Pod downwardapi-volume-eca94e15-7540-43ea-9e42-c578ce428a90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:03.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3118" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":16,"skipped":238,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:03.938: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 26 16:37:18.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 16:37:18.214: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 16:37:20.215: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 16:37:20.224: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 16:37:22.218: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 16:37:22.222: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:22.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-400" for this suite.

â€¢ [SLOW TEST:18.292 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":275,"completed":17,"skipped":239,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:22.231: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 26 16:37:22.380: INFO: Created pod &Pod{ObjectMeta:{dns-7792  dns-7792 /api/v1/namespaces/dns-7792/pods/dns-7792 f0b8efa9-ecba-485a-b899-23195b1f37c0 2056 0 2020-03-26 16:37:22 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-03-26 16:37:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rgczf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rgczf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rgczf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:37:22.423: INFO: The status of Pod dns-7792 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 16:37:24.429: INFO: The status of Pod dns-7792 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 26 16:37:24.430: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7792 PodName:dns-7792 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:37:24.430: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Verifying customized DNS server is configured on pod...
Mar 26 16:37:24.660: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7792 PodName:dns-7792 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:37:24.660: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:37:24.762: INFO: Deleting pod dns-7792...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7792" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":275,"completed":18,"skipped":250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 16:37:25.937: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 16:37:27.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837445, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837445, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837445, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837445, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 16:37:31.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
Mar 26 16:37:31.047: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:41.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-340" for this suite.
STEP: Destroying namespace "webhook-340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:16.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":275,"completed":19,"skipped":272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:41.495: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-9ee4c2d8-1202-44df-9d98-ee8423d94564-6113
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3367" for this suite.
STEP: Destroying namespace "nspatchtest-9ee4c2d8-1202-44df-9d98-ee8423d94564-6113" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":275,"completed":20,"skipped":313,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:41.831: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 16:37:42.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 16:37:45.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837463, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837463, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837463, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837462, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 16:37:48.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 26 16:37:48.087: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:48.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7296" for this suite.
STEP: Destroying namespace "webhook-7296-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.498 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":275,"completed":21,"skipped":323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:48.332: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2355" for this suite.

â€¢ [SLOW TEST:8.333 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":275,"completed":22,"skipped":383,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:56.666: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:37:56.848: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d3eceb60-4fb3-4e3e-8d46-d5b3e93ecb78" in namespace "security-context-test-4613" to be "Succeeded or Failed"
Mar 26 16:37:56.877: INFO: Pod "busybox-readonly-false-d3eceb60-4fb3-4e3e-8d46-d5b3e93ecb78": Phase="Pending", Reason="", readiness=false. Elapsed: 29.27381ms
Mar 26 16:37:58.882: INFO: Pod "busybox-readonly-false-d3eceb60-4fb3-4e3e-8d46-d5b3e93ecb78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033928859s
Mar 26 16:37:58.882: INFO: Pod "busybox-readonly-false-d3eceb60-4fb3-4e3e-8d46-d5b3e93ecb78" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:37:58.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4613" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":275,"completed":23,"skipped":396,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:37:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 26 16:37:59.078: INFO: Waiting up to 5m0s for pod "pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a" in namespace "emptydir-3231" to be "Succeeded or Failed"
Mar 26 16:37:59.095: INFO: Pod "pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.812825ms
Mar 26 16:38:01.101: INFO: Pod "pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023011345s
STEP: Saw pod success
Mar 26 16:38:01.102: INFO: Pod "pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a" satisfied condition "Succeeded or Failed"
Mar 26 16:38:01.105: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a container test-container: <nil>
STEP: delete the pod
Mar 26 16:38:01.137: INFO: Waiting for pod pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a to disappear
Mar 26 16:38:01.143: INFO: Pod pod-10061675-96ca-4b73-887a-3cd2cdf8cd1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:38:01.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3231" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":24,"skipped":416,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:38:01.153: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 26 16:38:01.334: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8394 /api/v1/namespaces/watch-8394/configmaps/e2e-watch-test-watch-closed 1cd916d5-a72d-443e-a81d-498608f49d9b 2466 0 2020-03-26 16:38:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 16:38:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 16:38:01.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8394 /api/v1/namespaces/watch-8394/configmaps/e2e-watch-test-watch-closed 1cd916d5-a72d-443e-a81d-498608f49d9b 2467 0 2020-03-26 16:38:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 16:38:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 26 16:38:01.356: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8394 /api/v1/namespaces/watch-8394/configmaps/e2e-watch-test-watch-closed 1cd916d5-a72d-443e-a81d-498608f49d9b 2468 0 2020-03-26 16:38:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 16:38:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 16:38:01.356: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8394 /api/v1/namespaces/watch-8394/configmaps/e2e-watch-test-watch-closed 1cd916d5-a72d-443e-a81d-498608f49d9b 2469 0 2020-03-26 16:38:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 16:38:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:38:01.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8394" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":275,"completed":25,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:38:01.368: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:38:01.538: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 26 16:38:06.547: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 16:38:06.565: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 26 16:38:08.571: INFO: Creating deployment "test-rollover-deployment"
Mar 26 16:38:08.593: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 26 16:38:10.608: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 26 16:38:10.617: INFO: Ensure that both replica sets have 1 created replica
Mar 26 16:38:10.622: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 26 16:38:10.645: INFO: Updating deployment test-rollover-deployment
Mar 26 16:38:10.645: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 26 16:38:12.655: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 26 16:38:12.666: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 26 16:38:12.672: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 16:38:12.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837492, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 16:38:14.681: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 16:38:14.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837492, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 16:38:16.683: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 16:38:16.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837492, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 16:38:18.685: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 16:38:18.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837492, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 16:38:20.683: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 16:38:20.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837492, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837488, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 16:38:22.690: INFO: 
Mar 26 16:38:22.690: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 16:38:22.733: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5086 /apis/apps/v1/namespaces/deployment-5086/deployments/test-rollover-deployment 604f9705-f765-41ff-88e4-4054dc492192 2663 2 2020-03-26 16:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 16:38:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 16:38:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00069c158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-26 16:38:08 +0000 UTC,LastTransitionTime:2020-03-26 16:38:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-03-26 16:38:22 +0000 UTC,LastTransitionTime:2020-03-26 16:38:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 16:38:22.737: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-5086 /apis/apps/v1/namespaces/deployment-5086/replicasets/test-rollover-deployment-84f7f6f64b 551c890f-07db-4224-9cb3-16b8862268a5 2652 2 2020-03-26 16:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 604f9705-f765-41ff-88e4-4054dc492192 0xc00069cd07 0xc00069cd08}] []  [{kube-controller-manager Update apps/v1 2020-03-26 16:38:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 48 52 102 57 55 48 53 45 102 55 54 53 45 52 49 102 102 45 56 56 101 52 45 52 48 53 52 100 99 52 57 50 49 57 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00069cec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:38:22.738: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 26 16:38:22.738: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5086 /apis/apps/v1/namespaces/deployment-5086/replicasets/test-rollover-controller 53d8fd5c-1dac-46a8-b1f4-4616e7d9e2f8 2661 2 2020-03-26 16:38:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 604f9705-f765-41ff-88e4-4054dc492192 0xc00069ca7f 0xc00069ca90}] []  [{e2e.test Update apps/v1 2020-03-26 16:38:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 16:38:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 48 52 102 57 55 48 53 45 102 55 54 53 45 52 49 102 102 45 56 56 101 52 45 52 48 53 52 100 99 52 57 50 49 57 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00069cb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:38:22.739: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-5086 /apis/apps/v1/namespaces/deployment-5086/replicasets/test-rollover-deployment-5686c4cfd5 94733d02-79bb-4933-b8cd-948bdc725a95 2614 2 2020-03-26 16:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 604f9705-f765-41ff-88e4-4054dc492192 0xc00069cbb7 0xc00069cbb8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 16:38:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 48 52 102 57 55 48 53 45 102 55 54 53 45 52 49 102 102 45 56 56 101 52 45 52 48 53 52 100 99 52 57 50 49 57 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00069cc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:38:22.743: INFO: Pod "test-rollover-deployment-84f7f6f64b-477g8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-477g8 test-rollover-deployment-84f7f6f64b- deployment-5086 /api/v1/namespaces/deployment-5086/pods/test-rollover-deployment-84f7f6f64b-477g8 a566ac3d-ae8a-4392-af9a-3224f756c55d 2625 0 2020-03-26 16:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 551c890f-07db-4224-9cb3-16b8862268a5 0xc000fea0a7 0xc000fea0a8}] []  [{kube-controller-manager Update v1 2020-03-26 16:38:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 53 49 99 56 57 48 102 45 48 55 100 98 45 52 50 50 52 45 57 99 98 51 45 49 54 98 56 56 54 50 50 54 56 97 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:38:12 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 50 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8hlkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8hlkk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8hlkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:38:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.29,StartTime:2020-03-26 16:38:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:cri-o://8ab8ef84437f78a116b8745c9706c620887df6503d42f4f18cec502f4b0ee7cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:38:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5086" for this suite.

â€¢ [SLOW TEST:21.386 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":275,"completed":26,"skipped":454,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:38:22.755: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 26 16:39:02.990: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:02.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4242" for this suite.
W0326 16:39:02.988642      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.

â€¢ [SLOW TEST:40.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":275,"completed":27,"skipped":460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 26 16:39:03.222: INFO: Waiting up to 5m0s for pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef" in namespace "emptydir-4566" to be "Succeeded or Failed"
Mar 26 16:39:03.224: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440033ms
Mar 26 16:39:05.232: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010874672s
Mar 26 16:39:07.237: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015340872s
Mar 26 16:39:09.240: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018222264s
Mar 26 16:39:11.246: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024621443s
Mar 26 16:39:13.251: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029777686s
STEP: Saw pod success
Mar 26 16:39:13.251: INFO: Pod "pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef" satisfied condition "Succeeded or Failed"
Mar 26 16:39:13.257: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef container test-container: <nil>
STEP: delete the pod
Mar 26 16:39:13.287: INFO: Waiting for pod pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef to disappear
Mar 26 16:39:13.290: INFO: Pod pod-3f9b5197-f1c7-42ce-8f89-a2a894eb96ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:13.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4566" for this suite.

â€¢ [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":28,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:13.301: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 16:39:13.454: INFO: Waiting up to 5m0s for pod "pod-d4d8c678-26e4-4043-97fb-b200c201bb88" in namespace "emptydir-4808" to be "Succeeded or Failed"
Mar 26 16:39:13.458: INFO: Pod "pod-d4d8c678-26e4-4043-97fb-b200c201bb88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475987ms
Mar 26 16:39:15.466: INFO: Pod "pod-d4d8c678-26e4-4043-97fb-b200c201bb88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01132459s
Mar 26 16:39:17.471: INFO: Pod "pod-d4d8c678-26e4-4043-97fb-b200c201bb88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016976202s
STEP: Saw pod success
Mar 26 16:39:17.472: INFO: Pod "pod-d4d8c678-26e4-4043-97fb-b200c201bb88" satisfied condition "Succeeded or Failed"
Mar 26 16:39:17.482: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-d4d8c678-26e4-4043-97fb-b200c201bb88 container test-container: <nil>
STEP: delete the pod
Mar 26 16:39:17.508: INFO: Waiting for pod pod-d4d8c678-26e4-4043-97fb-b200c201bb88 to disappear
Mar 26 16:39:17.512: INFO: Pod pod-d4d8c678-26e4-4043-97fb-b200c201bb88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:17.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4808" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":29,"skipped":529,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 16:39:20.246: INFO: Successfully updated pod "annotationupdatea67d1a09-26ec-4944-971c-7dbcfb8515e5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:24.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3920" for this suite.

â€¢ [SLOW TEST:6.767 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":30,"skipped":543,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:24.291: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 16:39:24.967: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 16:39:26.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837565, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837565, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837565, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720837564, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 16:39:30.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:30.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1506" for this suite.
STEP: Destroying namespace "webhook-1506-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":275,"completed":31,"skipped":544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:30.297: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 16:39:32.643: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:32.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9391" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":32,"skipped":574,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:32.683: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 16:39:32.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 16:39:32.855: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 16:39:32.857: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-dqhapg before test
Mar 26 16:39:32.868: INFO: annotationupdatea67d1a09-26ec-4944-971c-7dbcfb8515e5 from projected-3920 started at 2020-03-26 16:39:17 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.868: INFO: 	Container client-container ready: false, restart count 0
Mar 26 16:39:32.868: INFO: sonobuoy from sonobuoy started at 2020-03-26 16:32:42 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 26 16:39:32.868: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 16:39:32.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 16:39:32.868: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 16:39:32.868: INFO: kube-flannel-ds-amd64-vblvg from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.868: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 16:39:32.868: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-vf6bys before test
Mar 26 16:39:32.904: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 16:39:32.904: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 16:39:32.904: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 16:39:32.904: INFO: kube-flannel-ds-amd64-4db44 from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.904: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 16:39:32.904: INFO: coredns-6ccf845bfb-jklrr from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.904: INFO: 	Container coredns ready: true, restart count 0
Mar 26 16:39:32.904: INFO: coredns-6ccf845bfb-c9f7x from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 16:39:32.904: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d9b4e722-351f-41d1-8645-f708170dd9d6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d9b4e722-351f-41d1-8645-f708170dd9d6 off the node kubedee-test-worker-dqhapg
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d9b4e722-351f-41d1-8645-f708170dd9d6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:39:39.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2842" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:6.396 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":275,"completed":33,"skipped":595,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:39:39.080: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8287
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 26 16:39:39.349: INFO: Found 0 stateful pods, waiting for 3
Mar 26 16:39:49.364: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:39:49.365: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:39:49.365: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:39:49.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-8287 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:39:49.726: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:39:49.726: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:39:49.726: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 26 16:39:59.781: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 26 16:40:09.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-8287 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:40:10.089: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:40:10.089: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:40:10.089: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:40:20.134: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:40:20.134: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:20.134: INFO: Waiting for Pod statefulset-8287/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:30.145: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:40:30.145: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:30.145: INFO: Waiting for Pod statefulset-8287/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:40.148: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:40:40.148: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:40.148: INFO: Waiting for Pod statefulset-8287/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:40:50.145: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:40:50.145: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:41:00.148: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:41:00.148: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:41:10.146: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 26 16:41:20.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-8287 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:41:20.381: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:41:20.381: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:41:20.381: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 16:41:30.426: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 26 16:41:40.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-8287 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:41:40.632: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:41:40.632: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:41:40.632: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:41:50.652: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:41:50.652: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 26 16:41:50.652: INFO: Waiting for Pod statefulset-8287/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 26 16:42:00.668: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:42:00.669: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 26 16:42:00.669: INFO: Waiting for Pod statefulset-8287/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 26 16:42:10.664: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
Mar 26 16:42:10.664: INFO: Waiting for Pod statefulset-8287/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 26 16:42:20.664: INFO: Waiting for StatefulSet statefulset-8287/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 16:42:30.678: INFO: Deleting all statefulset in ns statefulset-8287
Mar 26 16:42:30.687: INFO: Scaling statefulset ss2 to 0
Mar 26 16:42:50.708: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 16:42:50.712: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:42:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8287" for this suite.

â€¢ [SLOW TEST:191.702 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":275,"completed":34,"skipped":603,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:42:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:42:51.036: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2102
I0326 16:42:51.176967      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2102, replica count: 1
I0326 16:42:52.315268      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 16:42:53.315621      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 16:42:53.448: INFO: Created: latency-svc-ljmnw
Mar 26 16:42:53.487: INFO: Got endpoints: latency-svc-ljmnw [71.985374ms]
Mar 26 16:42:53.529: INFO: Created: latency-svc-j9dvq
Mar 26 16:42:53.571: INFO: Got endpoints: latency-svc-j9dvq [82.507368ms]
Mar 26 16:42:53.590: INFO: Created: latency-svc-95smr
Mar 26 16:42:53.653: INFO: Got endpoints: latency-svc-95smr [163.372085ms]
Mar 26 16:42:53.660: INFO: Created: latency-svc-r5g2l
Mar 26 16:42:53.678: INFO: Got endpoints: latency-svc-r5g2l [190.109025ms]
Mar 26 16:42:53.691: INFO: Created: latency-svc-l6j2x
Mar 26 16:42:53.717: INFO: Created: latency-svc-6pj2f
Mar 26 16:42:53.740: INFO: Got endpoints: latency-svc-l6j2x [251.328981ms]
Mar 26 16:42:53.795: INFO: Created: latency-svc-xvbs5
Mar 26 16:42:53.795: INFO: Got endpoints: latency-svc-6pj2f [306.039381ms]
Mar 26 16:42:53.844: INFO: Created: latency-svc-rvx89
Mar 26 16:42:53.863: INFO: Got endpoints: latency-svc-xvbs5 [373.931286ms]
Mar 26 16:42:53.880: INFO: Created: latency-svc-zvmbr
Mar 26 16:42:53.895: INFO: Got endpoints: latency-svc-rvx89 [405.59644ms]
Mar 26 16:42:53.939: INFO: Got endpoints: latency-svc-zvmbr [450.87958ms]
Mar 26 16:42:53.977: INFO: Created: latency-svc-fvbpf
Mar 26 16:42:54.099: INFO: Created: latency-svc-dbclx
Mar 26 16:42:54.099: INFO: Got endpoints: latency-svc-dbclx [609.731777ms]
Mar 26 16:42:54.099: INFO: Got endpoints: latency-svc-fvbpf [609.7353ms]
Mar 26 16:42:54.157: INFO: Created: latency-svc-227wm
Mar 26 16:42:54.157: INFO: Created: latency-svc-6q9ll
Mar 26 16:42:54.199: INFO: Got endpoints: latency-svc-227wm [710.623971ms]
Mar 26 16:42:54.203: INFO: Got endpoints: latency-svc-6q9ll [714.428288ms]
Mar 26 16:42:54.229: INFO: Created: latency-svc-jbqtl
Mar 26 16:42:54.339: INFO: Created: latency-svc-dcznm
Mar 26 16:42:54.344: INFO: Created: latency-svc-ddsdz
Mar 26 16:42:54.345: INFO: Got endpoints: latency-svc-ddsdz [855.713199ms]
Mar 26 16:42:54.346: INFO: Got endpoints: latency-svc-dcznm [857.080427ms]
Mar 26 16:42:54.345: INFO: Got endpoints: latency-svc-jbqtl [855.763932ms]
Mar 26 16:42:54.368: INFO: Created: latency-svc-7psgn
Mar 26 16:42:54.389: INFO: Created: latency-svc-n78cb
Mar 26 16:42:54.412: INFO: Got endpoints: latency-svc-7psgn [841.78932ms]
Mar 26 16:42:54.451: INFO: Got endpoints: latency-svc-n78cb [798.000179ms]
Mar 26 16:42:54.474: INFO: Created: latency-svc-zvwrs
Mar 26 16:42:54.502: INFO: Created: latency-svc-5wb6p
Mar 26 16:42:54.504: INFO: Got endpoints: latency-svc-zvwrs [826.010849ms]
Mar 26 16:42:54.560: INFO: Created: latency-svc-m7wwc
Mar 26 16:42:54.601: INFO: Got endpoints: latency-svc-m7wwc [806.069904ms]
Mar 26 16:42:54.602: INFO: Got endpoints: latency-svc-5wb6p [861.147832ms]
Mar 26 16:42:54.656: INFO: Created: latency-svc-dxmdp
Mar 26 16:42:54.657: INFO: Created: latency-svc-th8kt
Mar 26 16:42:54.699: INFO: Got endpoints: latency-svc-dxmdp [835.784147ms]
Mar 26 16:42:54.701: INFO: Created: latency-svc-n4brh
Mar 26 16:42:54.722: INFO: Got endpoints: latency-svc-th8kt [827.049095ms]
Mar 26 16:42:54.723: INFO: Got endpoints: latency-svc-n4brh [779.693774ms]
Mar 26 16:42:54.762: INFO: Created: latency-svc-4x7pc
Mar 26 16:42:54.770: INFO: Created: latency-svc-7xqdh
Mar 26 16:42:54.800: INFO: Created: latency-svc-gwk42
Mar 26 16:42:54.829: INFO: Created: latency-svc-vcr2r
Mar 26 16:42:54.831: INFO: Created: latency-svc-4lpzd
Mar 26 16:42:54.867: INFO: Created: latency-svc-jtr9t
Mar 26 16:42:54.873: INFO: Got endpoints: latency-svc-7xqdh [773.699828ms]
Mar 26 16:42:54.887: INFO: Got endpoints: latency-svc-4x7pc [164.826426ms]
Mar 26 16:42:54.939: INFO: Got endpoints: latency-svc-gwk42 [216.634227ms]
Mar 26 16:42:54.940: INFO: Got endpoints: latency-svc-4lpzd [738.682593ms]
Mar 26 16:42:54.942: INFO: Got endpoints: latency-svc-vcr2r [738.927668ms]
Mar 26 16:42:55.044: INFO: Created: latency-svc-sh5kh
Mar 26 16:42:55.045: INFO: Created: latency-svc-7nlkc
Mar 26 16:42:55.051: INFO: Created: latency-svc-8dkm4
Mar 26 16:42:55.069: INFO: Created: latency-svc-cx922
Mar 26 16:42:55.085: INFO: Created: latency-svc-2ktz5
Mar 26 16:42:55.090: INFO: Got endpoints: latency-svc-8dkm4 [744.224828ms]
Mar 26 16:42:55.090: INFO: Got endpoints: latency-svc-jtr9t [744.620635ms]
Mar 26 16:42:55.092: INFO: Got endpoints: latency-svc-7nlkc [746.106047ms]
Mar 26 16:42:55.101: INFO: Got endpoints: latency-svc-sh5kh [228.047942ms]
Mar 26 16:42:55.119: INFO: Got endpoints: latency-svc-cx922 [706.075094ms]
Mar 26 16:42:55.187: INFO: Created: latency-svc-7xsbt
Mar 26 16:42:55.215: INFO: Got endpoints: latency-svc-7xsbt [710.806271ms]
Mar 26 16:42:55.215: INFO: Got endpoints: latency-svc-2ktz5 [764.649831ms]
Mar 26 16:42:55.305: INFO: Created: latency-svc-rd7wr
Mar 26 16:42:55.348: INFO: Created: latency-svc-29dkw
Mar 26 16:42:55.351: INFO: Got endpoints: latency-svc-rd7wr [748.798995ms]
Mar 26 16:42:55.384: INFO: Created: latency-svc-hrqls
Mar 26 16:42:55.403: INFO: Got endpoints: latency-svc-hrqls [703.928932ms]
Mar 26 16:42:55.403: INFO: Got endpoints: latency-svc-29dkw [802.054881ms]
Mar 26 16:42:55.450: INFO: Created: latency-svc-7nk8d
Mar 26 16:42:55.450: INFO: Got endpoints: latency-svc-7nk8d [1.351010339s]
Mar 26 16:42:55.485: INFO: Created: latency-svc-gdqx7
Mar 26 16:42:55.543: INFO: Got endpoints: latency-svc-gdqx7 [656.083709ms]
Mar 26 16:42:55.556: INFO: Created: latency-svc-9mlcc
Mar 26 16:42:55.603: INFO: Created: latency-svc-7qw82
Mar 26 16:42:55.683: INFO: Got endpoints: latency-svc-9mlcc [742.949ms]
Mar 26 16:42:55.683: INFO: Created: latency-svc-frwdx
Mar 26 16:42:55.696: INFO: Got endpoints: latency-svc-7qw82 [756.282623ms]
Mar 26 16:42:55.709: INFO: Got endpoints: latency-svc-frwdx [766.8409ms]
Mar 26 16:42:55.741: INFO: Created: latency-svc-hrms4
Mar 26 16:42:55.753: INFO: Created: latency-svc-fq8wp
Mar 26 16:42:55.767: INFO: Created: latency-svc-zpj79
Mar 26 16:42:55.802: INFO: Created: latency-svc-ck5wn
Mar 26 16:42:55.840: INFO: Got endpoints: latency-svc-hrms4 [749.67365ms]
Mar 26 16:42:55.841: INFO: Got endpoints: latency-svc-zpj79 [748.167079ms]
Mar 26 16:42:55.841: INFO: Got endpoints: latency-svc-fq8wp [750.355956ms]
Mar 26 16:42:55.866: INFO: Created: latency-svc-65pwn
Mar 26 16:42:55.887: INFO: Got endpoints: latency-svc-ck5wn [785.763669ms]
Mar 26 16:42:55.895: INFO: Created: latency-svc-llp62
Mar 26 16:42:55.912: INFO: Got endpoints: latency-svc-65pwn [789.014075ms]
Mar 26 16:42:55.931: INFO: Created: latency-svc-xtcqb
Mar 26 16:42:55.964: INFO: Got endpoints: latency-svc-llp62 [748.82934ms]
Mar 26 16:42:55.983: INFO: Got endpoints: latency-svc-xtcqb [767.983892ms]
Mar 26 16:42:55.994: INFO: Created: latency-svc-crsh8
Mar 26 16:42:56.010: INFO: Created: latency-svc-jcl4l
Mar 26 16:42:56.065: INFO: Got endpoints: latency-svc-crsh8 [714.3531ms]
Mar 26 16:42:56.140: INFO: Created: latency-svc-clln5
Mar 26 16:42:56.154: INFO: Got endpoints: latency-svc-jcl4l [751.14331ms]
Mar 26 16:42:56.186: INFO: Got endpoints: latency-svc-clln5 [782.703548ms]
Mar 26 16:42:56.227: INFO: Created: latency-svc-jvb7m
Mar 26 16:42:56.252: INFO: Created: latency-svc-mrkkb
Mar 26 16:42:56.269: INFO: Created: latency-svc-7kzd7
Mar 26 16:42:56.287: INFO: Got endpoints: latency-svc-jvb7m [836.470197ms]
Mar 26 16:42:56.352: INFO: Created: latency-svc-bcpkk
Mar 26 16:42:56.357: INFO: Got endpoints: latency-svc-mrkkb [813.419557ms]
Mar 26 16:42:56.373: INFO: Created: latency-svc-4tjsj
Mar 26 16:42:56.392: INFO: Got endpoints: latency-svc-7kzd7 [709.258821ms]
Mar 26 16:42:56.445: INFO: Got endpoints: latency-svc-bcpkk [748.966007ms]
Mar 26 16:42:56.491: INFO: Created: latency-svc-d6j62
Mar 26 16:42:56.491: INFO: Created: latency-svc-khd66
Mar 26 16:42:56.498: INFO: Got endpoints: latency-svc-khd66 [657.925027ms]
Mar 26 16:42:56.498: INFO: Got endpoints: latency-svc-4tjsj [789.005583ms]
Mar 26 16:42:56.533: INFO: Got endpoints: latency-svc-d6j62 [692.022818ms]
Mar 26 16:42:56.536: INFO: Created: latency-svc-bhpph
Mar 26 16:42:56.572: INFO: Created: latency-svc-ffxk7
Mar 26 16:42:56.587: INFO: Created: latency-svc-bmztt
Mar 26 16:42:56.606: INFO: Got endpoints: latency-svc-bhpph [764.937751ms]
Mar 26 16:42:56.636: INFO: Got endpoints: latency-svc-ffxk7 [749.662357ms]
Mar 26 16:42:56.640: INFO: Created: latency-svc-8ssq6
Mar 26 16:42:56.645: INFO: Got endpoints: latency-svc-bmztt [732.902671ms]
Mar 26 16:42:56.668: INFO: Created: latency-svc-7q7cz
Mar 26 16:42:56.683: INFO: Got endpoints: latency-svc-8ssq6 [718.764362ms]
Mar 26 16:42:56.693: INFO: Created: latency-svc-hmb9m
Mar 26 16:42:56.698: INFO: Got endpoints: latency-svc-7q7cz [714.397683ms]
Mar 26 16:42:56.708: INFO: Created: latency-svc-prtzn
Mar 26 16:42:56.716: INFO: Created: latency-svc-pkkdb
Mar 26 16:42:56.785: INFO: Created: latency-svc-d6dzr
Mar 26 16:42:56.785: INFO: Got endpoints: latency-svc-pkkdb [598.512719ms]
Mar 26 16:42:56.820: INFO: Created: latency-svc-kx2mz
Mar 26 16:42:56.842: INFO: Got endpoints: latency-svc-d6dzr [555.366269ms]
Mar 26 16:42:56.844: INFO: Got endpoints: latency-svc-hmb9m [778.396577ms]
Mar 26 16:42:56.844: INFO: Got endpoints: latency-svc-prtzn [689.16303ms]
Mar 26 16:42:56.856: INFO: Created: latency-svc-w6z96
Mar 26 16:42:56.873: INFO: Got endpoints: latency-svc-kx2mz [480.872456ms]
Mar 26 16:42:56.882: INFO: Created: latency-svc-7spnw
Mar 26 16:42:56.918: INFO: Created: latency-svc-mpf5s
Mar 26 16:42:56.922: INFO: Created: latency-svc-p8xxb
Mar 26 16:42:56.938: INFO: Created: latency-svc-gchlj
Mar 26 16:42:56.939: INFO: Created: latency-svc-cpfpx
Mar 26 16:42:56.956: INFO: Created: latency-svc-59cp6
Mar 26 16:42:56.976: INFO: Created: latency-svc-6svw4
Mar 26 16:42:57.009: INFO: Created: latency-svc-77z6s
Mar 26 16:42:57.009: INFO: Got endpoints: latency-svc-w6z96 [652.703077ms]
Mar 26 16:42:57.010: INFO: Created: latency-svc-qbhc4
Mar 26 16:42:57.017: INFO: Created: latency-svc-h5772
Mar 26 16:42:57.029: INFO: Created: latency-svc-dg9zw
Mar 26 16:42:57.043: INFO: Created: latency-svc-cwm6c
Mar 26 16:42:57.119: INFO: Created: latency-svc-brb7p
Mar 26 16:42:57.139: INFO: Got endpoints: latency-svc-7spnw [693.504267ms]
Mar 26 16:42:57.139: INFO: Got endpoints: latency-svc-gchlj [640.867655ms]
Mar 26 16:42:57.139: INFO: Got endpoints: latency-svc-cpfpx [605.959977ms]
Mar 26 16:42:57.141: INFO: Created: latency-svc-c5nzv
Mar 26 16:42:57.154: INFO: Created: latency-svc-9mq5j
Mar 26 16:42:57.166: INFO: Created: latency-svc-s6vq2
Mar 26 16:42:57.179: INFO: Created: latency-svc-twbf8
Mar 26 16:42:57.235: INFO: Created: latency-svc-l6gpz
Mar 26 16:42:57.238: INFO: Got endpoints: latency-svc-mpf5s [739.201951ms]
Mar 26 16:42:57.238: INFO: Got endpoints: latency-svc-p8xxb [632.046235ms]
Mar 26 16:42:57.302: INFO: Got endpoints: latency-svc-59cp6 [665.926976ms]
Mar 26 16:42:57.405: INFO: Created: latency-svc-blggq
Mar 26 16:42:57.406: INFO: Got endpoints: latency-svc-77z6s [707.933836ms]
Mar 26 16:42:57.413: INFO: Got endpoints: latency-svc-6svw4 [768.523412ms]
Mar 26 16:42:57.414: INFO: Got endpoints: latency-svc-dg9zw [571.405723ms]
Mar 26 16:42:57.416: INFO: Got endpoints: latency-svc-qbhc4 [732.480227ms]
Mar 26 16:42:57.414: INFO: Got endpoints: latency-svc-h5772 [628.608092ms]
Mar 26 16:42:57.529: INFO: Got endpoints: latency-svc-cwm6c [685.387124ms]
Mar 26 16:42:57.533: INFO: Got endpoints: latency-svc-brb7p [688.981444ms]
Mar 26 16:42:57.535: INFO: Created: latency-svc-jz2kr
Mar 26 16:42:57.536: INFO: Created: latency-svc-bp298
Mar 26 16:42:57.556: INFO: Got endpoints: latency-svc-s6vq2 [417.682835ms]
Mar 26 16:42:57.557: INFO: Got endpoints: latency-svc-c5nzv [683.589471ms]
Mar 26 16:42:57.557: INFO: Got endpoints: latency-svc-9mq5j [547.225956ms]
Mar 26 16:42:57.604: INFO: Created: latency-svc-ln7jw
Mar 26 16:42:57.623: INFO: Created: latency-svc-6xmxv
Mar 26 16:42:57.697: INFO: Got endpoints: latency-svc-jz2kr [458.739154ms]
Mar 26 16:42:57.697: INFO: Got endpoints: latency-svc-twbf8 [557.943884ms]
Mar 26 16:42:57.698: INFO: Got endpoints: latency-svc-l6gpz [558.181263ms]
Mar 26 16:42:57.698: INFO: Got endpoints: latency-svc-bp298 [395.248348ms]
Mar 26 16:42:57.698: INFO: Got endpoints: latency-svc-blggq [460.516108ms]
Mar 26 16:42:57.731: INFO: Got endpoints: latency-svc-ln7jw [324.88287ms]
Mar 26 16:42:57.733: INFO: Created: latency-svc-l8s48
Mar 26 16:42:57.736: INFO: Created: latency-svc-qs6x2
Mar 26 16:42:57.748: INFO: Got endpoints: latency-svc-6xmxv [331.30083ms]
Mar 26 16:42:57.763: INFO: Created: latency-svc-f2sw9
Mar 26 16:42:57.864: INFO: Got endpoints: latency-svc-l8s48 [450.183066ms]
Mar 26 16:42:57.864: INFO: Got endpoints: latency-svc-qs6x2 [448.194547ms]
Mar 26 16:42:57.869: INFO: Created: latency-svc-fxlqz
Mar 26 16:42:57.872: INFO: Created: latency-svc-rq5bt
Mar 26 16:42:57.873: INFO: Created: latency-svc-hrv4r
Mar 26 16:42:57.874: INFO: Created: latency-svc-7d6ww
Mar 26 16:42:57.916: INFO: Got endpoints: latency-svc-f2sw9 [499.557619ms]
Mar 26 16:42:57.924: INFO: Got endpoints: latency-svc-rq5bt [394.696608ms]
Mar 26 16:42:57.925: INFO: Created: latency-svc-8qcbk
Mar 26 16:42:57.969: INFO: Got endpoints: latency-svc-fxlqz [412.625053ms]
Mar 26 16:42:57.969: INFO: Got endpoints: latency-svc-hrv4r [412.394863ms]
Mar 26 16:42:58.005: INFO: Created: latency-svc-phnpz
Mar 26 16:42:58.017: INFO: Got endpoints: latency-svc-7d6ww [484.73581ms]
Mar 26 16:42:58.018: INFO: Got endpoints: latency-svc-8qcbk [461.256065ms]
Mar 26 16:42:58.030: INFO: Created: latency-svc-fgmbd
Mar 26 16:42:58.056: INFO: Got endpoints: latency-svc-phnpz [358.728402ms]
Mar 26 16:42:58.072: INFO: Created: latency-svc-878w2
Mar 26 16:42:58.086: INFO: Created: latency-svc-dggtk
Mar 26 16:42:58.094: INFO: Created: latency-svc-xppnp
Mar 26 16:42:58.117: INFO: Got endpoints: latency-svc-fgmbd [418.974112ms]
Mar 26 16:42:58.142: INFO: Created: latency-svc-wrtfl
Mar 26 16:42:58.201: INFO: Got endpoints: latency-svc-wrtfl [469.765074ms]
Mar 26 16:42:58.201: INFO: Got endpoints: latency-svc-878w2 [503.550498ms]
Mar 26 16:42:58.236: INFO: Got endpoints: latency-svc-dggtk [537.716796ms]
Mar 26 16:42:58.237: INFO: Got endpoints: latency-svc-xppnp [539.641077ms]
Mar 26 16:42:58.239: INFO: Created: latency-svc-54zn2
Mar 26 16:42:58.284: INFO: Created: latency-svc-5wbdp
Mar 26 16:42:58.366: INFO: Got endpoints: latency-svc-54zn2 [617.984116ms]
Mar 26 16:42:58.378: INFO: Created: latency-svc-8zf2r
Mar 26 16:42:58.378: INFO: Created: latency-svc-r2ckh
Mar 26 16:42:58.378: INFO: Created: latency-svc-jtfwr
Mar 26 16:42:58.378: INFO: Got endpoints: latency-svc-jtfwr [514.17101ms]
Mar 26 16:42:58.378: INFO: Got endpoints: latency-svc-5wbdp [514.318963ms]
Mar 26 16:42:58.401: INFO: Got endpoints: latency-svc-r2ckh [485.168829ms]
Mar 26 16:42:58.417: INFO: Got endpoints: latency-svc-8zf2r [492.668292ms]
Mar 26 16:42:58.458: INFO: Created: latency-svc-fqcbp
Mar 26 16:42:58.490: INFO: Got endpoints: latency-svc-fqcbp [520.33087ms]
Mar 26 16:42:58.545: INFO: Created: latency-svc-8btn7
Mar 26 16:42:58.545: INFO: Got endpoints: latency-svc-8btn7 [576.004765ms]
Mar 26 16:42:58.550: INFO: Created: latency-svc-xkv98
Mar 26 16:42:58.584: INFO: Created: latency-svc-6d42t
Mar 26 16:42:58.608: INFO: Created: latency-svc-zhb8q
Mar 26 16:42:58.625: INFO: Got endpoints: latency-svc-xkv98 [607.099738ms]
Mar 26 16:42:58.625: INFO: Got endpoints: latency-svc-6d42t [607.547305ms]
Mar 26 16:42:58.673: INFO: Got endpoints: latency-svc-zhb8q [616.87855ms]
Mar 26 16:42:58.675: INFO: Created: latency-svc-rdqj2
Mar 26 16:42:58.694: INFO: Got endpoints: latency-svc-rdqj2 [576.316151ms]
Mar 26 16:42:58.694: INFO: Created: latency-svc-pplds
Mar 26 16:42:58.725: INFO: Got endpoints: latency-svc-pplds [523.698139ms]
Mar 26 16:42:58.733: INFO: Created: latency-svc-z65j6
Mar 26 16:42:58.753: INFO: Created: latency-svc-gfm5l
Mar 26 16:42:58.764: INFO: Got endpoints: latency-svc-z65j6 [563.268886ms]
Mar 26 16:42:58.784: INFO: Created: latency-svc-946l5
Mar 26 16:42:58.792: INFO: Got endpoints: latency-svc-gfm5l [555.269527ms]
Mar 26 16:42:58.802: INFO: Got endpoints: latency-svc-946l5 [563.941691ms]
Mar 26 16:42:58.831: INFO: Created: latency-svc-42rl8
Mar 26 16:42:58.858: INFO: Created: latency-svc-5lnpb
Mar 26 16:42:58.859: INFO: Got endpoints: latency-svc-42rl8 [492.516974ms]
Mar 26 16:42:58.863: INFO: Created: latency-svc-t78tc
Mar 26 16:42:58.895: INFO: Got endpoints: latency-svc-t78tc [516.860544ms]
Mar 26 16:42:58.896: INFO: Got endpoints: latency-svc-5lnpb [517.078728ms]
Mar 26 16:42:58.901: INFO: Created: latency-svc-798z9
Mar 26 16:42:58.912: INFO: Got endpoints: latency-svc-798z9 [491.298747ms]
Mar 26 16:42:58.937: INFO: Created: latency-svc-xc8bn
Mar 26 16:42:58.961: INFO: Created: latency-svc-7pstk
Mar 26 16:42:58.981: INFO: Created: latency-svc-vzkrd
Mar 26 16:42:58.990: INFO: Got endpoints: latency-svc-xc8bn [589.000827ms]
Mar 26 16:42:59.022: INFO: Got endpoints: latency-svc-7pstk [532.341506ms]
Mar 26 16:42:59.049: INFO: Created: latency-svc-tw84j
Mar 26 16:42:59.050: INFO: Got endpoints: latency-svc-vzkrd [504.569305ms]
Mar 26 16:42:59.056: INFO: Created: latency-svc-rgdsl
Mar 26 16:42:59.077: INFO: Got endpoints: latency-svc-tw84j [451.644211ms]
Mar 26 16:42:59.088: INFO: Created: latency-svc-4qkdp
Mar 26 16:42:59.112: INFO: Created: latency-svc-slcxj
Mar 26 16:42:59.123: INFO: Created: latency-svc-68zt2
Mar 26 16:42:59.143: INFO: Created: latency-svc-wm7cz
Mar 26 16:42:59.173: INFO: Created: latency-svc-qk54q
Mar 26 16:42:59.184: INFO: Created: latency-svc-mjz4m
Mar 26 16:42:59.268: INFO: Created: latency-svc-7mbrm
Mar 26 16:42:59.286: INFO: Got endpoints: latency-svc-4qkdp [613.511921ms]
Mar 26 16:42:59.287: INFO: Got endpoints: latency-svc-rgdsl [661.364639ms]
Mar 26 16:42:59.310: INFO: Created: latency-svc-jn55h
Mar 26 16:42:59.345: INFO: Got endpoints: latency-svc-68zt2 [620.052384ms]
Mar 26 16:42:59.345: INFO: Got endpoints: latency-svc-slcxj [651.609783ms]
Mar 26 16:42:59.346: INFO: Got endpoints: latency-svc-wm7cz [581.049174ms]
Mar 26 16:42:59.372: INFO: Created: latency-svc-fl65l
Mar 26 16:42:59.380: INFO: Created: latency-svc-kr4fw
Mar 26 16:42:59.385: INFO: Got endpoints: latency-svc-mjz4m [592.384646ms]
Mar 26 16:42:59.385: INFO: Got endpoints: latency-svc-qk54q [583.49784ms]
Mar 26 16:42:59.410: INFO: Got endpoints: latency-svc-7mbrm [551.04643ms]
Mar 26 16:42:59.445: INFO: Created: latency-svc-w7d95
Mar 26 16:42:59.505: INFO: Got endpoints: latency-svc-jn55h [610.256555ms]
Mar 26 16:42:59.514: INFO: Created: latency-svc-zlctf
Mar 26 16:42:59.550: INFO: Created: latency-svc-gnk9r
Mar 26 16:42:59.571: INFO: Got endpoints: latency-svc-fl65l [675.624325ms]
Mar 26 16:42:59.572: INFO: Got endpoints: latency-svc-kr4fw [659.966571ms]
Mar 26 16:42:59.572: INFO: Got endpoints: latency-svc-w7d95 [581.574432ms]
Mar 26 16:42:59.625: INFO: Created: latency-svc-6fjth
Mar 26 16:42:59.627: INFO: Got endpoints: latency-svc-gnk9r [576.865545ms]
Mar 26 16:42:59.627: INFO: Got endpoints: latency-svc-zlctf [604.610898ms]
Mar 26 16:42:59.681: INFO: Created: latency-svc-mrnk9
Mar 26 16:42:59.706: INFO: Got endpoints: latency-svc-6fjth [200.360751ms]
Mar 26 16:42:59.744: INFO: Got endpoints: latency-svc-mrnk9 [666.95981ms]
Mar 26 16:42:59.762: INFO: Created: latency-svc-kvtvg
Mar 26 16:42:59.878: INFO: Got endpoints: latency-svc-kvtvg [591.794465ms]
Mar 26 16:42:59.898: INFO: Created: latency-svc-2x2df
Mar 26 16:42:59.923: INFO: Created: latency-svc-2ng5b
Mar 26 16:42:59.956: INFO: Created: latency-svc-r95hg
Mar 26 16:43:00.010: INFO: Got endpoints: latency-svc-2x2df [664.037054ms]
Mar 26 16:43:00.044: INFO: Got endpoints: latency-svc-r95hg [699.326099ms]
Mar 26 16:43:00.045: INFO: Got endpoints: latency-svc-2ng5b [757.991901ms]
Mar 26 16:43:00.080: INFO: Created: latency-svc-gqgtl
Mar 26 16:43:00.124: INFO: Got endpoints: latency-svc-gqgtl [778.350006ms]
Mar 26 16:43:00.134: INFO: Created: latency-svc-7gcc8
Mar 26 16:43:00.192: INFO: Created: latency-svc-qxwnx
Mar 26 16:43:00.210: INFO: Created: latency-svc-nsw2r
Mar 26 16:43:00.236: INFO: Created: latency-svc-ls2sm
Mar 26 16:43:00.261: INFO: Created: latency-svc-xp9pw
Mar 26 16:43:00.273: INFO: Got endpoints: latency-svc-7gcc8 [888.172993ms]
Mar 26 16:43:00.309: INFO: Created: latency-svc-7hcnq
Mar 26 16:43:00.390: INFO: Created: latency-svc-qp2t4
Mar 26 16:43:00.390: INFO: Created: latency-svc-jxpph
Mar 26 16:43:00.390: INFO: Got endpoints: latency-svc-ls2sm [818.872553ms]
Mar 26 16:43:00.390: INFO: Got endpoints: latency-svc-qxwnx [1.005300937s]
Mar 26 16:43:00.391: INFO: Got endpoints: latency-svc-nsw2r [980.922923ms]
Mar 26 16:43:00.409: INFO: Created: latency-svc-j9ssj
Mar 26 16:43:00.470: INFO: Got endpoints: latency-svc-7hcnq [842.763544ms]
Mar 26 16:43:00.470: INFO: Got endpoints: latency-svc-xp9pw [898.427759ms]
Mar 26 16:43:00.493: INFO: Got endpoints: latency-svc-jxpph [921.157912ms]
Mar 26 16:43:00.493: INFO: Got endpoints: latency-svc-qp2t4 [866.310166ms]
Mar 26 16:43:00.529: INFO: Created: latency-svc-6vszb
Mar 26 16:43:00.573: INFO: Created: latency-svc-cn6d7
Mar 26 16:43:00.574: INFO: Got endpoints: latency-svc-j9ssj [868.013501ms]
Mar 26 16:43:00.593: INFO: Got endpoints: latency-svc-6vszb [848.552367ms]
Mar 26 16:43:00.713: INFO: Created: latency-svc-hdn9l
Mar 26 16:43:00.713: INFO: Got endpoints: latency-svc-cn6d7 [834.554076ms]
Mar 26 16:43:00.721: INFO: Created: latency-svc-5nns4
Mar 26 16:43:00.721: INFO: Got endpoints: latency-svc-5nns4 [676.407404ms]
Mar 26 16:43:00.721: INFO: Got endpoints: latency-svc-hdn9l [711.560143ms]
Mar 26 16:43:00.731: INFO: Created: latency-svc-h4vkf
Mar 26 16:43:00.772: INFO: Created: latency-svc-dc4kn
Mar 26 16:43:00.779: INFO: Got endpoints: latency-svc-h4vkf [734.651454ms]
Mar 26 16:43:00.820: INFO: Created: latency-svc-nspv4
Mar 26 16:43:00.840: INFO: Created: latency-svc-bjz9f
Mar 26 16:43:00.881: INFO: Got endpoints: latency-svc-nspv4 [608.174125ms]
Mar 26 16:43:00.881: INFO: Got endpoints: latency-svc-dc4kn [757.487883ms]
Mar 26 16:43:00.955: INFO: Got endpoints: latency-svc-bjz9f [564.664011ms]
Mar 26 16:43:01.035: INFO: Created: latency-svc-9mr5q
Mar 26 16:43:01.080: INFO: Got endpoints: latency-svc-9mr5q [688.844384ms]
Mar 26 16:43:01.119: INFO: Created: latency-svc-k4ffx
Mar 26 16:43:01.164: INFO: Created: latency-svc-gv2d7
Mar 26 16:43:01.196: INFO: Got endpoints: latency-svc-k4ffx [805.437657ms]
Mar 26 16:43:01.211: INFO: Created: latency-svc-mcwz8
Mar 26 16:43:01.287: INFO: Got endpoints: latency-svc-mcwz8 [790.952884ms]
Mar 26 16:43:01.289: INFO: Got endpoints: latency-svc-gv2d7 [795.661023ms]
Mar 26 16:43:01.321: INFO: Created: latency-svc-vk4rg
Mar 26 16:43:01.342: INFO: Created: latency-svc-frwrr
Mar 26 16:43:01.357: INFO: Got endpoints: latency-svc-vk4rg [887.598943ms]
Mar 26 16:43:01.368: INFO: Created: latency-svc-hjr4k
Mar 26 16:43:01.405: INFO: Got endpoints: latency-svc-frwrr [934.454547ms]
Mar 26 16:43:01.436: INFO: Got endpoints: latency-svc-hjr4k [862.236494ms]
Mar 26 16:43:01.454: INFO: Created: latency-svc-mxpgt
Mar 26 16:43:01.472: INFO: Created: latency-svc-75hhs
Mar 26 16:43:01.503: INFO: Got endpoints: latency-svc-75hhs [790.366439ms]
Mar 26 16:43:01.504: INFO: Got endpoints: latency-svc-mxpgt [910.992196ms]
Mar 26 16:43:01.576: INFO: Created: latency-svc-9swp9
Mar 26 16:43:01.604: INFO: Created: latency-svc-95vqh
Mar 26 16:43:01.610: INFO: Got endpoints: latency-svc-9swp9 [887.176566ms]
Mar 26 16:43:01.729: INFO: Created: latency-svc-qtcz5
Mar 26 16:43:01.729: INFO: Created: latency-svc-9w4gj
Mar 26 16:43:01.756: INFO: Got endpoints: latency-svc-95vqh [1.034323811s]
Mar 26 16:43:01.828: INFO: Created: latency-svc-cvkg5
Mar 26 16:43:01.895: INFO: Created: latency-svc-h2xbr
Mar 26 16:43:01.896: INFO: Created: latency-svc-f8h96
Mar 26 16:43:01.936: INFO: Got endpoints: latency-svc-9w4gj [1.054366953s]
Mar 26 16:43:01.936: INFO: Got endpoints: latency-svc-qtcz5 [1.15703378s]
Mar 26 16:43:01.937: INFO: Got endpoints: latency-svc-cvkg5 [1.05486136s]
Mar 26 16:43:02.063: INFO: Got endpoints: latency-svc-f8h96 [1.108427455s]
Mar 26 16:43:02.064: INFO: Created: latency-svc-qrt2n
Mar 26 16:43:02.185: INFO: Got endpoints: latency-svc-qrt2n [988.822827ms]
Mar 26 16:43:02.185: INFO: Got endpoints: latency-svc-h2xbr [1.105687392s]
Mar 26 16:43:02.191: INFO: Created: latency-svc-k52x8
Mar 26 16:43:02.256: INFO: Created: latency-svc-7x4qw
Mar 26 16:43:02.292: INFO: Got endpoints: latency-svc-k52x8 [1.005254374s]
Mar 26 16:43:02.311: INFO: Got endpoints: latency-svc-7x4qw [1.022423488s]
Mar 26 16:43:02.317: INFO: Created: latency-svc-dwg2j
Mar 26 16:43:02.391: INFO: Got endpoints: latency-svc-dwg2j [1.033782438s]
Mar 26 16:43:02.448: INFO: Created: latency-svc-nhlk9
Mar 26 16:43:02.459: INFO: Created: latency-svc-hbm5r
Mar 26 16:43:02.498: INFO: Got endpoints: latency-svc-hbm5r [1.061153848s]
Mar 26 16:43:02.536: INFO: Created: latency-svc-mgvn2
Mar 26 16:43:02.576: INFO: Created: latency-svc-rf9mr
Mar 26 16:43:02.576: INFO: Got endpoints: latency-svc-mgvn2 [1.072205389s]
Mar 26 16:43:02.576: INFO: Got endpoints: latency-svc-nhlk9 [1.171440768s]
Mar 26 16:43:02.604: INFO: Got endpoints: latency-svc-rf9mr [1.100854466s]
Mar 26 16:43:02.626: INFO: Created: latency-svc-dxrhz
Mar 26 16:43:02.672: INFO: Got endpoints: latency-svc-dxrhz [1.061433062s]
Mar 26 16:43:02.672: INFO: Created: latency-svc-2fdjs
Mar 26 16:43:02.711: INFO: Created: latency-svc-tbp28
Mar 26 16:43:02.750: INFO: Got endpoints: latency-svc-tbp28 [813.573784ms]
Mar 26 16:43:02.750: INFO: Got endpoints: latency-svc-2fdjs [993.824036ms]
Mar 26 16:43:02.765: INFO: Created: latency-svc-j979k
Mar 26 16:43:02.789: INFO: Created: latency-svc-gj7qn
Mar 26 16:43:02.837: INFO: Got endpoints: latency-svc-gj7qn [900.316856ms]
Mar 26 16:43:02.837: INFO: Got endpoints: latency-svc-j979k [900.809509ms]
Mar 26 16:43:02.854: INFO: Created: latency-svc-nhmcq
Mar 26 16:43:02.888: INFO: Created: latency-svc-hfxnj
Mar 26 16:43:02.897: INFO: Got endpoints: latency-svc-nhmcq [833.589348ms]
Mar 26 16:43:02.900: INFO: Got endpoints: latency-svc-hfxnj [714.48327ms]
Mar 26 16:43:02.900: INFO: Latencies: [82.507368ms 163.372085ms 164.826426ms 190.109025ms 200.360751ms 216.634227ms 228.047942ms 251.328981ms 306.039381ms 324.88287ms 331.30083ms 358.728402ms 373.931286ms 394.696608ms 395.248348ms 405.59644ms 412.394863ms 412.625053ms 417.682835ms 418.974112ms 448.194547ms 450.183066ms 450.87958ms 451.644211ms 458.739154ms 460.516108ms 461.256065ms 469.765074ms 480.872456ms 484.73581ms 485.168829ms 491.298747ms 492.516974ms 492.668292ms 499.557619ms 503.550498ms 504.569305ms 514.17101ms 514.318963ms 516.860544ms 517.078728ms 520.33087ms 523.698139ms 532.341506ms 537.716796ms 539.641077ms 547.225956ms 551.04643ms 555.269527ms 555.366269ms 557.943884ms 558.181263ms 563.268886ms 563.941691ms 564.664011ms 571.405723ms 576.004765ms 576.316151ms 576.865545ms 581.049174ms 581.574432ms 583.49784ms 589.000827ms 591.794465ms 592.384646ms 598.512719ms 604.610898ms 605.959977ms 607.099738ms 607.547305ms 608.174125ms 609.731777ms 609.7353ms 610.256555ms 613.511921ms 616.87855ms 617.984116ms 620.052384ms 628.608092ms 632.046235ms 640.867655ms 651.609783ms 652.703077ms 656.083709ms 657.925027ms 659.966571ms 661.364639ms 664.037054ms 665.926976ms 666.95981ms 675.624325ms 676.407404ms 683.589471ms 685.387124ms 688.844384ms 688.981444ms 689.16303ms 692.022818ms 693.504267ms 699.326099ms 703.928932ms 706.075094ms 707.933836ms 709.258821ms 710.623971ms 710.806271ms 711.560143ms 714.3531ms 714.397683ms 714.428288ms 714.48327ms 718.764362ms 732.480227ms 732.902671ms 734.651454ms 738.682593ms 738.927668ms 739.201951ms 742.949ms 744.224828ms 744.620635ms 746.106047ms 748.167079ms 748.798995ms 748.82934ms 748.966007ms 749.662357ms 749.67365ms 750.355956ms 751.14331ms 756.282623ms 757.487883ms 757.991901ms 764.649831ms 764.937751ms 766.8409ms 767.983892ms 768.523412ms 773.699828ms 778.350006ms 778.396577ms 779.693774ms 782.703548ms 785.763669ms 789.005583ms 789.014075ms 790.366439ms 790.952884ms 795.661023ms 798.000179ms 802.054881ms 805.437657ms 806.069904ms 813.419557ms 813.573784ms 818.872553ms 826.010849ms 827.049095ms 833.589348ms 834.554076ms 835.784147ms 836.470197ms 841.78932ms 842.763544ms 848.552367ms 855.713199ms 855.763932ms 857.080427ms 861.147832ms 862.236494ms 866.310166ms 868.013501ms 887.176566ms 887.598943ms 888.172993ms 898.427759ms 900.316856ms 900.809509ms 910.992196ms 921.157912ms 934.454547ms 980.922923ms 988.822827ms 993.824036ms 1.005254374s 1.005300937s 1.022423488s 1.033782438s 1.034323811s 1.054366953s 1.05486136s 1.061153848s 1.061433062s 1.072205389s 1.100854466s 1.105687392s 1.108427455s 1.15703378s 1.171440768s 1.351010339s]
Mar 26 16:43:02.900: INFO: 50 %ile: 703.928932ms
Mar 26 16:43:02.900: INFO: 90 %ile: 934.454547ms
Mar 26 16:43:02.900: INFO: 99 %ile: 1.171440768s
Mar 26 16:43:02.900: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:43:02.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2102" for this suite.

â€¢ [SLOW TEST:12.198 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":275,"completed":35,"skipped":608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:43:02.983: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:43:03.324: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 26 16:43:08.350: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 16:43:08.351: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 16:43:08.811: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7466 /apis/apps/v1/namespaces/deployment-7466/deployments/test-cleanup-deployment 544aec8b-0e44-42b6-86a8-971865b5bfbc 5121 1 2020-03-26 16:43:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-03-26 16:43:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f2d948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 26 16:43:08.926: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:43:08.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7466" for this suite.

â€¢ [SLOW TEST:6.140 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":275,"completed":36,"skipped":632,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:43:09.126: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:43:25.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3382" for this suite.

â€¢ [SLOW TEST:16.876 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":275,"completed":37,"skipped":635,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:43:26.003: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:43:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5579" for this suite.

â€¢ [SLOW TEST:11.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":275,"completed":38,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:43:37.274: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-6960
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 26 16:43:37.480: INFO: Found 0 stateful pods, waiting for 3
Mar 26 16:43:47.490: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:43:47.490: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:43:47.490: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 26 16:43:47.529: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 26 16:43:57.611: INFO: Updating stateful set ss2
Mar 26 16:43:57.642: INFO: Waiting for Pod statefulset-6960/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:44:07.652: INFO: Waiting for Pod statefulset-6960/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 26 16:44:17.948: INFO: Found 2 stateful pods, waiting for 3
Mar 26 16:44:27.959: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:44:27.959: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:44:27.959: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 26 16:44:28.000: INFO: Updating stateful set ss2
Mar 26 16:44:28.051: INFO: Waiting for Pod statefulset-6960/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:44:38.083: INFO: Updating stateful set ss2
Mar 26 16:44:38.096: INFO: Waiting for StatefulSet statefulset-6960/ss2 to complete update
Mar 26 16:44:38.096: INFO: Waiting for Pod statefulset-6960/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 16:44:48.111: INFO: Waiting for StatefulSet statefulset-6960/ss2 to complete update
Mar 26 16:44:48.113: INFO: Waiting for Pod statefulset-6960/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 16:44:58.132: INFO: Deleting all statefulset in ns statefulset-6960
Mar 26 16:44:58.140: INFO: Scaling statefulset ss2 to 0
Mar 26 16:45:28.167: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 16:45:28.171: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:45:28.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6960" for this suite.

â€¢ [SLOW TEST:110.917 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":275,"completed":39,"skipped":668,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:45:28.192: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-bd8d7a01-71f7-428a-b5d1-c7fa9f925f0c
STEP: Creating a pod to test consume secrets
Mar 26 16:45:28.375: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca" in namespace "projected-6355" to be "Succeeded or Failed"
Mar 26 16:45:28.403: INFO: Pod "pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025796ms
Mar 26 16:45:30.411: INFO: Pod "pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036347152s
STEP: Saw pod success
Mar 26 16:45:30.411: INFO: Pod "pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca" satisfied condition "Succeeded or Failed"
Mar 26 16:45:30.414: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 16:45:30.452: INFO: Waiting for pod pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca to disappear
Mar 26 16:45:30.453: INFO: Pod pod-projected-secrets-a6086534-b955-45ce-b741-299f16a666ca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:45:30.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6355" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":40,"skipped":681,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:45:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-9377/configmap-test-92537e54-8ab9-4509-a646-64630c6b78f8
STEP: Creating a pod to test consume configMaps
Mar 26 16:45:30.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d" in namespace "configmap-9377" to be "Succeeded or Failed"
Mar 26 16:45:30.697: INFO: Pod "pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.242148ms
Mar 26 16:45:32.704: INFO: Pod "pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012421194s
STEP: Saw pod success
Mar 26 16:45:32.704: INFO: Pod "pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d" satisfied condition "Succeeded or Failed"
Mar 26 16:45:32.709: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d container env-test: <nil>
STEP: delete the pod
Mar 26 16:45:32.736: INFO: Waiting for pod pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d to disappear
Mar 26 16:45:32.737: INFO: Pod pod-configmaps-16cdcac5-0f85-4a39-9658-6817922c934d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:45:32.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9377" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":41,"skipped":695,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:45:32.744: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:45:36.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8667" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":42,"skipped":701,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:45:36.933: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 16:45:37.148: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 16:45:37.161: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 16:45:37.163: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-dqhapg before test
Mar 26 16:45:37.175: INFO: kube-flannel-ds-amd64-vblvg from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.175: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 16:45:37.175: INFO: sonobuoy from sonobuoy started at 2020-03-26 16:32:42 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.175: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 26 16:45:37.175: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 16:45:37.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 16:45:37.175: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 16:45:37.175: INFO: busybox-readonly-fsb101f51a-6425-4866-8b2c-4657a2081ecd from kubelet-test-8667 started at 2020-03-26 16:45:32 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.175: INFO: 	Container busybox-readonly-fsb101f51a-6425-4866-8b2c-4657a2081ecd ready: true, restart count 0
Mar 26 16:45:37.175: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-vf6bys before test
Mar 26 16:45:37.188: INFO: kube-flannel-ds-amd64-4db44 from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.188: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 16:45:37.188: INFO: coredns-6ccf845bfb-jklrr from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.188: INFO: 	Container coredns ready: true, restart count 0
Mar 26 16:45:37.188: INFO: coredns-6ccf845bfb-c9f7x from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 16:45:37.188: INFO: 	Container coredns ready: true, restart count 0
Mar 26 16:45:37.188: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 16:45:37.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 16:45:37.188: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-78ca538a-7320-444f-a25b-dc99b50a50b8 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-78ca538a-7320-444f-a25b-dc99b50a50b8 off the node kubedee-test-worker-dqhapg
STEP: verifying the node doesn't have the label kubernetes.io/e2e-78ca538a-7320-444f-a25b-dc99b50a50b8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:50:41.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4037" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:304.558 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":275,"completed":43,"skipped":708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:50:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-3c0e3e43-c58d-4c2e-a8bb-f53d6072fd5b
STEP: Creating a pod to test consume configMaps
Mar 26 16:50:41.717: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0" in namespace "projected-5073" to be "Succeeded or Failed"
Mar 26 16:50:41.723: INFO: Pod "pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.601536ms
Mar 26 16:50:43.728: INFO: Pod "pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01085489s
STEP: Saw pod success
Mar 26 16:50:43.728: INFO: Pod "pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0" satisfied condition "Succeeded or Failed"
Mar 26 16:50:43.731: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 16:50:43.755: INFO: Waiting for pod pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0 to disappear
Mar 26 16:50:43.756: INFO: Pod pod-projected-configmaps-c9bd8c80-6a42-4871-9e33-d634b19a4dc0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:50:43.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5073" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":44,"skipped":792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:50:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5200
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:50:43.896: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:50:50.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5200" for this suite.

â€¢ [SLOW TEST:6.943 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":275,"completed":45,"skipped":821,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:50:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-e1511ab0-3ce9-45b3-b9e6-2a9ce6d6cd3d
STEP: Creating a pod to test consume secrets
Mar 26 16:50:50.951: INFO: Waiting up to 5m0s for pod "pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992" in namespace "secrets-1439" to be "Succeeded or Failed"
Mar 26 16:50:50.960: INFO: Pod "pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992": Phase="Pending", Reason="", readiness=false. Elapsed: 8.99725ms
Mar 26 16:50:52.967: INFO: Pod "pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015257224s
Mar 26 16:50:54.973: INFO: Pod "pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021311399s
STEP: Saw pod success
Mar 26 16:50:54.973: INFO: Pod "pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992" satisfied condition "Succeeded or Failed"
Mar 26 16:50:54.977: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 16:50:54.998: INFO: Waiting for pod pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992 to disappear
Mar 26 16:50:54.999: INFO: Pod pod-secrets-5cc1b0cb-ab2c-4730-8cec-fe05dd56e992 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:50:54.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1439" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":46,"skipped":823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:50:55.009: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-4279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4279 to expose endpoints map[]
Mar 26 16:50:55.263: INFO: successfully validated that service endpoint-test2 in namespace services-4279 exposes endpoints map[] (40.460064ms elapsed)
STEP: Creating pod pod1 in namespace services-4279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4279 to expose endpoints map[pod1:[80]]
Mar 26 16:50:59.370: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.074447929s elapsed, will retry)
Mar 26 16:51:00.386: INFO: successfully validated that service endpoint-test2 in namespace services-4279 exposes endpoints map[pod1:[80]] (5.0898933s elapsed)
STEP: Creating pod pod2 in namespace services-4279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4279 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 26 16:51:03.459: INFO: successfully validated that service endpoint-test2 in namespace services-4279 exposes endpoints map[pod1:[80] pod2:[80]] (3.062692493s elapsed)
STEP: Deleting pod pod1 in namespace services-4279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4279 to expose endpoints map[pod2:[80]]
Mar 26 16:51:03.507: INFO: successfully validated that service endpoint-test2 in namespace services-4279 exposes endpoints map[pod2:[80]] (21.478048ms elapsed)
STEP: Deleting pod pod2 in namespace services-4279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4279 to expose endpoints map[]
Mar 26 16:51:04.575: INFO: successfully validated that service endpoint-test2 in namespace services-4279 exposes endpoints map[] (1.056354407s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:04.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4279" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:9.830 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":275,"completed":47,"skipped":904,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-847" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":275,"completed":48,"skipped":908,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:09.065: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-8ad65783-297b-4423-9dfc-93a8984be8f7
STEP: Creating a pod to test consume secrets
Mar 26 16:51:09.250: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec" in namespace "projected-481" to be "Succeeded or Failed"
Mar 26 16:51:09.275: INFO: Pod "pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec": Phase="Pending", Reason="", readiness=false. Elapsed: 25.421909ms
Mar 26 16:51:11.283: INFO: Pod "pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033421691s
STEP: Saw pod success
Mar 26 16:51:11.283: INFO: Pod "pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec" satisfied condition "Succeeded or Failed"
Mar 26 16:51:11.288: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 16:51:11.331: INFO: Waiting for pod pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec to disappear
Mar 26 16:51:11.339: INFO: Pod pod-projected-secrets-5db90142-e40e-4019-af29-bf0e761721ec no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:11.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-481" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":49,"skipped":919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:51:11.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-1710'
Mar 26 16:51:12.740: INFO: stderr: ""
Mar 26 16:51:12.740: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar 26 16:51:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-1710'
Mar 26 16:51:13.030: INFO: stderr: ""
Mar 26 16:51:13.030: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 16:51:14.049: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:51:14.049: INFO: Found 0 / 1
Mar 26 16:51:15.036: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:51:15.036: INFO: Found 1 / 1
Mar 26 16:51:15.036: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 16:51:15.040: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 16:51:15.040: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 16:51:15.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 describe pod agnhost-master-cdgm6 --namespace=kubectl-1710'
Mar 26 16:51:15.157: INFO: stderr: ""
Mar 26 16:51:15.157: INFO: stdout: "Name:         agnhost-master-cdgm6\nNamespace:    kubectl-1710\nPriority:     0\nNode:         kubedee-test-worker-dqhapg/10.107.34.127\nStart Time:   Thu, 26 Mar 2020 16:51:12 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.69\nIPs:\n  IP:           10.244.1.69\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   cri-o://ff38322e1b48588e3cd3e4d557a3cbf056e33ef6d7f28b1b653a7c6701c2e804\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 26 Mar 2020 16:51:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wgn2t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wgn2t:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wgn2t\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Scheduled  3s    default-scheduler                    Successfully assigned kubectl-1710/agnhost-master-cdgm6 to kubedee-test-worker-dqhapg\n  Normal  Pulled     2s    kubelet, kubedee-test-worker-dqhapg  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    1s    kubelet, kubedee-test-worker-dqhapg  Created container agnhost-master\n  Normal  Started    1s    kubelet, kubedee-test-worker-dqhapg  Started container agnhost-master\n"
Mar 26 16:51:15.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 describe rc agnhost-master --namespace=kubectl-1710'
Mar 26 16:51:15.277: INFO: stderr: ""
Mar 26 16:51:15.277: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1710\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-cdgm6\n"
Mar 26 16:51:15.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 describe service agnhost-master --namespace=kubectl-1710'
Mar 26 16:51:15.421: INFO: stderr: ""
Mar 26 16:51:15.421: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1710\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.32.0.109\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.69:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 26 16:51:15.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 describe node kubedee-test-controller'
Mar 26 16:51:15.578: INFO: stderr: ""
Mar 26 16:51:15.578: INFO: stdout: "Name:               kubedee-test-controller\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    ingress-nginx=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kubedee-test-controller\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"26:fc:6d:0d:f2:cf\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.107.34.199\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 26 Mar 2020 16:30:52 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kubedee-test-controller\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 26 Mar 2020 16:51:14 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 26 Mar 2020 16:49:24 +0000   Thu, 26 Mar 2020 16:30:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 26 Mar 2020 16:49:24 +0000   Thu, 26 Mar 2020 16:30:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 26 Mar 2020 16:49:24 +0000   Thu, 26 Mar 2020 16:30:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 26 Mar 2020 16:49:24 +0000   Thu, 26 Mar 2020 16:31:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.107.34.199\n  Hostname:    kubedee-test-controller\nCapacity:\n  cpu:                2\n  ephemeral-storage:  14648436Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3940864Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14648436Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3940864Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 2213ed71bc8c4388af1bcad8b66472a1\n  System UUID:                38AD0D6C-3D3A-4993-9296-6305780073D1\n  Boot ID:                    35118be7-fe5b-4b5b-824e-efdcf425cc78\n  Kernel Version:             4.15.0-91-generic\n  OS Image:                   Ubuntu 18.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.16.1\n  Kubelet Version:            v1.18.0\n  Kube-Proxy Version:         v1.18.0\nPodCIDR:                      10.244.2.0/24\nPodCIDRs:                     10.244.2.0/24\nNon-terminated Pods:          (3 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-flannel-ds-amd64-9t684                                100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      20m\n  sonobuoy                    sonobuoy-e2e-job-21889e77e6f04f3d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-pbjm2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                100m (5%)  100m (5%)\n  memory             50Mi (1%)  50Mi (1%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Starting   20m   kube-proxy, kubedee-test-controller  Starting kube-proxy.\n  Normal  NodeReady  19m   kubelet, kubedee-test-controller     Node kubedee-test-controller status is now: NodeReady\n"
Mar 26 16:51:15.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 describe namespace kubectl-1710'
Mar 26 16:51:15.672: INFO: stderr: ""
Mar 26 16:51:15.672: INFO: stdout: "Name:         kubectl-1710\nLabels:       e2e-framework=kubectl\n              e2e-run=33c5d069-26ee-4ed9-8532-3bb0bd124aa3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:15.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1710" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":275,"completed":50,"skipped":953,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-1024
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1024
STEP: Deleting pre-stop pod
Mar 26 16:51:24.947: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:24.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1024" for this suite.

â€¢ [SLOW TEST:9.300 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":275,"completed":51,"skipped":960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:24.989: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8938
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8938
STEP: creating replication controller externalsvc in namespace services-8938
I0326 16:51:25.464969      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8938, replica count: 2
I0326 16:51:28.533299      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 16:51:31.535410      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 26 16:51:31.602: INFO: Creating new exec pod
Mar 26 16:51:33.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-8938 execpod5vzgv -- /bin/sh -x -c nslookup nodeport-service'
Mar 26 16:51:34.078: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 26 16:51:34.078: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nnodeport-service.services-8938.svc.cluster.local\tcanonical name = externalsvc.services-8938.svc.cluster.local.\nName:\texternalsvc.services-8938.svc.cluster.local\nAddress: 10.32.0.123\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8938, will wait for the garbage collector to delete the pods
Mar 26 16:51:34.159: INFO: Deleting ReplicationController externalsvc took: 20.777513ms
Mar 26 16:51:34.560: INFO: Terminating ReplicationController externalsvc pods took: 401.046386ms
Mar 26 16:51:44.026: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:44.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8938" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:19.141 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":275,"completed":52,"skipped":999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:44.145: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Mar 26 16:51:44.340: INFO: Waiting up to 5m0s for pod "var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da" in namespace "var-expansion-702" to be "Succeeded or Failed"
Mar 26 16:51:44.349: INFO: Pod "var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da": Phase="Pending", Reason="", readiness=false. Elapsed: 9.342801ms
Mar 26 16:51:46.352: INFO: Pod "var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012602053s
STEP: Saw pod success
Mar 26 16:51:46.352: INFO: Pod "var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da" satisfied condition "Succeeded or Failed"
Mar 26 16:51:46.355: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da container dapi-container: <nil>
STEP: delete the pod
Mar 26 16:51:46.388: INFO: Waiting for pod var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da to disappear
Mar 26 16:51:46.400: INFO: Pod var-expansion-99e3fc85-a171-4853-97a5-1b06b5f111da no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:46.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-702" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":275,"completed":53,"skipped":1049,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:46.415: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 16:51:47.293: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 16:51:49.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720838307, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720838307, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720838307, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720838307, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 16:51:52.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:51:52.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5022" for this suite.
STEP: Destroying namespace "webhook-5022-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":275,"completed":54,"skipped":1051,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:51:52.581: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2799
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-05c9746b-32fc-4ef2-8f05-67dc033aab9a
STEP: Creating configMap with name cm-test-opt-upd-ebfc27c4-8dce-43ab-bb8b-578aef1da4db
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-05c9746b-32fc-4ef2-8f05-67dc033aab9a
STEP: Updating configmap cm-test-opt-upd-ebfc27c4-8dce-43ab-bb8b-578aef1da4db
STEP: Creating configMap with name cm-test-opt-create-6aaa6578-ea2b-47ce-a3f3-f423174df14a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:53:01.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2799" for this suite.

â€¢ [SLOW TEST:68.739 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":55,"skipped":1054,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:53:01.321: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:53:01.472: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 26 16:53:01.487: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 26 16:53:06.492: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 16:53:06.492: INFO: Creating deployment "test-rolling-update-deployment"
Mar 26 16:53:06.503: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 26 16:53:06.515: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 26 16:53:08.544: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 26 16:53:08.553: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 16:53:08.621: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2876 /apis/apps/v1/namespaces/deployment-2876/deployments/test-rolling-update-deployment 1a9561e0-fac0-424d-8b0b-76aac2edc4f7 8819 1 2020-03-26 16:53:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-03-26 16:53:06 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 16:53:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0032e8fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-26 16:53:06 +0000 UTC,LastTransitionTime:2020-03-26 16:53:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-03-26 16:53:08 +0000 UTC,LastTransitionTime:2020-03-26 16:53:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 16:53:08.632: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-2876 /apis/apps/v1/namespaces/deployment-2876/replicasets/test-rolling-update-deployment-59d5cb45c7 67289d17-ed87-4a08-a724-d29c34538885 8808 1 2020-03-26 16:53:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1a9561e0-fac0-424d-8b0b-76aac2edc4f7 0xc0032e9537 0xc0032e9538}] []  [{kube-controller-manager Update apps/v1 2020-03-26 16:53:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 97 57 53 54 49 101 48 45 102 97 99 48 45 52 50 52 100 45 56 98 48 98 45 55 54 97 97 99 50 101 100 99 52 102 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0032e95c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:53:08.632: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 26 16:53:08.632: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2876 /apis/apps/v1/namespaces/deployment-2876/replicasets/test-rolling-update-controller 96a3d7bd-0fee-43fa-9004-2776b01b0991 8817 2 2020-03-26 16:53:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1a9561e0-fac0-424d-8b0b-76aac2edc4f7 0xc0032e942f 0xc0032e9440}] []  [{e2e.test Update apps/v1 2020-03-26 16:53:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 16:53:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 97 57 53 54 49 101 48 45 102 97 99 48 45 52 50 52 100 45 56 98 48 98 45 55 54 97 97 99 50 101 100 99 52 102 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032e94d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:53:08.644: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-dwgbc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-dwgbc test-rolling-update-deployment-59d5cb45c7- deployment-2876 /api/v1/namespaces/deployment-2876/pods/test-rolling-update-deployment-59d5cb45c7-dwgbc a465d5ac-774f-436c-9785-218286352e79 8807 0 2020-03-26 16:53:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 67289d17-ed87-4a08-a724-d29c34538885 0xc00069c767 0xc00069c768}] []  [{kube-controller-manager Update v1 2020-03-26 16:53:06 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 55 50 56 57 100 49 55 45 101 100 56 55 45 52 97 48 56 45 97 55 50 52 45 100 50 57 99 51 52 53 51 56 56 56 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:53:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 48 46 50 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sjr8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sjr8h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sjr8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:53:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:53:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:53:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:53:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:10.244.0.22,StartTime:2020-03-26 16:53:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:53:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:cri-o://037deb15d3106bb9b018b3c83fe198de89fd79f2b6fa2827b1cbb7b10abb3824,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:53:08.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2876" for this suite.

â€¢ [SLOW TEST:7.337 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":56,"skipped":1057,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:53:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 16:53:08.826: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:53:15.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6003" for this suite.

â€¢ [SLOW TEST:6.592 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":275,"completed":57,"skipped":1061,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:53:15.252: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3222
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3222
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3222
Mar 26 16:53:15.447: INFO: Found 0 stateful pods, waiting for 1
Mar 26 16:53:25.456: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 26 16:53:25.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:53:25.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:53:25.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:53:25.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 16:53:25.751: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 16:53:35.760: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 16:53:35.760: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 16:53:35.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999692s
Mar 26 16:53:36.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994707067s
Mar 26 16:53:37.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990512741s
Mar 26 16:53:38.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983690616s
Mar 26 16:53:39.808: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978803062s
Mar 26 16:53:40.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969242997s
Mar 26 16:53:41.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962797273s
Mar 26 16:53:42.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.956467097s
Mar 26 16:53:43.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94828648s
Mar 26 16:53:44.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.167522ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3222
Mar 26 16:53:45.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:53:46.080: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:53:46.080: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:53:46.080: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:53:46.083: INFO: Found 1 stateful pods, waiting for 3
Mar 26 16:53:56.101: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:53:56.101: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 16:53:56.101: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 26 16:53:56.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:53:56.409: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:53:56.409: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:53:56.409: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 16:53:56.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:53:56.620: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:53:56.621: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:53:56.621: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 16:53:56.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 16:53:56.801: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 16:53:56.801: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 16:53:56.801: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 16:53:56.801: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 16:53:56.807: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 26 16:54:06.823: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 16:54:06.823: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 16:54:06.823: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 16:54:06.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999299s
Mar 26 16:54:07.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990426865s
Mar 26 16:54:08.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981590883s
Mar 26 16:54:09.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.940182105s
Mar 26 16:54:10.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.93290098s
Mar 26 16:54:11.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.923431507s
Mar 26 16:54:12.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.917532554s
Mar 26 16:54:13.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.911279833s
Mar 26 16:54:14.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.904501133s
Mar 26 16:54:15.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 898.655145ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3222
Mar 26 16:54:16.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:54:17.137: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:54:17.137: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:54:17.137: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:54:17.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:54:17.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:54:17.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:54:17.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:54:17.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-3222 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 16:54:17.448: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 16:54:17.448: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 16:54:17.448: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 16:54:17.448: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 16:54:37.493: INFO: Deleting all statefulset in ns statefulset-3222
Mar 26 16:54:37.498: INFO: Scaling statefulset ss to 0
Mar 26 16:54:37.508: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 16:54:37.510: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:54:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3222" for this suite.

â€¢ [SLOW TEST:82.288 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":275,"completed":58,"skipped":1065,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:54:37.555: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 16:54:37.724: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:54:40.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2933" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":275,"completed":59,"skipped":1081,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:54:40.514: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Mar 26 16:54:40.729: INFO: Waiting up to 5m0s for pod "client-containers-056416c9-a9c1-41b6-976c-1048c5b05594" in namespace "containers-3529" to be "Succeeded or Failed"
Mar 26 16:54:40.746: INFO: Pod "client-containers-056416c9-a9c1-41b6-976c-1048c5b05594": Phase="Pending", Reason="", readiness=false. Elapsed: 16.329223ms
Mar 26 16:54:42.802: INFO: Pod "client-containers-056416c9-a9c1-41b6-976c-1048c5b05594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072426756s
Mar 26 16:54:44.808: INFO: Pod "client-containers-056416c9-a9c1-41b6-976c-1048c5b05594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078188077s
STEP: Saw pod success
Mar 26 16:54:44.809: INFO: Pod "client-containers-056416c9-a9c1-41b6-976c-1048c5b05594" satisfied condition "Succeeded or Failed"
Mar 26 16:54:44.814: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod client-containers-056416c9-a9c1-41b6-976c-1048c5b05594 container test-container: <nil>
STEP: delete the pod
Mar 26 16:54:44.867: INFO: Waiting for pod client-containers-056416c9-a9c1-41b6-976c-1048c5b05594 to disappear
Mar 26 16:54:44.877: INFO: Pod client-containers-056416c9-a9c1-41b6-976c-1048c5b05594 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:54:44.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3529" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":275,"completed":60,"skipped":1085,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:54:44.887: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5889
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 16:54:45.081: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 16:54:45.122: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 16:54:47.130: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 16:54:49.131: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:54:51.131: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:54:53.132: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:54:55.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:54:57.136: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:54:59.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:55:01.127: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 16:55:01.138: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 26 16:55:03.143: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 26 16:55:05.146: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 26 16:55:09.211: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.80 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5889 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:55:09.211: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:55:10.452: INFO: Found all expected endpoints: [netserver-0]
Mar 26 16:55:10.456: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5889 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:55:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:55:11.538: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:55:11.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5889" for this suite.

â€¢ [SLOW TEST:26.664 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":61,"skipped":1087,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:55:11.552: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-62ce304f-747c-4fef-8adc-29cd955851d8
STEP: Creating secret with name secret-projected-all-test-volume-71b4dca5-f31f-470c-8468-e01f9bff9b3b
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 26 16:55:11.721: INFO: Waiting up to 5m0s for pod "projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c" in namespace "projected-814" to be "Succeeded or Failed"
Mar 26 16:55:11.723: INFO: Pod "projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748123ms
Mar 26 16:55:13.726: INFO: Pod "projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004885848s
STEP: Saw pod success
Mar 26 16:55:13.726: INFO: Pod "projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c" satisfied condition "Succeeded or Failed"
Mar 26 16:55:13.728: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 26 16:55:13.753: INFO: Waiting for pod projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c to disappear
Mar 26 16:55:13.754: INFO: Pod projected-volume-e6f70ec6-18f7-445b-8934-a963d2dad99c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:55:13.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-814" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":275,"completed":62,"skipped":1088,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:55:13.761: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 26 16:55:14.286: INFO: Pod name wrapped-volume-race-870cb37d-0eb5-493d-b5a3-e73b9ce5ed6d: Found 0 pods out of 5
Mar 26 16:55:19.293: INFO: Pod name wrapped-volume-race-870cb37d-0eb5-493d-b5a3-e73b9ce5ed6d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-870cb37d-0eb5-493d-b5a3-e73b9ce5ed6d in namespace emptydir-wrapper-3387, will wait for the garbage collector to delete the pods
Mar 26 16:55:31.414: INFO: Deleting ReplicationController wrapped-volume-race-870cb37d-0eb5-493d-b5a3-e73b9ce5ed6d took: 25.822068ms
Mar 26 16:55:31.915: INFO: Terminating ReplicationController wrapped-volume-race-870cb37d-0eb5-493d-b5a3-e73b9ce5ed6d pods took: 500.410188ms
STEP: Creating RC which spawns configmap-volume pods
Mar 26 16:55:38.468: INFO: Pod name wrapped-volume-race-6ea58f83-9e3e-4e12-9449-4db3714f07be: Found 1 pods out of 5
Mar 26 16:55:43.485: INFO: Pod name wrapped-volume-race-6ea58f83-9e3e-4e12-9449-4db3714f07be: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6ea58f83-9e3e-4e12-9449-4db3714f07be in namespace emptydir-wrapper-3387, will wait for the garbage collector to delete the pods
Mar 26 16:55:53.585: INFO: Deleting ReplicationController wrapped-volume-race-6ea58f83-9e3e-4e12-9449-4db3714f07be took: 7.768058ms
Mar 26 16:55:53.786: INFO: Terminating ReplicationController wrapped-volume-race-6ea58f83-9e3e-4e12-9449-4db3714f07be pods took: 200.292862ms
STEP: Creating RC which spawns configmap-volume pods
Mar 26 16:56:08.406: INFO: Pod name wrapped-volume-race-5a2ffb9d-91a9-4ee5-9707-d0ddc3ea9abe: Found 0 pods out of 5
Mar 26 16:56:13.414: INFO: Pod name wrapped-volume-race-5a2ffb9d-91a9-4ee5-9707-d0ddc3ea9abe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5a2ffb9d-91a9-4ee5-9707-d0ddc3ea9abe in namespace emptydir-wrapper-3387, will wait for the garbage collector to delete the pods
Mar 26 16:56:23.514: INFO: Deleting ReplicationController wrapped-volume-race-5a2ffb9d-91a9-4ee5-9707-d0ddc3ea9abe took: 22.929302ms
Mar 26 16:56:23.915: INFO: Terminating ReplicationController wrapped-volume-race-5a2ffb9d-91a9-4ee5-9707-d0ddc3ea9abe pods took: 400.992689ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:56:34.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3387" for this suite.

â€¢ [SLOW TEST:80.488 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":275,"completed":63,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:56:34.251: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-9458bd06-59f5-43ad-b309-f9eb9432be2a
STEP: Creating a pod to test consume configMaps
Mar 26 16:56:34.407: INFO: Waiting up to 5m0s for pod "pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368" in namespace "configmap-2863" to be "Succeeded or Failed"
Mar 26 16:56:34.409: INFO: Pod "pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412143ms
Mar 26 16:56:36.421: INFO: Pod "pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014343039s
STEP: Saw pod success
Mar 26 16:56:36.421: INFO: Pod "pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368" satisfied condition "Succeeded or Failed"
Mar 26 16:56:36.427: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 16:56:36.455: INFO: Waiting for pod pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368 to disappear
Mar 26 16:56:36.457: INFO: Pod pod-configmaps-c029aae6-ce92-4141-9e09-df0e93012368 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:56:36.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2863" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":64,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:56:36.463: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 26 16:56:36.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3827'
Mar 26 16:56:36.816: INFO: stderr: ""
Mar 26 16:56:36.816: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Mar 26 16:56:36.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete pods e2e-test-httpd-pod --namespace=kubectl-3827'
Mar 26 16:56:48.193: INFO: stderr: ""
Mar 26 16:56:48.193: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:56:48.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3827" for this suite.

â€¢ [SLOW TEST:11.772 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":275,"completed":65,"skipped":1148,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:56:48.236: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6219 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6219;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6219 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6219;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6219.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6219.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6219.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6219.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6219.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6219.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6219.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6219.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6219.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 54.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.54_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6219 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6219;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6219 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6219;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6219.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6219.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6219.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6219.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6219.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6219.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6219.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6219.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6219.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6219.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 54.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.54_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 16:56:52.725: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.729: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-6219 from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.734: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6219 from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.737: INFO: Unable to read wheezy_udp@dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.739: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.748: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.750: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.774: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.779: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.783: INFO: Unable to read jessie_udp@dns-test-service.dns-6219 from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-6219 from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.789: INFO: Unable to read jessie_udp@dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.794: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6219.svc from pod dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc: the server could not find the requested resource (get pods dns-test-936a091a-4463-4225-a0a7-a17b048dfadc)
Mar 26 16:56:52.809: INFO: Lookups using dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6219 wheezy_tcp@dns-test-service.dns-6219 wheezy_udp@dns-test-service.dns-6219.svc wheezy_tcp@dns-test-service.dns-6219.svc wheezy_udp@_http._tcp.dns-test-service.dns-6219.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6219.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6219 jessie_tcp@dns-test-service.dns-6219 jessie_udp@dns-test-service.dns-6219.svc jessie_tcp@dns-test-service.dns-6219.svc jessie_udp@_http._tcp.dns-test-service.dns-6219.svc jessie_tcp@_http._tcp.dns-test-service.dns-6219.svc]

Mar 26 16:56:57.889: INFO: DNS probes using dns-6219/dns-test-936a091a-4463-4225-a0a7-a17b048dfadc succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:56:58.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6219" for this suite.

â€¢ [SLOW TEST:10.024 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":275,"completed":66,"skipped":1148,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:56:58.262: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8819
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 26 16:56:58.536: INFO: Waiting up to 5m0s for pod "pod-1fca5a7a-e469-4123-9c74-87a326929307" in namespace "emptydir-8819" to be "Succeeded or Failed"
Mar 26 16:56:58.549: INFO: Pod "pod-1fca5a7a-e469-4123-9c74-87a326929307": Phase="Pending", Reason="", readiness=false. Elapsed: 12.849144ms
Mar 26 16:57:00.554: INFO: Pod "pod-1fca5a7a-e469-4123-9c74-87a326929307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018170253s
Mar 26 16:57:02.560: INFO: Pod "pod-1fca5a7a-e469-4123-9c74-87a326929307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023848259s
STEP: Saw pod success
Mar 26 16:57:02.560: INFO: Pod "pod-1fca5a7a-e469-4123-9c74-87a326929307" satisfied condition "Succeeded or Failed"
Mar 26 16:57:02.569: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-1fca5a7a-e469-4123-9c74-87a326929307 container test-container: <nil>
STEP: delete the pod
Mar 26 16:57:02.599: INFO: Waiting for pod pod-1fca5a7a-e469-4123-9c74-87a326929307 to disappear
Mar 26 16:57:02.604: INFO: Pod pod-1fca5a7a-e469-4123-9c74-87a326929307 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:57:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8819" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":67,"skipped":1148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:57:02.626: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:57:02.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2372" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":275,"completed":68,"skipped":1175,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:57:02.964: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 16:57:03.113: INFO: Creating deployment "webserver-deployment"
Mar 26 16:57:03.127: INFO: Waiting for observed generation 1
Mar 26 16:57:05.163: INFO: Waiting for all required pods to come up
Mar 26 16:57:05.268: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 26 16:57:11.321: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 26 16:57:11.340: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 26 16:57:11.353: INFO: Updating deployment webserver-deployment
Mar 26 16:57:11.353: INFO: Waiting for observed generation 2
Mar 26 16:57:13.376: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 26 16:57:13.378: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 26 16:57:13.387: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 26 16:57:13.406: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 26 16:57:13.406: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 26 16:57:13.414: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 26 16:57:13.420: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 26 16:57:13.420: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 26 16:57:13.451: INFO: Updating deployment webserver-deployment
Mar 26 16:57:13.451: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 26 16:57:13.505: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 26 16:57:15.552: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 16:57:15.650: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1440 /apis/apps/v1/namespaces/deployment-1440/deployments/webserver-deployment aeff49d4-e5b3-4cc7-9fb7-42595ee7a015 11143 3 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003760838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-26 16:57:13 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-03-26 16:57:14 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 26 16:57:15.669: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-1440 /apis/apps/v1/namespaces/deployment-1440/replicasets/webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 11128 3 2020-03-26 16:57:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment aeff49d4-e5b3-4cc7-9fb7-42595ee7a015 0xc003760cb7 0xc003760cb8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 102 102 52 57 100 52 45 101 53 98 51 45 52 99 99 55 45 57 102 98 55 45 52 50 53 57 53 101 101 55 97 48 49 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003760d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:57:15.669: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 26 16:57:15.669: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-1440 /apis/apps/v1/namespaces/deployment-1440/replicasets/webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 11142 3 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment aeff49d4-e5b3-4cc7-9fb7-42595ee7a015 0xc003760d97 0xc003760d98}] []  [{kube-controller-manager Update apps/v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 102 102 52 57 100 52 45 101 53 98 51 45 52 99 99 55 45 57 102 98 55 45 52 50 53 57 53 101 101 55 97 48 49 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003760e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 26 16:57:15.738: INFO: Pod "webserver-deployment-6676bcd6d4-4jkww" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4jkww webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-4jkww 411e6688-4d74-4e1f-a471-8f5c793d5cc0 11126 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc0037782f7 0xc0037782f8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.739: INFO: Pod "webserver-deployment-6676bcd6d4-62hlk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-62hlk webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-62hlk e88c238b-6dc3-4579-8523-edd99f147718 11030 0 2020-03-26 16:57:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778420 0xc003778421}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.739: INFO: Pod "webserver-deployment-6676bcd6d4-6qddc" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-6qddc webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-6qddc 12122274-88e3-4398-9693-2baad5201df6 11146 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc0037785b7 0xc0037785b8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.740: INFO: Pod "webserver-deployment-6676bcd6d4-j2xfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-j2xfh webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-j2xfh aa206f6c-5706-40a1-8a3a-e3ffb0bce8b6 11172 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778757 0xc003778758}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.740: INFO: Pod "webserver-deployment-6676bcd6d4-kg8tf" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kg8tf webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-kg8tf 88bf0db9-63ea-4687-bdba-db8b7e930505 11057 0 2020-03-26 16:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc0037788f7 0xc0037788f8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:12 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.740: INFO: Pod "webserver-deployment-6676bcd6d4-mqglh" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-mqglh webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-mqglh a2d22518-9d77-4b01-b3d7-5fee74d53605 11018 0 2020-03-26 16:57:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778a97 0xc003778a98}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.741: INFO: Pod "webserver-deployment-6676bcd6d4-p527p" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-p527p webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-p527p af357d91-9d38-473a-82c0-86a300b959cc 11025 0 2020-03-26 16:57:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778c37 0xc003778c38}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.741: INFO: Pod "webserver-deployment-6676bcd6d4-pfjd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pfjd8 webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-pfjd8 3dd89443-9198-4e68-b18b-252efefeae0e 11056 0 2020-03-26 16:57:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778dd7 0xc003778dd8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:12 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.741: INFO: Pod "webserver-deployment-6676bcd6d4-qxl28" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qxl28 webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-qxl28 eda0380e-e9da-4e16-9a4c-02927c47e42f 11184 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003778f77 0xc003778f78}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.742: INFO: Pod "webserver-deployment-6676bcd6d4-r4b9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-r4b9h webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-r4b9h bfc99e84-71bf-41d8-8e01-6090b4eefe99 11182 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003779117 0xc003779118}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.742: INFO: Pod "webserver-deployment-6676bcd6d4-sbbzl" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-sbbzl webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-sbbzl 906bb679-a5fa-4f19-8a95-bd485e5a083f 11181 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc0037792b7 0xc0037792b8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.742: INFO: Pod "webserver-deployment-6676bcd6d4-sn46h" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-sn46h webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-sn46h 5a5b9d8d-9528-4a93-bd18-af5deb2315ad 11120 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003779457 0xc003779458}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.742: INFO: Pod "webserver-deployment-6676bcd6d4-xfl2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-xfl2z webserver-deployment-6676bcd6d4- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-6676bcd6d4-xfl2z b159dd86-858c-4b3b-8bd7-59ea36ceca7d 11081 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 4c762c8a-6a82-43a6-bc99-e61500d06910 0xc003779580 0xc003779581}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 99 55 54 50 99 56 97 45 54 97 56 50 45 52 51 97 54 45 98 99 57 57 45 101 54 49 53 48 48 100 48 54 57 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.742: INFO: Pod "webserver-deployment-84855cf797-2cmkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2cmkg webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-2cmkg 967a5384-271c-48c1-b468-22864707db21 11111 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779717 0xc003779718}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.743: INFO: Pod "webserver-deployment-84855cf797-2vr4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2vr4r webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-2vr4r 5956dbe1-f0e8-41a8-8117-56c55ec8c186 11168 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779897 0xc003779898}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.743: INFO: Pod "webserver-deployment-84855cf797-768rg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-768rg webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-768rg 7a1aef83-01c4-4477-8640-b3845f90d1f0 11082 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779a17 0xc003779a18}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.743: INFO: Pod "webserver-deployment-84855cf797-7d6vc" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7d6vc webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-7d6vc eaab067e-ec0d-49ce-90cc-5ead53e17776 10971 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779b97 0xc003779b98}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:09 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 48 46 51 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:10.244.0.35,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://2c569c3ab6b20f12f590a930196da3a4debc0b2ac7737cec572870b423b2ef7b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.744: INFO: Pod "webserver-deployment-84855cf797-bhgmm" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bhgmm webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-bhgmm 0f59a7d6-eb38-409b-9ec2-3936a4fa3479 11125 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779d30 0xc003779d31}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.753: INFO: Pod "webserver-deployment-84855cf797-bz4jr" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bz4jr webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-bz4jr 1a015e71-95b2-41e6-9503-959bea0ed103 11094 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779e50 0xc003779e51}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.753: INFO: Pod "webserver-deployment-84855cf797-cc5lm" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cc5lm webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-cc5lm cf3b5e31-22ad-4f71-8d8a-2d8d8e2e24da 10968 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003779fc7 0xc003779fc8}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:09 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 48 46 51 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:10.244.0.34,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://64003173a1632001e3d5c2094e979ff75bda4ea59c9bf6e5da4b4bd34eb57f5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.754: INFO: Pod "webserver-deployment-84855cf797-cwpqx" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cwpqx webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-cwpqx d557a384-9e5f-49c0-8ad4-fe4e682ea42e 10983 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846160 0xc003846161}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 57 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.97,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4d7a5088bde1a6463652e7cf7aeffc8609a206ca64961cd60b78757fb8dec1fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.754: INFO: Pod "webserver-deployment-84855cf797-gw8wh" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-gw8wh webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-gw8wh a2d24077-3fc3-4d16-b233-efeb4161fe67 11151 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc0038462f0 0xc0038462f1}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.755: INFO: Pod "webserver-deployment-84855cf797-h5drp" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-h5drp webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-h5drp 93c0c89b-8093-4a87-a1fb-b0fed2ed95c6 10981 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846467 0xc003846468}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 57 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.98,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://1a28a29cdd0bb39ffaf78a06e1d8ea1cf016784ad7b183f1a01d6ea3f1ca7a95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.755: INFO: Pod "webserver-deployment-84855cf797-jhqh4" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jhqh4 webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-jhqh4 01c3d725-a776-49e3-9357-872491e6e2f8 10987 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846600 0xc003846601}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 57 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.96,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://faf5d24192223eb2a9b457d0998d8175f6802f672990f7bfcf903cd2598a2366,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.755: INFO: Pod "webserver-deployment-84855cf797-kzhhp" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-kzhhp webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-kzhhp e8210c50-0ba9-41b4-a801-c7b2e9228c67 11155 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846790 0xc003846791}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.755: INFO: Pod "webserver-deployment-84855cf797-pcmrg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-pcmrg webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-pcmrg 26f7e408-c0b3-4c24-b750-1f9ce008c331 11175 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846907 0xc003846908}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.756: INFO: Pod "webserver-deployment-84855cf797-qmr24" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qmr24 webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-qmr24 d952d742-aaa2-4827-8a13-438f42d11b66 10988 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846a87 0xc003846a88}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 57 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.99,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://373caa6ef327886ec9eba94a1d49d65016ded1ba7a28067dce12946ab8141eba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.756: INFO: Pod "webserver-deployment-84855cf797-r2zbf" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-r2zbf webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-r2zbf 1f80833b-3694-4d1e-9f32-a66a93efebf3 11178 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846c20 0xc003846c21}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.756: INFO: Pod "webserver-deployment-84855cf797-rfr2z" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rfr2z webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-rfr2z 874a6eb6-7c6f-4bb5-b3f8-2ab867a6595e 10947 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846d97 0xc003846d98}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 48 46 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:10.244.0.33,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f0c70a2e939a38fabd5e73b3c6d6fc44eb74669b8b8dab6ef070bcd83dd102d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.757: INFO: Pod "webserver-deployment-84855cf797-rknf6" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rknf6 webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-rknf6 e5acc95a-53ea-460a-a249-149335f44992 10951 0 2020-03-26 16:57:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003846f30 0xc003846f31}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 48 46 51 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:10.244.0.32,StartTime:2020-03-26 16:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 16:57:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://95de9032428db79f2d0bf43c374d1b45e00e133f371b84dc77a3e977d8b2d324,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.757: INFO: Pod "webserver-deployment-84855cf797-s5d56" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-s5d56 webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-s5d56 f575e403-ca42-445e-b36e-685cb95f6069 11119 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc0038470c0 0xc0038470c1}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.767: INFO: Pod "webserver-deployment-84855cf797-w2sf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-w2sf5 webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-w2sf5 759e56f3-1f8e-4a39-90a6-4c54e7eaa915 11138 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc0038471e0 0xc0038471e1}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 16:57:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 16:57:15.767: INFO: Pod "webserver-deployment-84855cf797-zbl9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-zbl9l webserver-deployment-84855cf797- deployment-1440 /api/v1/namespaces/deployment-1440/pods/webserver-deployment-84855cf797-zbl9l e266887b-db00-4463-840a-16bccef10905 11183 0 2020-03-26 16:57:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 2b242bc5-6e00-429a-a560-771a0f7181b0 0xc003847357 0xc003847358}] []  [{kube-controller-manager Update v1 2020-03-26 16:57:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 98 50 52 50 98 99 53 45 54 101 48 48 45 52 50 57 97 45 97 53 54 48 45 55 55 49 97 48 102 55 49 56 49 98 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 16:57:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbx29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbx29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbx29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-vf6bys,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 16:57:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.71,PodIP:,StartTime:2020-03-26 16:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:57:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1440" for this suite.

â€¢ [SLOW TEST:12.864 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":275,"completed":69,"skipped":1182,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:57:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-b4480cf3-1451-48b5-a9da-62697517464a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:57:16.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8707" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":275,"completed":70,"skipped":1191,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:57:16.223: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9345
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 26 16:57:16.938: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:57:34.336: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:57:53.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9345" for this suite.

â€¢ [SLOW TEST:37.599 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":275,"completed":71,"skipped":1191,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:57:53.816: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-990
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 16:57:53.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 16:57:54.012: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 16:57:56.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:57:58.015: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:00.015: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:02.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:04.018: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:06.024: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:08.018: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:10.019: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:12.018: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 16:58:14.020: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 16:58:14.033: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 26 16:58:16.112: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.114:8080/dial?request=hostname&protocol=udp&host=10.244.1.113&port=8081&tries=1'] Namespace:pod-network-test-990 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:58:16.112: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:58:16.382: INFO: Waiting for responses: map[]
Mar 26 16:58:16.384: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.114:8080/dial?request=hostname&protocol=udp&host=10.244.0.49&port=8081&tries=1'] Namespace:pod-network-test-990 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 16:58:16.384: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 16:58:16.453: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 16:58:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-990" for this suite.

â€¢ [SLOW TEST:22.657 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":275,"completed":72,"skipped":1203,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 16:58:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-6928d6ae-2f9a-4513-9961-37ac8b579f15 in namespace container-probe-3684
Mar 26 16:58:18.670: INFO: Started pod test-webserver-6928d6ae-2f9a-4513-9961-37ac8b579f15 in namespace container-probe-3684
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 16:58:18.672: INFO: Initial restart count of pod test-webserver-6928d6ae-2f9a-4513-9961-37ac8b579f15 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:02:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3684" for this suite.

â€¢ [SLOW TEST:243.042 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":73,"skipped":1207,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:02:19.534: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:02:19.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1" in namespace "projected-610" to be "Succeeded or Failed"
Mar 26 17:02:19.912: INFO: Pod "downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.975785ms
Mar 26 17:02:21.919: INFO: Pod "downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013002635s
Mar 26 17:02:23.927: INFO: Pod "downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021102605s
STEP: Saw pod success
Mar 26 17:02:23.927: INFO: Pod "downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1" satisfied condition "Succeeded or Failed"
Mar 26 17:02:23.933: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1 container client-container: <nil>
STEP: delete the pod
Mar 26 17:02:23.970: INFO: Waiting for pod downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1 to disappear
Mar 26 17:02:23.974: INFO: Pod downwardapi-volume-5944bc34-b9c2-4136-81ae-1b2ff9da64f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:02:23.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-610" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":275,"completed":74,"skipped":1215,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:02:23.982: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-b79ae217-803e-46a2-9a26-416f3ca15fe0
STEP: Creating a pod to test consume configMaps
Mar 26 17:02:24.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e" in namespace "configmap-4521" to be "Succeeded or Failed"
Mar 26 17:02:24.149: INFO: Pod "pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.872654ms
Mar 26 17:02:26.160: INFO: Pod "pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021159831s
Mar 26 17:02:28.165: INFO: Pod "pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025646296s
STEP: Saw pod success
Mar 26 17:02:28.165: INFO: Pod "pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e" satisfied condition "Succeeded or Failed"
Mar 26 17:02:28.168: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:02:28.195: INFO: Waiting for pod pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e to disappear
Mar 26 17:02:28.200: INFO: Pod pod-configmaps-39c2374e-7eba-424d-8207-1be516f03f3e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:02:28.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4521" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":75,"skipped":1215,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:02:28.208: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3566
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-29381390-e90e-45d7-822e-035ddeb48ec8
STEP: Creating secret with name s-test-opt-upd-5639295c-d569-48cb-8fbd-6ea018aac2d8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-29381390-e90e-45d7-822e-035ddeb48ec8
STEP: Updating secret s-test-opt-upd-5639295c-d569-48cb-8fbd-6ea018aac2d8
STEP: Creating secret with name s-test-opt-create-c5fd7bbb-cf18-4635-912e-d0596075c526
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:03:39.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3566" for this suite.

â€¢ [SLOW TEST:70.898 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":76,"skipped":1222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:03:39.110: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-251e5eba-8293-48c8-b460-96eb9c610560
STEP: Creating a pod to test consume configMaps
Mar 26 17:03:39.293: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a" in namespace "configmap-345" to be "Succeeded or Failed"
Mar 26 17:03:39.302: INFO: Pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.803128ms
Mar 26 17:03:41.306: INFO: Pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012656387s
Mar 26 17:03:43.313: INFO: Pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019383526s
Mar 26 17:03:45.317: INFO: Pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023203877s
STEP: Saw pod success
Mar 26 17:03:45.317: INFO: Pod "pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a" satisfied condition "Succeeded or Failed"
Mar 26 17:03:45.319: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:03:45.366: INFO: Waiting for pod pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a to disappear
Mar 26 17:03:45.371: INFO: Pod pod-configmaps-2f5ea204-f5e1-4109-9117-024b5c43773a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:03:45.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-345" for this suite.

â€¢ [SLOW TEST:6.268 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":77,"skipped":1248,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:03:45.379: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-c8c167a8-cb02-425f-8965-db6a2504a75d
STEP: Creating a pod to test consume configMaps
Mar 26 17:03:45.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0" in namespace "configmap-3763" to be "Succeeded or Failed"
Mar 26 17:03:45.584: INFO: Pod "pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.639492ms
Mar 26 17:03:47.588: INFO: Pod "pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023729787s
STEP: Saw pod success
Mar 26 17:03:47.588: INFO: Pod "pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0" satisfied condition "Succeeded or Failed"
Mar 26 17:03:47.590: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:03:47.622: INFO: Waiting for pod pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0 to disappear
Mar 26 17:03:47.627: INFO: Pod pod-configmaps-6b1c5f69-dac6-4a45-b9e3-ff43a4221be0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:03:47.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3763" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":78,"skipped":1269,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:03:47.642: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:03:48.478: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 17:03:50.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839028, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839028, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839028, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839028, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:03:53.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:03:53.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6851" for this suite.
STEP: Destroying namespace "webhook-6851-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.416 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":275,"completed":79,"skipped":1290,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:03:54.058: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:03:54.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23" in namespace "downward-api-1587" to be "Succeeded or Failed"
Mar 26 17:03:54.441: INFO: Pod "downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23": Phase="Pending", Reason="", readiness=false. Elapsed: 32.835753ms
Mar 26 17:03:56.445: INFO: Pod "downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036788133s
STEP: Saw pod success
Mar 26 17:03:56.445: INFO: Pod "downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23" satisfied condition "Succeeded or Failed"
Mar 26 17:03:56.448: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23 container client-container: <nil>
STEP: delete the pod
Mar 26 17:03:56.494: INFO: Waiting for pod downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23 to disappear
Mar 26 17:03:56.503: INFO: Pod downwardapi-volume-f52e632c-be5c-448e-98d7-42454cac5a23 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:03:56.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1587" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":80,"skipped":1295,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:03:56.516: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 26 17:04:02.873: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:02.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0326 17:04:02.873101      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7575" for this suite.

â€¢ [SLOW TEST:6.404 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":275,"completed":81,"skipped":1306,"failed":0}
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:03.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6745" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":275,"completed":82,"skipped":1306,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:03.325: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-540
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:04:03.644: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:09.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-540" for this suite.

â€¢ [SLOW TEST:6.432 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":275,"completed":83,"skipped":1315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:09.761: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-78ab71da-1b56-4993-9404-fc6290702216
STEP: Creating a pod to test consume configMaps
Mar 26 17:04:09.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3" in namespace "configmap-2218" to be "Succeeded or Failed"
Mar 26 17:04:09.916: INFO: Pod "pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029225ms
Mar 26 17:04:11.919: INFO: Pod "pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005470466s
Mar 26 17:04:13.925: INFO: Pod "pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010732s
STEP: Saw pod success
Mar 26 17:04:13.925: INFO: Pod "pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3" satisfied condition "Succeeded or Failed"
Mar 26 17:04:13.929: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:04:13.953: INFO: Waiting for pod pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3 to disappear
Mar 26 17:04:13.956: INFO: Pod pod-configmaps-f5ecc735-06a8-45c3-8615-dd4aef98e0a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:13.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2218" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":84,"skipped":1352,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:13.964: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5898
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:14.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5898" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":275,"completed":85,"skipped":1357,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:14.129: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-9170
STEP: creating replication controller nodeport-test in namespace services-9170
I0326 17:04:14.474866      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9170, replica count: 2
I0326 17:04:17.534820      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:04:20.535332      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:04:23.535: INFO: Creating new exec pod
I0326 17:04:23.535765      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:04:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9170 execpodgxhcv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 26 17:04:27.779: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 26 17:04:27.779: INFO: stdout: ""
Mar 26 17:04:27.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9170 execpodgxhcv -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.7 80'
Mar 26 17:04:27.969: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.7 80\nConnection to 10.32.0.7 80 port [tcp/http] succeeded!\n"
Mar 26 17:04:27.969: INFO: stdout: ""
Mar 26 17:04:27.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9170 execpodgxhcv -- /bin/sh -x -c nc -zv -t -w 2 10.107.34.127 31018'
Mar 26 17:04:28.147: INFO: stderr: "+ nc -zv -t -w 2 10.107.34.127 31018\nConnection to 10.107.34.127 31018 port [tcp/31018] succeeded!\n"
Mar 26 17:04:28.147: INFO: stdout: ""
Mar 26 17:04:28.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9170 execpodgxhcv -- /bin/sh -x -c nc -zv -t -w 2 10.107.34.71 31018'
Mar 26 17:04:28.474: INFO: stderr: "+ nc -zv -t -w 2 10.107.34.71 31018\nConnection to 10.107.34.71 31018 port [tcp/31018] succeeded!\n"
Mar 26 17:04:28.474: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:28.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9170" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:14.356 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":275,"completed":86,"skipped":1369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:04:29.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:04:32.121: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:04:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4329-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:38.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4459" for this suite.
STEP: Destroying namespace "webhook-4459-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":275,"completed":87,"skipped":1411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:38.417: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 17:04:41.646: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:41.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3904" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":275,"completed":88,"skipped":1434,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:41.705: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:47.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9580" for this suite.

â€¢ [SLOW TEST:5.500 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":275,"completed":89,"skipped":1449,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:47.208: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:04:47.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c" in namespace "downward-api-949" to be "Succeeded or Failed"
Mar 26 17:04:47.448: INFO: Pod "downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.212239ms
Mar 26 17:04:49.454: INFO: Pod "downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018778062s
STEP: Saw pod success
Mar 26 17:04:49.454: INFO: Pod "downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c" satisfied condition "Succeeded or Failed"
Mar 26 17:04:49.458: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c container client-container: <nil>
STEP: delete the pod
Mar 26 17:04:49.507: INFO: Waiting for pod downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c to disappear
Mar 26 17:04:49.511: INFO: Pod downwardapi-volume-824a2d7d-3998-4d07-9d11-18cecd702f1c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:49.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-949" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":90,"skipped":1455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:49.520: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:04:56.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9130" for this suite.

â€¢ [SLOW TEST:7.203 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":275,"completed":91,"skipped":1493,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:04:56.723: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:05:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6876" for this suite.

â€¢ [SLOW TEST:11.249 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":275,"completed":92,"skipped":1494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:05:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4089
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-258c302a-79c7-43b1-9479-f70e1b1ffacb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:05:10.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4089" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":93,"skipped":1545,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:05:10.259: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:05:11.002: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:05:14.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:05:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1583" for this suite.
STEP: Destroying namespace "webhook-1583-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":275,"completed":94,"skipped":1556,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:05:14.499: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-cksk
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:05:14.659: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cksk" in namespace "subpath-7837" to be "Succeeded or Failed"
Mar 26 17:05:14.663: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691219ms
Mar 26 17:05:16.666: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007527219s
Mar 26 17:05:18.674: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 4.014968516s
Mar 26 17:05:20.682: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 6.023657828s
Mar 26 17:05:22.691: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 8.032103429s
Mar 26 17:05:24.697: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 10.038116706s
Mar 26 17:05:26.705: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 12.046284423s
Mar 26 17:05:28.714: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 14.05534058s
Mar 26 17:05:30.720: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 16.06100673s
Mar 26 17:05:32.726: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 18.067187074s
Mar 26 17:05:34.731: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 20.072686508s
Mar 26 17:05:36.736: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Running", Reason="", readiness=true. Elapsed: 22.077499073s
Mar 26 17:05:38.742: INFO: Pod "pod-subpath-test-projected-cksk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.083279477s
STEP: Saw pod success
Mar 26 17:05:38.742: INFO: Pod "pod-subpath-test-projected-cksk" satisfied condition "Succeeded or Failed"
Mar 26 17:05:38.746: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-subpath-test-projected-cksk container test-container-subpath-projected-cksk: <nil>
STEP: delete the pod
Mar 26 17:05:38.768: INFO: Waiting for pod pod-subpath-test-projected-cksk to disappear
Mar 26 17:05:38.770: INFO: Pod pod-subpath-test-projected-cksk no longer exists
STEP: Deleting pod pod-subpath-test-projected-cksk
Mar 26 17:05:38.771: INFO: Deleting pod "pod-subpath-test-projected-cksk" in namespace "subpath-7837"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:05:38.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7837" for this suite.

â€¢ [SLOW TEST:24.281 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":275,"completed":95,"skipped":1566,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:05:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-36
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:05:55.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-36" for this suite.

â€¢ [SLOW TEST:17.206 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":275,"completed":96,"skipped":1566,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:05:55.989: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Mar 26 17:05:56.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-5981'
Mar 26 17:05:57.704: INFO: stderr: ""
Mar 26 17:05:57.704: INFO: stdout: "pod/pause created\n"
Mar 26 17:05:57.704: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 26 17:05:57.704: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5981" to be "running and ready"
Mar 26 17:05:57.719: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.475351ms
Mar 26 17:05:59.725: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.020579405s
Mar 26 17:05:59.725: INFO: Pod "pause" satisfied condition "running and ready"
Mar 26 17:05:59.725: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 26 17:05:59.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 label pods pause testing-label=testing-label-value --namespace=kubectl-5981'
Mar 26 17:05:59.846: INFO: stderr: ""
Mar 26 17:05:59.846: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 26 17:05:59.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pod pause -L testing-label --namespace=kubectl-5981'
Mar 26 17:05:59.933: INFO: stderr: ""
Mar 26 17:05:59.933: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 26 17:05:59.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 label pods pause testing-label- --namespace=kubectl-5981'
Mar 26 17:06:00.028: INFO: stderr: ""
Mar 26 17:06:00.028: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 26 17:06:00.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pod pause -L testing-label --namespace=kubectl-5981'
Mar 26 17:06:00.120: INFO: stderr: ""
Mar 26 17:06:00.120: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Mar 26 17:06:00.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-5981'
Mar 26 17:06:00.217: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:06:00.217: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 26 17:06:00.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get rc,svc -l name=pause --no-headers --namespace=kubectl-5981'
Mar 26 17:06:00.344: INFO: stderr: "No resources found in kubectl-5981 namespace.\n"
Mar 26 17:06:00.344: INFO: stdout: ""
Mar 26 17:06:00.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -l name=pause --namespace=kubectl-5981 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 17:06:00.461: INFO: stderr: ""
Mar 26 17:06:00.461: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:06:00.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5981" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":275,"completed":97,"skipped":1568,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:06:00.471: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 26 17:06:00.661: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Mar 26 17:06:01.029: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar 26 17:06:03.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:05.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:07.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:09.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:11.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:13.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:15.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:17.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:19.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:21.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:23.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839161, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:06:27.087: INFO: Waited 1.838899729s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:06:27.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7380" for this suite.

â€¢ [SLOW TEST:27.243 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":275,"completed":98,"skipped":1572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:06:27.720: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-80896699-2208-4e13-b01a-75e658f154ec
STEP: Creating a pod to test consume configMaps
Mar 26 17:06:28.275: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade" in namespace "projected-9910" to be "Succeeded or Failed"
Mar 26 17:06:28.278: INFO: Pod "pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.545001ms
Mar 26 17:06:30.287: INFO: Pod "pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011114417s
Mar 26 17:06:32.291: INFO: Pod "pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015109516s
STEP: Saw pod success
Mar 26 17:06:32.291: INFO: Pod "pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade" satisfied condition "Succeeded or Failed"
Mar 26 17:06:32.293: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:06:32.324: INFO: Waiting for pod pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade to disappear
Mar 26 17:06:32.331: INFO: Pod pod-projected-configmaps-8b547bc3-1940-4f12-a3b5-be3b4239bade no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:06:32.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9910" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":99,"skipped":1596,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:06:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:06:32.560: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4124185c-3c2f-4870-8cfa-a59e1069508e" in namespace "security-context-test-3468" to be "Succeeded or Failed"
Mar 26 17:06:32.572: INFO: Pod "busybox-user-65534-4124185c-3c2f-4870-8cfa-a59e1069508e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.894547ms
Mar 26 17:06:34.577: INFO: Pod "busybox-user-65534-4124185c-3c2f-4870-8cfa-a59e1069508e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01688131s
Mar 26 17:06:34.577: INFO: Pod "busybox-user-65534-4124185c-3c2f-4870-8cfa-a59e1069508e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:06:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3468" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":100,"skipped":1603,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:06:34.600: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-2284
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 17:06:34.834: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 17:06:34.904: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:06:36.917: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:38.911: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:40.908: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:42.911: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:44.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:46.934: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:48.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:50.908: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:52.913: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:06:54.909: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 17:06:54.915: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 26 17:06:57.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.138:8080/dial?request=hostname&protocol=http&host=10.244.1.137&port=8080&tries=1'] Namespace:pod-network-test-2284 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:06:57.003: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:06:57.251: INFO: Waiting for responses: map[]
Mar 26 17:06:57.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.138:8080/dial?request=hostname&protocol=http&host=10.244.0.62&port=8080&tries=1'] Namespace:pod-network-test-2284 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:06:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:06:57.342: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:06:57.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2284" for this suite.

â€¢ [SLOW TEST:22.752 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":275,"completed":101,"skipped":1618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:06:57.355: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:08.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9507" for this suite.

â€¢ [SLOW TEST:11.499 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":275,"completed":102,"skipped":1686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:08.858: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:07:09.637: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:07:11.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839229, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839229, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839229, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839229, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:07:14.742: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 26 17:07:16.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 attach --namespace=webhook-256 to-be-attached-pod -i -c=container1'
Mar 26 17:07:16.945: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:16.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-256" for this suite.
STEP: Destroying namespace "webhook-256-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.287 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":275,"completed":103,"skipped":1716,"failed":0}
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:17.145: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 26 17:07:19.966: INFO: Successfully updated pod "adopt-release-wg9lw"
STEP: Checking that the Job readopts the Pod
Mar 26 17:07:19.966: INFO: Waiting up to 15m0s for pod "adopt-release-wg9lw" in namespace "job-5809" to be "adopted"
Mar 26 17:07:19.974: INFO: Pod "adopt-release-wg9lw": Phase="Running", Reason="", readiness=true. Elapsed: 7.393684ms
Mar 26 17:07:21.982: INFO: Pod "adopt-release-wg9lw": Phase="Running", Reason="", readiness=true. Elapsed: 2.0154124s
Mar 26 17:07:21.982: INFO: Pod "adopt-release-wg9lw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 26 17:07:22.503: INFO: Successfully updated pod "adopt-release-wg9lw"
STEP: Checking that the Job releases the Pod
Mar 26 17:07:22.503: INFO: Waiting up to 15m0s for pod "adopt-release-wg9lw" in namespace "job-5809" to be "released"
Mar 26 17:07:22.521: INFO: Pod "adopt-release-wg9lw": Phase="Running", Reason="", readiness=true. Elapsed: 18.343897ms
Mar 26 17:07:22.521: INFO: Pod "adopt-release-wg9lw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:22.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5809" for this suite.

â€¢ [SLOW TEST:5.400 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":275,"completed":104,"skipped":1716,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:22.546: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:07:22.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f" in namespace "projected-2373" to be "Succeeded or Failed"
Mar 26 17:07:22.789: INFO: Pod "downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098745ms
Mar 26 17:07:24.795: INFO: Pod "downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010140332s
Mar 26 17:07:26.802: INFO: Pod "downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016771356s
STEP: Saw pod success
Mar 26 17:07:26.802: INFO: Pod "downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f" satisfied condition "Succeeded or Failed"
Mar 26 17:07:26.807: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f container client-container: <nil>
STEP: delete the pod
Mar 26 17:07:26.836: INFO: Waiting for pod downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f to disappear
Mar 26 17:07:26.839: INFO: Pod downwardapi-volume-52a8a569-95f5-4019-8863-856fd8fe1a5f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:26.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2373" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":105,"skipped":1717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:26.847: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2394
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 26 17:07:27.047: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:53.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2394" for this suite.

â€¢ [SLOW TEST:26.261 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":275,"completed":106,"skipped":1741,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:53.109: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 17:07:53.311: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 17:07:53.322: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 17:07:53.324: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-dqhapg before test
Mar 26 17:07:53.341: INFO: adopt-release-s4ppr from job-5809 started at 2020-03-26 17:07:22 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.341: INFO: 	Container c ready: true, restart count 0
Mar 26 17:07:53.341: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:07:53.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 17:07:53.341: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:07:53.341: INFO: adopt-release-z55nd from job-5809 started at 2020-03-26 17:07:17 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.341: INFO: 	Container c ready: true, restart count 0
Mar 26 17:07:53.341: INFO: sonobuoy from sonobuoy started at 2020-03-26 16:32:42 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.341: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 26 17:07:53.341: INFO: kube-flannel-ds-amd64-vblvg from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.341: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:07:53.341: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-vf6bys before test
Mar 26 17:07:53.348: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:07:53.349: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 17:07:53.349: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:07:53.349: INFO: adopt-release-wg9lw from job-5809 started at 2020-03-26 17:07:17 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.349: INFO: 	Container c ready: true, restart count 0
Mar 26 17:07:53.349: INFO: kube-flannel-ds-amd64-4db44 from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.349: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:07:53.349: INFO: coredns-6ccf845bfb-jklrr from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.349: INFO: 	Container coredns ready: true, restart count 0
Mar 26 17:07:53.349: INFO: coredns-6ccf845bfb-c9f7x from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:07:53.349: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node kubedee-test-worker-dqhapg
STEP: verifying the node has the label node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod adopt-release-s4ppr requesting resource cpu=0m on Node kubedee-test-worker-dqhapg
Mar 26 17:07:53.385: INFO: Pod adopt-release-wg9lw requesting resource cpu=0m on Node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod adopt-release-z55nd requesting resource cpu=0m on Node kubedee-test-worker-dqhapg
Mar 26 17:07:53.385: INFO: Pod coredns-6ccf845bfb-c9f7x requesting resource cpu=100m on Node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod coredns-6ccf845bfb-jklrr requesting resource cpu=100m on Node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod kube-flannel-ds-amd64-4db44 requesting resource cpu=100m on Node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod kube-flannel-ds-amd64-vblvg requesting resource cpu=100m on Node kubedee-test-worker-dqhapg
Mar 26 17:07:53.385: INFO: Pod sonobuoy requesting resource cpu=0m on Node kubedee-test-worker-dqhapg
Mar 26 17:07:53.385: INFO: Pod sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt requesting resource cpu=0m on Node kubedee-test-worker-vf6bys
Mar 26 17:07:53.385: INFO: Pod sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 requesting resource cpu=0m on Node kubedee-test-worker-dqhapg
STEP: Starting Pods to consume most of the cluster CPU.
Mar 26 17:07:53.385: INFO: Creating a pod which consumes cpu=1330m on Node kubedee-test-worker-dqhapg
Mar 26 17:07:53.392: INFO: Creating a pod which consumes cpu=1190m on Node kubedee-test-worker-vf6bys
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5.15ffe9a16f1051a2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3680/filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5 to kubedee-test-worker-vf6bys]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5.15ffe9a19fa895bd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5.15ffe9a1b1d08f73], Reason = [Created], Message = [Created container filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5.15ffe9a1b6d2edef], Reason = [Started], Message = [Started container filler-pod-73ecaf50-a591-43a3-9e68-2d6a97987bf5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a283631b-4553-428d-be6d-413d8320ef85.15ffe9a16e700c3b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3680/filler-pod-a283631b-4553-428d-be6d-413d8320ef85 to kubedee-test-worker-dqhapg]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a283631b-4553-428d-be6d-413d8320ef85.15ffe9a19f16c0e1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a283631b-4553-428d-be6d-413d8320ef85.15ffe9a1b5210f0d], Reason = [Created], Message = [Created container filler-pod-a283631b-4553-428d-be6d-413d8320ef85]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a283631b-4553-428d-be6d-413d8320ef85.15ffe9a1b743a718], Reason = [Started], Message = [Started container filler-pod-a283631b-4553-428d-be6d-413d8320ef85]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ffe9a25ed6c86b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node kubedee-test-worker-vf6bys
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kubedee-test-worker-dqhapg
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:07:58.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3680" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:5.391 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":275,"completed":107,"skipped":1742,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:07:58.500: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3158, will wait for the garbage collector to delete the pods
Mar 26 17:08:02.769: INFO: Deleting Job.batch foo took: 13.122848ms
Mar 26 17:08:03.170: INFO: Terminating Job.batch foo pods took: 400.435561ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:08:43.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3158" for this suite.

â€¢ [SLOW TEST:45.500 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":275,"completed":108,"skipped":1744,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:08:44.004: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:08:44.202: INFO: (0) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 12.517976ms)
Mar 26 17:08:44.206: INFO: (1) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.658122ms)
Mar 26 17:08:44.210: INFO: (2) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.362043ms)
Mar 26 17:08:44.214: INFO: (3) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.782947ms)
Mar 26 17:08:44.218: INFO: (4) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.82373ms)
Mar 26 17:08:44.222: INFO: (5) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.612649ms)
Mar 26 17:08:44.225: INFO: (6) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.330672ms)
Mar 26 17:08:44.229: INFO: (7) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.390711ms)
Mar 26 17:08:44.232: INFO: (8) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.944834ms)
Mar 26 17:08:44.235: INFO: (9) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.876315ms)
Mar 26 17:08:44.238: INFO: (10) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.644971ms)
Mar 26 17:08:44.241: INFO: (11) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.707667ms)
Mar 26 17:08:44.243: INFO: (12) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.5069ms)
Mar 26 17:08:44.246: INFO: (13) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.359588ms)
Mar 26 17:08:44.248: INFO: (14) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.762467ms)
Mar 26 17:08:44.252: INFO: (15) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.622306ms)
Mar 26 17:08:44.255: INFO: (16) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.95172ms)
Mar 26 17:08:44.258: INFO: (17) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.834306ms)
Mar 26 17:08:44.261: INFO: (18) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.177132ms)
Mar 26 17:08:44.264: INFO: (19) /api/v1/nodes/kubedee-test-worker-dqhapg/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.63717ms)
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:08:44.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8187" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":275,"completed":109,"skipped":1753,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:08:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9789
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9789
I0326 17:08:44.569996      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9789, replica count: 2
I0326 17:08:47.640121      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:08:50.643847      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:08:50.644: INFO: Creating new exec pod
Mar 26 17:08:53.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9789 execpodgnr6d -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 17:08:54.102: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 26 17:08:54.102: INFO: stdout: ""
Mar 26 17:08:54.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9789 execpodgnr6d -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.226 80'
Mar 26 17:08:54.314: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.226 80\nConnection to 10.32.0.226 80 port [tcp/http] succeeded!\n"
Mar 26 17:08:54.314: INFO: stdout: ""
Mar 26 17:08:54.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9789 execpodgnr6d -- /bin/sh -x -c nc -zv -t -w 2 10.107.34.127 30767'
Mar 26 17:08:54.488: INFO: stderr: "+ nc -zv -t -w 2 10.107.34.127 30767\nConnection to 10.107.34.127 30767 port [tcp/30767] succeeded!\n"
Mar 26 17:08:54.488: INFO: stdout: ""
Mar 26 17:08:54.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-9789 execpodgnr6d -- /bin/sh -x -c nc -zv -t -w 2 10.107.34.71 30767'
Mar 26 17:08:54.675: INFO: stderr: "+ nc -zv -t -w 2 10.107.34.71 30767\nConnection to 10.107.34.71 30767 port [tcp/30767] succeeded!\n"
Mar 26 17:08:54.675: INFO: stdout: ""
Mar 26 17:08:54.675: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:08:54.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9789" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:10.513 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":275,"completed":110,"skipped":1761,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:08:54.794: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2531.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2531.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:08:57.064: INFO: Unable to read jessie_udp@PodARecord from pod dns-2531/dns-test-30f7570f-b18f-482d-ac6c-34a072192356: the server could not find the requested resource (get pods dns-test-30f7570f-b18f-482d-ac6c-34a072192356)
Mar 26 17:08:57.067: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2531/dns-test-30f7570f-b18f-482d-ac6c-34a072192356: the server could not find the requested resource (get pods dns-test-30f7570f-b18f-482d-ac6c-34a072192356)
Mar 26 17:08:57.068: INFO: Lookups using dns-2531/dns-test-30f7570f-b18f-482d-ac6c-34a072192356 failed for: [jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 26 17:09:02.220: INFO: DNS probes using dns-2531/dns-test-30f7570f-b18f-482d-ac6c-34a072192356 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:02.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2531" for this suite.

â€¢ [SLOW TEST:7.466 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":275,"completed":111,"skipped":1774,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:09:03.173: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:09:05.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839343, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839343, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839343, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839343, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:09:08.233: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:20.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5777" for this suite.
STEP: Destroying namespace "webhook-5777-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:18.555 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":275,"completed":112,"skipped":1777,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:20.818: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 17:09:21.206: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 17:09:21.222: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 17:09:21.224: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-dqhapg before test
Mar 26 17:09:21.243: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:09:21.243: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 17:09:21.243: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:09:21.243: INFO: sonobuoy from sonobuoy started at 2020-03-26 16:32:42 +0000 UTC (1 container statuses recorded)
Mar 26 17:09:21.243: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 26 17:09:21.243: INFO: kube-flannel-ds-amd64-vblvg from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:09:21.243: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:09:21.243: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-vf6bys before test
Mar 26 17:09:21.302: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:09:21.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 26 17:09:21.302: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:09:21.302: INFO: coredns-6ccf845bfb-c9f7x from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:09:21.302: INFO: 	Container coredns ready: true, restart count 0
Mar 26 17:09:21.302: INFO: kube-flannel-ds-amd64-4db44 from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:09:21.302: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:09:21.302: INFO: coredns-6ccf845bfb-jklrr from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:09:21.302: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ffe9b5e883ed0a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:22.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9463" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":275,"completed":113,"skipped":1784,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:22.360: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:09:22.562: INFO: Creating deployment "test-recreate-deployment"
Mar 26 17:09:22.576: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 26 17:09:22.594: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 26 17:09:24.603: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 26 17:09:24.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839362, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839362, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839362, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839362, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:09:26.615: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 26 17:09:26.644: INFO: Updating deployment test-recreate-deployment
Mar 26 17:09:26.644: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 17:09:26.879: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5466 /apis/apps/v1/namespaces/deployment-5466/deployments/test-recreate-deployment e62d7e5a-4369-4a76-9215-0a1b8b083300 15169 2 2020-03-26 17:09:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0025f1008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-26 17:09:26 +0000 UTC,LastTransitionTime:2020-03-26 17:09:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-03-26 17:09:26 +0000 UTC,LastTransitionTime:2020-03-26 17:09:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 26 17:09:26.906: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-5466 /apis/apps/v1/namespaces/deployment-5466/replicasets/test-recreate-deployment-d5667d9c7 23634ea6-6710-4e84-9847-e4121952d764 15167 1 2020-03-26 17:09:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e62d7e5a-4369-4a76-9215-0a1b8b083300 0xc0054c3ae0 0xc0054c3ae1}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 54 50 100 55 101 53 97 45 52 51 54 57 45 52 97 55 54 45 57 50 49 53 45 48 97 49 98 56 98 48 56 51 51 48 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0054c3b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:09:26.906: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 26 17:09:26.907: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-5466 /apis/apps/v1/namespaces/deployment-5466/replicasets/test-recreate-deployment-74d98b5f7c 4886b3f5-b703-4d95-b947-5d66017101c9 15155 2 2020-03-26 17:09:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e62d7e5a-4369-4a76-9215-0a1b8b083300 0xc0054c39a7 0xc0054c39a8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 54 50 100 55 101 53 97 45 52 51 54 57 45 52 97 55 54 45 57 50 49 53 45 48 97 49 98 56 98 48 56 51 51 48 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0054c3a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:09:26.924: INFO: Pod "test-recreate-deployment-d5667d9c7-54tmq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-54tmq test-recreate-deployment-d5667d9c7- deployment-5466 /api/v1/namespaces/deployment-5466/pods/test-recreate-deployment-d5667d9c7-54tmq 0fce869f-ebe9-4259-91d2-bfbf1a298f38 15168 0 2020-03-26 17:09:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 23634ea6-6710-4e84-9847-e4121952d764 0xc003c20250 0xc003c20251}] []  [{kube-controller-manager Update v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 51 54 51 52 101 97 54 45 54 55 49 48 45 52 101 56 52 45 57 56 52 55 45 101 52 49 50 49 57 53 50 100 55 54 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:09:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h4vxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h4vxf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h4vxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:09:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:09:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:09:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:,StartTime:2020-03-26 17:09:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:26.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5466" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":114,"skipped":1792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:26.947: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 26 17:09:27.188: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 26 17:09:32.195: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:32.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8640" for this suite.

â€¢ [SLOW TEST:5.316 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":275,"completed":115,"skipped":1815,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:09:35.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5439" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":275,"completed":116,"skipped":1817,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:09:35.586: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7859
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 26 17:09:35.796: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 26 17:09:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:10:01.120: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:19.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7859" for this suite.

â€¢ [SLOW TEST:44.348 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":275,"completed":117,"skipped":1823,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:36.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4174" for this suite.

â€¢ [SLOW TEST:16.285 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":275,"completed":118,"skipped":1832,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:36.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9876" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":275,"completed":119,"skipped":1843,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:36.433: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8408.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.175_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8408.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.175_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:10:40.897: INFO: Unable to read wheezy_udp@dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.904: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.907: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.925: INFO: Unable to read jessie_udp@dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.927: INFO: Unable to read jessie_tcp@dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.929: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.932: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local from pod dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32: the server could not find the requested resource (get pods dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32)
Mar 26 17:10:40.943: INFO: Lookups using dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32 failed for: [wheezy_udp@dns-test-service.dns-8408.svc.cluster.local wheezy_tcp@dns-test-service.dns-8408.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local jessie_udp@dns-test-service.dns-8408.svc.cluster.local jessie_tcp@dns-test-service.dns-8408.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8408.svc.cluster.local]

Mar 26 17:10:46.034: INFO: DNS probes using dns-8408/dns-test-407bb9d6-071c-4865-8d5b-f222aacf1b32 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:46.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8408" for this suite.

â€¢ [SLOW TEST:10.125 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":275,"completed":120,"skipped":1858,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:46.561: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 26 17:10:46.854: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5343 /api/v1/namespaces/watch-5343/configmaps/e2e-watch-test-resource-version 8a6e9fde-b34e-491b-a278-0695875230ee 15614 0 2020-03-26 17:10:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-03-26 17:10:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:10:46.854: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5343 /api/v1/namespaces/watch-5343/configmaps/e2e-watch-test-resource-version 8a6e9fde-b34e-491b-a278-0695875230ee 15615 0 2020-03-26 17:10:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-03-26 17:10:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:46.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5343" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":275,"completed":121,"skipped":1869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:46.868: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Mar 26 17:10:47.077: INFO: Waiting up to 5m0s for pod "var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d" in namespace "var-expansion-1386" to be "Succeeded or Failed"
Mar 26 17:10:47.115: INFO: Pod "var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d": Phase="Pending", Reason="", readiness=false. Elapsed: 37.64802ms
Mar 26 17:10:49.121: INFO: Pod "var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043151997s
Mar 26 17:10:51.127: INFO: Pod "var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048997858s
STEP: Saw pod success
Mar 26 17:10:51.127: INFO: Pod "var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d" satisfied condition "Succeeded or Failed"
Mar 26 17:10:51.131: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:10:51.178: INFO: Waiting for pod var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d to disappear
Mar 26 17:10:51.181: INFO: Pod var-expansion-66ee8a10-11cd-4305-bdfe-875bd20d105d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:10:51.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1386" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":275,"completed":122,"skipped":1891,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:10:51.193: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3
STEP: creating replication controller externalsvc in namespace services-3
I0326 17:10:51.498621      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3, replica count: 2
I0326 17:10:54.565035      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:10:57.565722      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 26 17:10:57.598: INFO: Creating new exec pod
Mar 26 17:10:59.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-3 execpodc2prg -- /bin/sh -x -c nslookup clusterip-service'
Mar 26 17:10:59.871: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 26 17:10:59.871: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nclusterip-service.services-3.svc.cluster.local\tcanonical name = externalsvc.services-3.svc.cluster.local.\nName:\texternalsvc.services-3.svc.cluster.local\nAddress: 10.32.0.78\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3, will wait for the garbage collector to delete the pods
Mar 26 17:10:59.940: INFO: Deleting ReplicationController externalsvc took: 11.686148ms
Mar 26 17:11:00.341: INFO: Terminating ReplicationController externalsvc pods took: 400.306602ms
Mar 26 17:11:08.330: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:08.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:17.269 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":275,"completed":123,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:08.467: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Mar 26 17:11:08.692: INFO: Waiting up to 5m0s for pod "var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22" in namespace "var-expansion-4419" to be "Succeeded or Failed"
Mar 26 17:11:08.702: INFO: Pod "var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22": Phase="Pending", Reason="", readiness=false. Elapsed: 9.796052ms
Mar 26 17:11:10.711: INFO: Pod "var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019057004s
STEP: Saw pod success
Mar 26 17:11:10.711: INFO: Pod "var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22" satisfied condition "Succeeded or Failed"
Mar 26 17:11:10.733: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22 container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:11:10.765: INFO: Waiting for pod var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22 to disappear
Mar 26 17:11:10.768: INFO: Pod var-expansion-95477b46-3e7f-47cd-b0a7-013649c60c22 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4419" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":275,"completed":124,"skipped":1931,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:10.780: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 17:11:11.106: INFO: Waiting up to 5m0s for pod "pod-47c1e251-9278-43cf-ab3d-7822129c5556" in namespace "emptydir-7655" to be "Succeeded or Failed"
Mar 26 17:11:11.116: INFO: Pod "pod-47c1e251-9278-43cf-ab3d-7822129c5556": Phase="Pending", Reason="", readiness=false. Elapsed: 9.0208ms
Mar 26 17:11:13.121: INFO: Pod "pod-47c1e251-9278-43cf-ab3d-7822129c5556": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014524032s
STEP: Saw pod success
Mar 26 17:11:13.121: INFO: Pod "pod-47c1e251-9278-43cf-ab3d-7822129c5556" satisfied condition "Succeeded or Failed"
Mar 26 17:11:13.125: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-47c1e251-9278-43cf-ab3d-7822129c5556 container test-container: <nil>
STEP: delete the pod
Mar 26 17:11:13.165: INFO: Waiting for pod pod-47c1e251-9278-43cf-ab3d-7822129c5556 to disappear
Mar 26 17:11:13.171: INFO: Pod pod-47c1e251-9278-43cf-ab3d-7822129c5556 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:13.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7655" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":125,"skipped":1937,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:11:14.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:11:16.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839474, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839474, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839474, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839473, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:11:19.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7721" for this suite.
STEP: Destroying namespace "webhook-7721-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.390 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":275,"completed":126,"skipped":1947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:11:19.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854" in namespace "downward-api-7752" to be "Succeeded or Failed"
Mar 26 17:11:19.844: INFO: Pod "downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854": Phase="Pending", Reason="", readiness=false. Elapsed: 15.286602ms
Mar 26 17:11:21.850: INFO: Pod "downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021846121s
STEP: Saw pod success
Mar 26 17:11:21.850: INFO: Pod "downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854" satisfied condition "Succeeded or Failed"
Mar 26 17:11:21.853: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854 container client-container: <nil>
STEP: delete the pod
Mar 26 17:11:21.897: INFO: Waiting for pod downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854 to disappear
Mar 26 17:11:21.900: INFO: Pod downwardapi-volume-a0e172c9-93af-4ba8-a9a9-634ea9150854 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:21.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7752" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":127,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:21.912: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 26 17:11:22.064: INFO: Waiting up to 5m0s for pod "pod-de0ab7c7-f60a-4775-9256-f39f99b65c87" in namespace "emptydir-4741" to be "Succeeded or Failed"
Mar 26 17:11:22.083: INFO: Pod "pod-de0ab7c7-f60a-4775-9256-f39f99b65c87": Phase="Pending", Reason="", readiness=false. Elapsed: 18.596501ms
Mar 26 17:11:24.087: INFO: Pod "pod-de0ab7c7-f60a-4775-9256-f39f99b65c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022933271s
STEP: Saw pod success
Mar 26 17:11:24.087: INFO: Pod "pod-de0ab7c7-f60a-4775-9256-f39f99b65c87" satisfied condition "Succeeded or Failed"
Mar 26 17:11:24.090: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-de0ab7c7-f60a-4775-9256-f39f99b65c87 container test-container: <nil>
STEP: delete the pod
Mar 26 17:11:24.104: INFO: Waiting for pod pod-de0ab7c7-f60a-4775-9256-f39f99b65c87 to disappear
Mar 26 17:11:24.107: INFO: Pod pod-de0ab7c7-f60a-4775-9256-f39f99b65c87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:24.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4741" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":128,"skipped":2070,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:24.115: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 17:11:26.829: INFO: Successfully updated pod "labelsupdate6a2ecfea-f175-41b1-bea3-a273ed0fa178"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:28.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1865" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":129,"skipped":2076,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:28.860: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-03395c2c-086d-476c-b958-c349fbae419a
STEP: Creating a pod to test consume secrets
Mar 26 17:11:29.074: INFO: Waiting up to 5m0s for pod "pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf" in namespace "secrets-4133" to be "Succeeded or Failed"
Mar 26 17:11:29.083: INFO: Pod "pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091089ms
Mar 26 17:11:31.101: INFO: Pod "pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026405743s
STEP: Saw pod success
Mar 26 17:11:31.101: INFO: Pod "pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf" satisfied condition "Succeeded or Failed"
Mar 26 17:11:31.114: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:11:31.148: INFO: Waiting for pod pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf to disappear
Mar 26 17:11:31.150: INFO: Pod pod-secrets-61a24e97-aef6-4680-a45f-ee16d1c5c1cf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:31.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4133" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":130,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:31.159: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:11:31.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0" in namespace "projected-3683" to be "Succeeded or Failed"
Mar 26 17:11:31.380: INFO: Pod "downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.360471ms
Mar 26 17:11:33.385: INFO: Pod "downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018199254s
Mar 26 17:11:35.401: INFO: Pod "downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034087845s
STEP: Saw pod success
Mar 26 17:11:35.401: INFO: Pod "downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0" satisfied condition "Succeeded or Failed"
Mar 26 17:11:35.405: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0 container client-container: <nil>
STEP: delete the pod
Mar 26 17:11:35.428: INFO: Waiting for pod downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0 to disappear
Mar 26 17:11:35.438: INFO: Pod downwardapi-volume-4dcd6947-5a8f-4542-8c21-b319c8ad1dc0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:35.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3683" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":131,"skipped":2097,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6097.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6097.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6097.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6097.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:11:39.669: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.672: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.674: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.683: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.685: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.687: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.689: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6097.svc.cluster.local from pod dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d: the server could not find the requested resource (get pods dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d)
Mar 26 17:11:39.693: INFO: Lookups using dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6097.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6097.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6097.svc.cluster.local jessie_udp@dns-test-service-2.dns-6097.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6097.svc.cluster.local]

Mar 26 17:11:44.733: INFO: DNS probes using dns-6097/dns-test-60405b4d-8e2f-49c7-bce6-675a5e70fb3d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:11:44.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6097" for this suite.

â€¢ [SLOW TEST:9.500 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":275,"completed":132,"skipped":2112,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:11:44.949: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-pqmm
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:11:45.355: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pqmm" in namespace "subpath-3034" to be "Succeeded or Failed"
Mar 26 17:11:45.371: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Pending", Reason="", readiness=false. Elapsed: 15.290562ms
Mar 26 17:11:47.376: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020487476s
Mar 26 17:11:49.383: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 4.027468276s
Mar 26 17:11:51.390: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 6.034744234s
Mar 26 17:11:53.397: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 8.042010495s
Mar 26 17:11:55.412: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 10.056435906s
Mar 26 17:11:57.417: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.062094846s
Mar 26 17:11:59.423: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 14.06782914s
Mar 26 17:12:01.428: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 16.07259681s
Mar 26 17:12:03.433: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 18.077980874s
Mar 26 17:12:05.438: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Running", Reason="", readiness=true. Elapsed: 20.082314541s
Mar 26 17:12:07.442: INFO: Pod "pod-subpath-test-configmap-pqmm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.086500604s
STEP: Saw pod success
Mar 26 17:12:07.442: INFO: Pod "pod-subpath-test-configmap-pqmm" satisfied condition "Succeeded or Failed"
Mar 26 17:12:07.445: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-subpath-test-configmap-pqmm container test-container-subpath-configmap-pqmm: <nil>
STEP: delete the pod
Mar 26 17:12:07.469: INFO: Waiting for pod pod-subpath-test-configmap-pqmm to disappear
Mar 26 17:12:07.482: INFO: Pod pod-subpath-test-configmap-pqmm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pqmm
Mar 26 17:12:07.482: INFO: Deleting pod "pod-subpath-test-configmap-pqmm" in namespace "subpath-3034"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:12:07.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3034" for this suite.

â€¢ [SLOW TEST:22.550 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":275,"completed":133,"skipped":2120,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:12:07.501: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:12:08.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:12:10.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839528, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:12:13.281: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:12:13.285: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8909-crds.webhook.example.com via the AdmissionRegistration API
Mar 26 17:12:18.905: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:12:19.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5934" for this suite.
STEP: Destroying namespace "webhook-5934-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:12.493 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":275,"completed":134,"skipped":2123,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:12:19.994: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7006
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:12:20.153: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 17:12:28.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-7006 create -f -'
Mar 26 17:12:30.456: INFO: stderr: ""
Mar 26 17:12:30.456: INFO: stdout: "e2e-test-crd-publish-openapi-3019-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 26 17:12:30.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-7006 delete e2e-test-crd-publish-openapi-3019-crds test-cr'
Mar 26 17:12:30.572: INFO: stderr: ""
Mar 26 17:12:30.572: INFO: stdout: "e2e-test-crd-publish-openapi-3019-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 26 17:12:30.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-7006 apply -f -'
Mar 26 17:12:31.867: INFO: stderr: ""
Mar 26 17:12:31.867: INFO: stdout: "e2e-test-crd-publish-openapi-3019-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 26 17:12:31.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-7006 delete e2e-test-crd-publish-openapi-3019-crds test-cr'
Mar 26 17:12:31.965: INFO: stderr: ""
Mar 26 17:12:31.965: INFO: stdout: "e2e-test-crd-publish-openapi-3019-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 26 17:12:31.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-3019-crds'
Mar 26 17:12:33.016: INFO: stderr: ""
Mar 26 17:12:33.016: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3019-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:12:36.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7006" for this suite.

â€¢ [SLOW TEST:16.057 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":275,"completed":135,"skipped":2143,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:12:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 26 17:12:36.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-9600'
Mar 26 17:12:37.399: INFO: stderr: ""
Mar 26 17:12:37.399: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 17:12:37.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:12:37.602: INFO: stderr: ""
Mar 26 17:12:37.603: INFO: stdout: "update-demo-nautilus-545gt update-demo-nautilus-jzmtn "
Mar 26 17:12:37.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-545gt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:37.795: INFO: stderr: ""
Mar 26 17:12:37.795: INFO: stdout: ""
Mar 26 17:12:37.795: INFO: update-demo-nautilus-545gt is created but not running
Mar 26 17:12:42.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:12:43.052: INFO: stderr: ""
Mar 26 17:12:43.052: INFO: stdout: "update-demo-nautilus-545gt update-demo-nautilus-jzmtn "
Mar 26 17:12:43.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-545gt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:43.235: INFO: stderr: ""
Mar 26 17:12:43.235: INFO: stdout: ""
Mar 26 17:12:43.235: INFO: update-demo-nautilus-545gt is created but not running
Mar 26 17:12:48.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:12:48.380: INFO: stderr: ""
Mar 26 17:12:48.380: INFO: stdout: "update-demo-nautilus-545gt update-demo-nautilus-jzmtn "
Mar 26 17:12:48.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-545gt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:48.501: INFO: stderr: ""
Mar 26 17:12:48.501: INFO: stdout: "true"
Mar 26 17:12:48.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-545gt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:48.597: INFO: stderr: ""
Mar 26 17:12:48.597: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:12:48.597: INFO: validating pod update-demo-nautilus-545gt
Mar 26 17:12:48.602: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:12:48.613: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:12:48.613: INFO: update-demo-nautilus-545gt is verified up and running
Mar 26 17:12:48.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:48.701: INFO: stderr: ""
Mar 26 17:12:48.701: INFO: stdout: "true"
Mar 26 17:12:48.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:12:48.780: INFO: stderr: ""
Mar 26 17:12:48.780: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:12:48.780: INFO: validating pod update-demo-nautilus-jzmtn
Mar 26 17:12:48.784: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:12:48.784: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:12:48.785: INFO: update-demo-nautilus-jzmtn is verified up and running
STEP: scaling down the replication controller
Mar 26 17:12:48.789: INFO: scanned /root for discovery docs: <nil>
Mar 26 17:12:48.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9600'
Mar 26 17:12:49.969: INFO: stderr: ""
Mar 26 17:12:49.969: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 17:12:49.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:12:50.076: INFO: stderr: ""
Mar 26 17:12:50.076: INFO: stdout: "update-demo-nautilus-545gt update-demo-nautilus-jzmtn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 26 17:12:55.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:12:55.186: INFO: stderr: ""
Mar 26 17:12:55.186: INFO: stdout: "update-demo-nautilus-545gt update-demo-nautilus-jzmtn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 26 17:13:00.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:13:00.313: INFO: stderr: ""
Mar 26 17:13:00.313: INFO: stdout: "update-demo-nautilus-jzmtn "
Mar 26 17:13:00.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:00.397: INFO: stderr: ""
Mar 26 17:13:00.397: INFO: stdout: "true"
Mar 26 17:13:00.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:00.485: INFO: stderr: ""
Mar 26 17:13:00.485: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:13:00.485: INFO: validating pod update-demo-nautilus-jzmtn
Mar 26 17:13:00.490: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:13:00.490: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:13:00.490: INFO: update-demo-nautilus-jzmtn is verified up and running
STEP: scaling up the replication controller
Mar 26 17:13:00.493: INFO: scanned /root for discovery docs: <nil>
Mar 26 17:13:00.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9600'
Mar 26 17:13:01.646: INFO: stderr: ""
Mar 26 17:13:01.646: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 17:13:01.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:13:01.772: INFO: stderr: ""
Mar 26 17:13:01.772: INFO: stdout: "update-demo-nautilus-ffmm4 update-demo-nautilus-jzmtn "
Mar 26 17:13:01.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-ffmm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:01.854: INFO: stderr: ""
Mar 26 17:13:01.854: INFO: stdout: ""
Mar 26 17:13:01.854: INFO: update-demo-nautilus-ffmm4 is created but not running
Mar 26 17:13:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9600'
Mar 26 17:13:06.980: INFO: stderr: ""
Mar 26 17:13:06.980: INFO: stdout: "update-demo-nautilus-ffmm4 update-demo-nautilus-jzmtn "
Mar 26 17:13:06.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-ffmm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:07.061: INFO: stderr: ""
Mar 26 17:13:07.061: INFO: stdout: "true"
Mar 26 17:13:07.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-ffmm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:07.141: INFO: stderr: ""
Mar 26 17:13:07.141: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:13:07.141: INFO: validating pod update-demo-nautilus-ffmm4
Mar 26 17:13:07.145: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:13:07.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:13:07.145: INFO: update-demo-nautilus-ffmm4 is verified up and running
Mar 26 17:13:07.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:07.233: INFO: stderr: ""
Mar 26 17:13:07.233: INFO: stdout: "true"
Mar 26 17:13:07.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jzmtn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9600'
Mar 26 17:13:07.307: INFO: stderr: ""
Mar 26 17:13:07.307: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:13:07.307: INFO: validating pod update-demo-nautilus-jzmtn
Mar 26 17:13:07.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:13:07.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:13:07.311: INFO: update-demo-nautilus-jzmtn is verified up and running
STEP: using delete to clean up resources
Mar 26 17:13:07.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-9600'
Mar 26 17:13:07.443: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:13:07.444: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 17:13:07.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9600'
Mar 26 17:13:07.539: INFO: stderr: "No resources found in kubectl-9600 namespace.\n"
Mar 26 17:13:07.539: INFO: stdout: ""
Mar 26 17:13:07.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -l name=update-demo --namespace=kubectl-9600 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 17:13:07.641: INFO: stderr: ""
Mar 26 17:13:07.641: INFO: stdout: "update-demo-nautilus-ffmm4\nupdate-demo-nautilus-jzmtn\n"
Mar 26 17:13:08.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9600'
Mar 26 17:13:08.407: INFO: stderr: "No resources found in kubectl-9600 namespace.\n"
Mar 26 17:13:08.407: INFO: stdout: ""
Mar 26 17:13:08.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -l name=update-demo --namespace=kubectl-9600 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 17:13:08.638: INFO: stderr: ""
Mar 26 17:13:08.638: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:08.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9600" for this suite.

â€¢ [SLOW TEST:32.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":275,"completed":136,"skipped":2143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1079
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:13:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 26 17:13:17.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 create -f -'
Mar 26 17:13:19.618: INFO: stderr: ""
Mar 26 17:13:19.618: INFO: stdout: "e2e-test-crd-publish-openapi-5356-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 26 17:13:19.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 delete e2e-test-crd-publish-openapi-5356-crds test-foo'
Mar 26 17:13:19.710: INFO: stderr: ""
Mar 26 17:13:19.710: INFO: stdout: "e2e-test-crd-publish-openapi-5356-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 26 17:13:19.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 apply -f -'
Mar 26 17:13:20.942: INFO: stderr: ""
Mar 26 17:13:20.942: INFO: stdout: "e2e-test-crd-publish-openapi-5356-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 26 17:13:20.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 delete e2e-test-crd-publish-openapi-5356-crds test-foo'
Mar 26 17:13:21.055: INFO: stderr: ""
Mar 26 17:13:21.055: INFO: stdout: "e2e-test-crd-publish-openapi-5356-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 26 17:13:21.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 create -f -'
Mar 26 17:13:21.829: INFO: rc: 1
Mar 26 17:13:21.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 apply -f -'
Mar 26 17:13:22.664: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 26 17:13:22.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 create -f -'
Mar 26 17:13:23.065: INFO: rc: 1
Mar 26 17:13:23.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-1079 apply -f -'
Mar 26 17:13:23.602: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 26 17:13:23.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-5356-crds'
Mar 26 17:13:23.971: INFO: stderr: ""
Mar 26 17:13:23.971: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5356-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 26 17:13:23.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-5356-crds.metadata'
Mar 26 17:13:24.377: INFO: stderr: ""
Mar 26 17:13:24.377: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5356-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 26 17:13:24.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-5356-crds.spec'
Mar 26 17:13:24.843: INFO: stderr: ""
Mar 26 17:13:24.843: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5356-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 26 17:13:24.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-5356-crds.spec.bars'
Mar 26 17:13:25.410: INFO: stderr: ""
Mar 26 17:13:25.410: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5356-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 26 17:13:25.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-5356-crds.spec.bars2'
Mar 26 17:13:25.973: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:29.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1079" for this suite.

â€¢ [SLOW TEST:20.874 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":275,"completed":137,"skipped":2184,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:29.549: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1499
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-b1430de5-e7c9-43ff-86b6-855117e4aa01
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b1430de5-e7c9-43ff-86b6-855117e4aa01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:33.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1499" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":138,"skipped":2188,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:33.840: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 17:13:36.083: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:36.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9049" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":139,"skipped":2194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-0118f6e4-4cfe-4485-96fb-bbe5aa2cf95c
STEP: Creating a pod to test consume secrets
Mar 26 17:13:36.382: INFO: Waiting up to 5m0s for pod "pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798" in namespace "secrets-2165" to be "Succeeded or Failed"
Mar 26 17:13:36.390: INFO: Pod "pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798": Phase="Pending", Reason="", readiness=false. Elapsed: 7.820876ms
Mar 26 17:13:38.397: INFO: Pod "pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015336899s
STEP: Saw pod success
Mar 26 17:13:38.397: INFO: Pod "pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798" satisfied condition "Succeeded or Failed"
Mar 26 17:13:38.401: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798 container secret-env-test: <nil>
STEP: delete the pod
Mar 26 17:13:38.421: INFO: Waiting for pod pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798 to disappear
Mar 26 17:13:38.423: INFO: Pod pod-secrets-519a5c7a-4839-4fd1-a787-78a15123c798 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2165" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":275,"completed":140,"skipped":2226,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:38.431: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 17:13:38.576: INFO: Waiting up to 5m0s for pod "pod-00d068da-c29d-4922-90c4-05df0f6e4e54" in namespace "emptydir-8605" to be "Succeeded or Failed"
Mar 26 17:13:38.581: INFO: Pod "pod-00d068da-c29d-4922-90c4-05df0f6e4e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072883ms
Mar 26 17:13:40.588: INFO: Pod "pod-00d068da-c29d-4922-90c4-05df0f6e4e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01110471s
Mar 26 17:13:42.594: INFO: Pod "pod-00d068da-c29d-4922-90c4-05df0f6e4e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017598444s
STEP: Saw pod success
Mar 26 17:13:42.594: INFO: Pod "pod-00d068da-c29d-4922-90c4-05df0f6e4e54" satisfied condition "Succeeded or Failed"
Mar 26 17:13:42.599: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-00d068da-c29d-4922-90c4-05df0f6e4e54 container test-container: <nil>
STEP: delete the pod
Mar 26 17:13:42.620: INFO: Waiting for pod pod-00d068da-c29d-4922-90c4-05df0f6e4e54 to disappear
Mar 26 17:13:42.623: INFO: Pod pod-00d068da-c29d-4922-90c4-05df0f6e4e54 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:42.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8605" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":141,"skipped":2236,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:42.635: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1499
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1499
I0326 17:13:42.887519      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1499, replica count: 2
I0326 17:13:45.938840      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:13:48.939444      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:13:48.939: INFO: Creating new exec pod
Mar 26 17:13:51.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-1499 execpodp82k2 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 17:13:52.265: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 26 17:13:52.265: INFO: stdout: ""
Mar 26 17:13:52.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=services-1499 execpodp82k2 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.249 80'
Mar 26 17:13:52.466: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.249 80\nConnection to 10.32.0.249 80 port [tcp/http] succeeded!\n"
Mar 26 17:13:52.466: INFO: stdout: ""
Mar 26 17:13:52.466: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:13:52.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1499" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:9.951 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":275,"completed":142,"skipped":2251,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:13:52.586: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-02d0fff1-7f17-4536-a03e-170892fcf592 in namespace container-probe-2609
Mar 26 17:13:54.783: INFO: Started pod busybox-02d0fff1-7f17-4536-a03e-170892fcf592 in namespace container-probe-2609
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:13:54.787: INFO: Initial restart count of pod busybox-02d0fff1-7f17-4536-a03e-170892fcf592 is 0
Mar 26 17:14:48.992: INFO: Restart count of pod container-probe-2609/busybox-02d0fff1-7f17-4536-a03e-170892fcf592 is now 1 (54.20509697s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:14:49.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2609" for this suite.

â€¢ [SLOW TEST:56.460 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":143,"skipped":2252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:14:49.049: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:14:49.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:14:51.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839689, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839689, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839689, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839689, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:14:54.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:14:54.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6183" for this suite.
STEP: Destroying namespace "webhook-6183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.070 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":275,"completed":144,"skipped":2281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:14:55.119: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2310
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2310
STEP: Creating statefulset with conflicting port in namespace statefulset-2310
STEP: Waiting until pod test-pod will start running in namespace statefulset-2310
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2310
Mar 26 17:14:59.569: INFO: Observed stateful pod in namespace: statefulset-2310, name: ss-0, uid: 051f7ff7-ec45-49f3-986d-0e87d22bbbff, status phase: Pending. Waiting for statefulset controller to delete.
Mar 26 17:14:59.583: INFO: Observed stateful pod in namespace: statefulset-2310, name: ss-0, uid: 051f7ff7-ec45-49f3-986d-0e87d22bbbff, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 17:14:59.595: INFO: Observed stateful pod in namespace: statefulset-2310, name: ss-0, uid: 051f7ff7-ec45-49f3-986d-0e87d22bbbff, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 17:14:59.619: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2310
STEP: Removing pod with conflicting port in namespace statefulset-2310
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2310 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:15:03.672: INFO: Deleting all statefulset in ns statefulset-2310
Mar 26 17:15:03.676: INFO: Scaling statefulset ss to 0
Mar 26 17:15:13.703: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:15:13.707: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:15:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2310" for this suite.

â€¢ [SLOW TEST:18.678 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":275,"completed":145,"skipped":2313,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:15:13.804: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-58da1c07-748a-486a-a02d-8af716bb9331
STEP: Creating a pod to test consume configMaps
Mar 26 17:15:14.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec" in namespace "projected-9366" to be "Succeeded or Failed"
Mar 26 17:15:14.028: INFO: Pod "pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.197609ms
Mar 26 17:15:16.035: INFO: Pod "pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017262898s
Mar 26 17:15:18.042: INFO: Pod "pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023748125s
STEP: Saw pod success
Mar 26 17:15:18.042: INFO: Pod "pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec" satisfied condition "Succeeded or Failed"
Mar 26 17:15:18.045: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:15:18.080: INFO: Waiting for pod pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec to disappear
Mar 26 17:15:18.082: INFO: Pod pod-projected-configmaps-b55919b0-1696-4af4-84e1-36090b961eec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:15:18.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9366" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":146,"skipped":2318,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:15:18.091: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7108
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-92aca267-5ced-49e3-a545-2751149f3186
STEP: Creating secret with name s-test-opt-upd-02786b64-9e27-4498-b7dd-9453dd917949
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-92aca267-5ced-49e3-a545-2751149f3186
STEP: Updating secret s-test-opt-upd-02786b64-9e27-4498-b7dd-9453dd917949
STEP: Creating secret with name s-test-opt-create-576468b1-a148-491f-8b05-44e97e0c7ada
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:16:38.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7108" for this suite.

â€¢ [SLOW TEST:80.841 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":147,"skipped":2336,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:16:38.939: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-acd60aa4-3800-4f9c-985b-1768f30927a9
STEP: Creating a pod to test consume configMaps
Mar 26 17:16:39.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d" in namespace "projected-4436" to be "Succeeded or Failed"
Mar 26 17:16:39.098: INFO: Pod "pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175749ms
Mar 26 17:16:41.105: INFO: Pod "pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010835859s
STEP: Saw pod success
Mar 26 17:16:41.105: INFO: Pod "pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d" satisfied condition "Succeeded or Failed"
Mar 26 17:16:41.109: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:16:41.155: INFO: Waiting for pod pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d to disappear
Mar 26 17:16:41.159: INFO: Pod pod-projected-configmaps-f2763469-bca4-4ff0-a32b-02d7ed3da35d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:16:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4436" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":148,"skipped":2354,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:16:41.170: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 26 17:16:41.382: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:16:53.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9552" for this suite.

â€¢ [SLOW TEST:12.770 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":275,"completed":149,"skipped":2366,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:16:53.941: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:16:54.801: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 26 17:16:56.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839814, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839814, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839814, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720839814, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:16:59.909: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:16:59.914: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:06.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3307" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:12.663 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":275,"completed":150,"skipped":2378,"failed":0}
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:06.605: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Mar 26 17:17:06.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-9865 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 26 17:17:07.073: INFO: stderr: ""
Mar 26 17:17:07.073: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Mar 26 17:17:07.073: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 26 17:17:07.085: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9865" to be "running and ready, or succeeded"
Mar 26 17:17:07.099: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.655891ms
Mar 26 17:17:09.106: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020992287s
Mar 26 17:17:11.112: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.027715784s
Mar 26 17:17:11.113: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 26 17:17:11.113: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 26 17:17:11.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865'
Mar 26 17:17:11.228: INFO: stderr: ""
Mar 26 17:17:11.228: INFO: stdout: "I0326 17:17:08.431710       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/5c4r 221\nI0326 17:17:08.631968       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/p8k9 368\nI0326 17:17:08.831961       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/l6k 261\nI0326 17:17:09.031886       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/889 443\nI0326 17:17:09.231975       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/npr 487\nI0326 17:17:09.431898       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/k2fq 228\nI0326 17:17:09.631943       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/hsk2 363\nI0326 17:17:09.831983       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wqm 329\nI0326 17:17:10.032488       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/vzj 220\nI0326 17:17:10.248639       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/4mf 323\nI0326 17:17:10.431920       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bltn 561\nI0326 17:17:10.632009       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/mbqf 235\nI0326 17:17:10.832085       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/c26z 300\nI0326 17:17:11.031965       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vwk 470\n"
STEP: limiting log lines
Mar 26 17:17:11.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865 --tail=1'
Mar 26 17:17:11.336: INFO: stderr: ""
Mar 26 17:17:11.336: INFO: stdout: "I0326 17:17:11.231840       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dbdm 341\n"
Mar 26 17:17:11.336: INFO: got output "I0326 17:17:11.231840       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dbdm 341\n"
STEP: limiting log bytes
Mar 26 17:17:11.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865 --limit-bytes=1'
Mar 26 17:17:11.472: INFO: stderr: ""
Mar 26 17:17:11.472: INFO: stdout: "I"
Mar 26 17:17:11.472: INFO: got output "I"
STEP: exposing timestamps
Mar 26 17:17:11.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865 --tail=1 --timestamps'
Mar 26 17:17:11.726: INFO: stderr: ""
Mar 26 17:17:11.726: INFO: stdout: "2020-03-26T17:17:11.63673196Z I0326 17:17:11.631919       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/x5tt 203\n"
Mar 26 17:17:11.726: INFO: got output "2020-03-26T17:17:11.63673196Z I0326 17:17:11.631919       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/x5tt 203\n"
STEP: restricting to a time range
Mar 26 17:17:14.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865 --since=1s'
Mar 26 17:17:14.361: INFO: stderr: ""
Mar 26 17:17:14.361: INFO: stdout: "I0326 17:17:13.431927       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzz 303\nI0326 17:17:13.631996       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/xbs 397\nI0326 17:17:13.832047       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/trzn 201\nI0326 17:17:14.031917       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/mfn 310\nI0326 17:17:14.231971       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/4jnk 369\n"
Mar 26 17:17:14.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 logs logs-generator logs-generator --namespace=kubectl-9865 --since=24h'
Mar 26 17:17:14.497: INFO: stderr: ""
Mar 26 17:17:14.497: INFO: stdout: "I0326 17:17:08.431710       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/5c4r 221\nI0326 17:17:08.631968       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/p8k9 368\nI0326 17:17:08.831961       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/l6k 261\nI0326 17:17:09.031886       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/889 443\nI0326 17:17:09.231975       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/npr 487\nI0326 17:17:09.431898       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/k2fq 228\nI0326 17:17:09.631943       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/hsk2 363\nI0326 17:17:09.831983       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wqm 329\nI0326 17:17:10.032488       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/vzj 220\nI0326 17:17:10.248639       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/4mf 323\nI0326 17:17:10.431920       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bltn 561\nI0326 17:17:10.632009       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/mbqf 235\nI0326 17:17:10.832085       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/c26z 300\nI0326 17:17:11.031965       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vwk 470\nI0326 17:17:11.231840       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dbdm 341\nI0326 17:17:11.431940       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/8xg 252\nI0326 17:17:11.631919       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/x5tt 203\nI0326 17:17:11.831923       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/6db 360\nI0326 17:17:12.032010       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/x6b 262\nI0326 17:17:12.231949       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hwxk 379\nI0326 17:17:12.431858       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/d6r 308\nI0326 17:17:12.631941       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ds2 396\nI0326 17:17:12.831976       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/pd5 222\nI0326 17:17:13.032079       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/s8pq 514\nI0326 17:17:13.231922       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/zbvt 581\nI0326 17:17:13.431927       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzz 303\nI0326 17:17:13.631996       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/xbs 397\nI0326 17:17:13.832047       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/trzn 201\nI0326 17:17:14.031917       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/mfn 310\nI0326 17:17:14.231971       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/4jnk 369\nI0326 17:17:14.431867       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/7n56 300\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Mar 26 17:17:14.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete pod logs-generator --namespace=kubectl-9865'
Mar 26 17:17:16.270: INFO: stderr: ""
Mar 26 17:17:16.270: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:16.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9865" for this suite.

â€¢ [SLOW TEST:9.675 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":275,"completed":151,"skipped":2378,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Mar 26 17:17:16.459: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2343" to be "Succeeded or Failed"
Mar 26 17:17:16.487: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 27.580676ms
Mar 26 17:17:18.490: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030616351s
Mar 26 17:17:20.496: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03634107s
STEP: Saw pod success
Mar 26 17:17:20.496: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Mar 26 17:17:20.500: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 26 17:17:20.531: INFO: Waiting for pod pod-host-path-test to disappear
Mar 26 17:17:20.534: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:20.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2343" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":152,"skipped":2386,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:20.568: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:17:20.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e" in namespace "downward-api-295" to be "Succeeded or Failed"
Mar 26 17:17:20.780: INFO: Pod "downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.926739ms
Mar 26 17:17:22.783: INFO: Pod "downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010201343s
Mar 26 17:17:24.797: INFO: Pod "downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023922381s
STEP: Saw pod success
Mar 26 17:17:24.797: INFO: Pod "downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e" satisfied condition "Succeeded or Failed"
Mar 26 17:17:24.818: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e container client-container: <nil>
STEP: delete the pod
Mar 26 17:17:24.851: INFO: Waiting for pod downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e to disappear
Mar 26 17:17:24.863: INFO: Pod downwardapi-volume-aa83fd70-fc40-42a0-9be4-9472ddf8503e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:24.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-295" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":275,"completed":153,"skipped":2398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:24.877: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-fc5508fa-ef36-431b-9214-077d0543acd6
STEP: Creating a pod to test consume secrets
Mar 26 17:17:25.127: INFO: Waiting up to 5m0s for pod "pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207" in namespace "secrets-9920" to be "Succeeded or Failed"
Mar 26 17:17:25.133: INFO: Pod "pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847224ms
Mar 26 17:17:27.136: INFO: Pod "pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009438017s
STEP: Saw pod success
Mar 26 17:17:27.136: INFO: Pod "pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207" satisfied condition "Succeeded or Failed"
Mar 26 17:17:27.139: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:17:27.170: INFO: Waiting for pod pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207 to disappear
Mar 26 17:17:27.177: INFO: Pod pod-secrets-314664f2-4fd7-46ed-8ade-ea0e824b6207 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:27.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9920" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":154,"skipped":2464,"failed":0}

------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:17:27.363: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334" in namespace "security-context-test-4219" to be "Succeeded or Failed"
Mar 26 17:17:27.380: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334": Phase="Pending", Reason="", readiness=false. Elapsed: 16.383417ms
Mar 26 17:17:29.385: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022136827s
Mar 26 17:17:31.391: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027297148s
Mar 26 17:17:33.405: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042111658s
Mar 26 17:17:35.411: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.048050366s
Mar 26 17:17:35.411: INFO: Pod "alpine-nnp-false-402bf41f-c507-48dc-94ad-b778f9e81334" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4219" for this suite.

â€¢ [SLOW TEST:8.245 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":155,"skipped":2464,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:35.433: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:35.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8129" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":275,"completed":156,"skipped":2471,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:35.612: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 26 17:17:35.792: INFO: Waiting up to 5m0s for pod "pod-821350a2-b326-45f5-8ae8-ea172aa717cb" in namespace "emptydir-923" to be "Succeeded or Failed"
Mar 26 17:17:35.797: INFO: Pod "pod-821350a2-b326-45f5-8ae8-ea172aa717cb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201432ms
Mar 26 17:17:37.803: INFO: Pod "pod-821350a2-b326-45f5-8ae8-ea172aa717cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010583888s
Mar 26 17:17:39.808: INFO: Pod "pod-821350a2-b326-45f5-8ae8-ea172aa717cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016547396s
STEP: Saw pod success
Mar 26 17:17:39.809: INFO: Pod "pod-821350a2-b326-45f5-8ae8-ea172aa717cb" satisfied condition "Succeeded or Failed"
Mar 26 17:17:39.814: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-821350a2-b326-45f5-8ae8-ea172aa717cb container test-container: <nil>
STEP: delete the pod
Mar 26 17:17:39.844: INFO: Waiting for pod pod-821350a2-b326-45f5-8ae8-ea172aa717cb to disappear
Mar 26 17:17:39.846: INFO: Pod pod-821350a2-b326-45f5-8ae8-ea172aa717cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:39.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-923" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":157,"skipped":2481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:39.857: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:17:40.058: INFO: Waiting up to 5m0s for pod "downward-api-069104b7-a7d1-4737-b815-eeb1f653502a" in namespace "downward-api-6259" to be "Succeeded or Failed"
Mar 26 17:17:40.069: INFO: Pod "downward-api-069104b7-a7d1-4737-b815-eeb1f653502a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.321868ms
Mar 26 17:17:42.080: INFO: Pod "downward-api-069104b7-a7d1-4737-b815-eeb1f653502a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022039327s
STEP: Saw pod success
Mar 26 17:17:42.080: INFO: Pod "downward-api-069104b7-a7d1-4737-b815-eeb1f653502a" satisfied condition "Succeeded or Failed"
Mar 26 17:17:42.084: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downward-api-069104b7-a7d1-4737-b815-eeb1f653502a container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:17:42.118: INFO: Waiting for pod downward-api-069104b7-a7d1-4737-b815-eeb1f653502a to disappear
Mar 26 17:17:42.137: INFO: Pod downward-api-069104b7-a7d1-4737-b815-eeb1f653502a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:42.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6259" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":275,"completed":158,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:17:42.449: INFO: Waiting up to 5m0s for pod "downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5" in namespace "downward-api-9650" to be "Succeeded or Failed"
Mar 26 17:17:42.470: INFO: Pod "downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.086663ms
Mar 26 17:17:44.479: INFO: Pod "downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030267247s
STEP: Saw pod success
Mar 26 17:17:44.479: INFO: Pod "downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5" satisfied condition "Succeeded or Failed"
Mar 26 17:17:44.483: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5 container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:17:44.517: INFO: Waiting for pod downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5 to disappear
Mar 26 17:17:44.522: INFO: Pod downward-api-35c552e5-b4b7-4bc9-944c-1fa7255f7fb5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:44.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9650" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":275,"completed":159,"skipped":2531,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:44.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8285" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":275,"completed":160,"skipped":2546,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:44.718: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 26 17:17:44.889: INFO: Waiting up to 5m0s for pod "pod-a212b70d-3d44-4fda-a515-60044c4fa5a3" in namespace "emptydir-2772" to be "Succeeded or Failed"
Mar 26 17:17:44.892: INFO: Pod "pod-a212b70d-3d44-4fda-a515-60044c4fa5a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556873ms
Mar 26 17:17:46.896: INFO: Pod "pod-a212b70d-3d44-4fda-a515-60044c4fa5a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006688893s
Mar 26 17:17:48.902: INFO: Pod "pod-a212b70d-3d44-4fda-a515-60044c4fa5a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012931821s
STEP: Saw pod success
Mar 26 17:17:48.902: INFO: Pod "pod-a212b70d-3d44-4fda-a515-60044c4fa5a3" satisfied condition "Succeeded or Failed"
Mar 26 17:17:48.906: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-a212b70d-3d44-4fda-a515-60044c4fa5a3 container test-container: <nil>
STEP: delete the pod
Mar 26 17:17:48.935: INFO: Waiting for pod pod-a212b70d-3d44-4fda-a515-60044c4fa5a3 to disappear
Mar 26 17:17:48.941: INFO: Pod pod-a212b70d-3d44-4fda-a515-60044c4fa5a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:48.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2772" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":161,"skipped":2555,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-1651fa8b-e1a1-4e7c-93be-3df34e424a6f
STEP: Creating a pod to test consume configMaps
Mar 26 17:17:49.178: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0" in namespace "projected-9596" to be "Succeeded or Failed"
Mar 26 17:17:49.182: INFO: Pod "pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688147ms
Mar 26 17:17:51.186: INFO: Pod "pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008374259s
Mar 26 17:17:53.192: INFO: Pod "pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013910538s
STEP: Saw pod success
Mar 26 17:17:53.192: INFO: Pod "pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0" satisfied condition "Succeeded or Failed"
Mar 26 17:17:53.198: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:17:53.217: INFO: Waiting for pod pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0 to disappear
Mar 26 17:17:53.220: INFO: Pod pod-projected-configmaps-32c91545-1418-4bed-8759-d6be251015d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:17:53.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9596" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":162,"skipped":2567,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:17:53.227: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:17:53.429: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:17:55.435: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:17:57.434: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:17:59.433: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:01.436: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:03.435: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:05.435: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:07.433: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:09.437: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:11.438: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:13.441: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:15.436: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:17.433: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = false)
Mar 26 17:18:19.432: INFO: The status of Pod test-webserver-4cc7b91a-4964-4010-ae85-0281d161528d is Running (Ready = true)
Mar 26 17:18:19.434: INFO: Container started at 2020-03-26 17:17:54 +0000 UTC, pod became ready at 2020-03-26 17:18:18 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:18:19.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8257" for this suite.

â€¢ [SLOW TEST:26.216 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":275,"completed":163,"skipped":2571,"failed":0}
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:18:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Mar 26 17:18:19.687: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar 26 17:18:19.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:21.257: INFO: stderr: ""
Mar 26 17:18:21.257: INFO: stdout: "service/agnhost-slave created\n"
Mar 26 17:18:21.257: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar 26 17:18:21.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:22.845: INFO: stderr: ""
Mar 26 17:18:22.845: INFO: stdout: "service/agnhost-master created\n"
Mar 26 17:18:22.845: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 26 17:18:22.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:24.558: INFO: stderr: ""
Mar 26 17:18:24.558: INFO: stdout: "service/frontend created\n"
Mar 26 17:18:24.558: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 26 17:18:24.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:25.389: INFO: stderr: ""
Mar 26 17:18:25.389: INFO: stdout: "deployment.apps/frontend created\n"
Mar 26 17:18:25.389: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 26 17:18:25.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:27.470: INFO: stderr: ""
Mar 26 17:18:27.470: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar 26 17:18:27.470: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 26 17:18:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-4358'
Mar 26 17:18:29.573: INFO: stderr: ""
Mar 26 17:18:29.574: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar 26 17:18:29.574: INFO: Waiting for all frontend pods to be Running.
Mar 26 17:18:29.627: INFO: Waiting for frontend to serve content.
Mar 26 17:18:30.681: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Mar 26 17:18:35.700: INFO: Trying to add a new entry to the guestbook.
Mar 26 17:18:35.711: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 26 17:18:35.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:35.919: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:35.919: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 17:18:35.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:36.101: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:36.101: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 17:18:36.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:36.416: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:36.416: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 17:18:36.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:36.565: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:36.565: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 17:18:36.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:36.968: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:36.968: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 17:18:36.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-4358'
Mar 26 17:18:37.315: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:18:37.315: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:18:37.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4358" for this suite.

â€¢ [SLOW TEST:17.920 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":275,"completed":164,"skipped":2571,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:18:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-225
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-225
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-225
Mar 26 17:18:37.908: INFO: Found 0 stateful pods, waiting for 1
Mar 26 17:18:47.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 26 17:18:47.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:18:48.252: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:18:48.252: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:18:48.252: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:18:48.255: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 17:18:58.263: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:18:58.263: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:18:58.307: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:18:58.308: INFO: ss-0  kubedee-test-worker-dqhapg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:18:58.326: INFO: ss-1                              Pending         []
Mar 26 17:18:58.326: INFO: 
Mar 26 17:18:58.326: INFO: StatefulSet ss has not reached scale 3, at 2
Mar 26 17:18:59.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.95577049s
Mar 26 17:19:00.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95163752s
Mar 26 17:19:01.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.939521445s
Mar 26 17:19:02.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.932075759s
Mar 26 17:19:03.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.921314657s
Mar 26 17:19:04.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914214782s
Mar 26 17:19:05.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.905681992s
Mar 26 17:19:06.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.896976115s
Mar 26 17:19:07.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 891.455672ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-225
Mar 26 17:19:08.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:19:08.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 17:19:08.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:19:08.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:19:08.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:19:08.763: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 17:19:08.763: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:19:08.763: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:19:08.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:19:08.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 17:19:08.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:19:08.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:19:08.939: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 26 17:19:18.948: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:19:18.948: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:19:18.948: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 26 17:19:18.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:19:19.153: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:19:19.153: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:19:19.153: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:19:19.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:19:19.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:19:19.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:19:19.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:19:19.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 exec --namespace=statefulset-225 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:19:19.635: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:19:19.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:19:19.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:19:19.635: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:19:19.652: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 26 17:19:29.662: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:19:29.662: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:19:29.662: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:19:29.679: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:29.679: INFO: ss-0  kubedee-test-worker-dqhapg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:29.679: INFO: ss-1  kubedee-test-worker-vf6bys  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:29.679: INFO: ss-2  kubedee-test-worker-dqhapg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:29.679: INFO: 
Mar 26 17:19:29.679: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:19:30.722: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:30.722: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:30.722: INFO: ss-1  kubedee-test-worker-vf6bys  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:30.722: INFO: ss-2  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:30.722: INFO: 
Mar 26 17:19:30.722: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:19:31.728: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:31.729: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:31.729: INFO: ss-1  kubedee-test-worker-vf6bys  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:31.729: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:31.729: INFO: 
Mar 26 17:19:31.729: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:19:32.736: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:32.736: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:32.736: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:32.736: INFO: 
Mar 26 17:19:32.736: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:33.743: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:33.743: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:33.743: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:33.743: INFO: 
Mar 26 17:19:33.743: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:34.750: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:34.750: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:34.750: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:34.750: INFO: 
Mar 26 17:19:34.750: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:35.766: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:35.766: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:35.766: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:35.766: INFO: 
Mar 26 17:19:35.766: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:36.769: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:36.769: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:36.770: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:36.770: INFO: 
Mar 26 17:19:36.770: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:37.776: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Mar 26 17:19:37.776: INFO: ss-0  kubedee-test-worker-dqhapg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:37 +0000 UTC  }]
Mar 26 17:19:37.776: INFO: ss-2  kubedee-test-worker-dqhapg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:19:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:18:58 +0000 UTC  }]
Mar 26 17:19:37.776: INFO: 
Mar 26 17:19:37.776: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:19:38.781: INFO: Verifying statefulset ss doesn't scale past 0 for another 896.260607ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-225
Mar 26 17:19:39.804: INFO: Scaling statefulset ss to 0
Mar 26 17:19:39.817: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:19:39.819: INFO: Deleting all statefulset in ns statefulset-225
Mar 26 17:19:39.822: INFO: Scaling statefulset ss to 0
Mar 26 17:19:39.829: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:19:39.831: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:19:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-225" for this suite.

â€¢ [SLOW TEST:62.496 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":275,"completed":165,"skipped":2572,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:19:39.861: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:19:40.141: INFO: Waiting up to 5m0s for pod "downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd" in namespace "downward-api-1665" to be "Succeeded or Failed"
Mar 26 17:19:40.146: INFO: Pod "downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.762943ms
Mar 26 17:19:42.152: INFO: Pod "downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010334057s
STEP: Saw pod success
Mar 26 17:19:42.152: INFO: Pod "downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd" satisfied condition "Succeeded or Failed"
Mar 26 17:19:42.158: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:19:42.224: INFO: Waiting for pod downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd to disappear
Mar 26 17:19:42.243: INFO: Pod downward-api-bf1214ba-e980-4f22-bc9f-8f3ac8d093bd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:19:42.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1665" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":275,"completed":166,"skipped":2576,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:19:42.264: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-05526d68-777c-4a31-b0e6-d38688d8347d
STEP: Creating a pod to test consume secrets
Mar 26 17:19:42.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42" in namespace "projected-1926" to be "Succeeded or Failed"
Mar 26 17:19:42.453: INFO: Pod "pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42": Phase="Pending", Reason="", readiness=false. Elapsed: 21.498832ms
Mar 26 17:19:44.458: INFO: Pod "pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026593087s
Mar 26 17:19:46.463: INFO: Pod "pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031988281s
STEP: Saw pod success
Mar 26 17:19:46.463: INFO: Pod "pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42" satisfied condition "Succeeded or Failed"
Mar 26 17:19:46.468: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:19:46.487: INFO: Waiting for pod pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42 to disappear
Mar 26 17:19:46.489: INFO: Pod pod-projected-secrets-75129f42-8ba9-4b2f-9a3d-789fb9202f42 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:19:46.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1926" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":167,"skipped":2577,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:19:46.496: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3837
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 26 17:19:46.644: INFO: Waiting up to 5m0s for pod "pod-74efe029-340e-40e5-b610-e9a2dc7b9adc" in namespace "emptydir-3837" to be "Succeeded or Failed"
Mar 26 17:19:46.657: INFO: Pod "pod-74efe029-340e-40e5-b610-e9a2dc7b9adc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.693119ms
Mar 26 17:19:48.663: INFO: Pod "pod-74efe029-340e-40e5-b610-e9a2dc7b9adc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019272096s
Mar 26 17:19:50.669: INFO: Pod "pod-74efe029-340e-40e5-b610-e9a2dc7b9adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024835321s
STEP: Saw pod success
Mar 26 17:19:50.669: INFO: Pod "pod-74efe029-340e-40e5-b610-e9a2dc7b9adc" satisfied condition "Succeeded or Failed"
Mar 26 17:19:50.674: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-74efe029-340e-40e5-b610-e9a2dc7b9adc container test-container: <nil>
STEP: delete the pod
Mar 26 17:19:50.709: INFO: Waiting for pod pod-74efe029-340e-40e5-b610-e9a2dc7b9adc to disappear
Mar 26 17:19:50.711: INFO: Pod pod-74efe029-340e-40e5-b610-e9a2dc7b9adc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:19:50.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3837" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":168,"skipped":2585,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:19:50.718: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 26 17:19:50.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 create -f - --namespace=kubectl-1318'
Mar 26 17:19:52.231: INFO: stderr: ""
Mar 26 17:19:52.231: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 17:19:52.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1318'
Mar 26 17:19:52.374: INFO: stderr: ""
Mar 26 17:19:52.374: INFO: stdout: "update-demo-nautilus-hhhsw update-demo-nautilus-jd8fb "
Mar 26 17:19:52.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-hhhsw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1318'
Mar 26 17:19:52.508: INFO: stderr: ""
Mar 26 17:19:52.508: INFO: stdout: ""
Mar 26 17:19:52.508: INFO: update-demo-nautilus-hhhsw is created but not running
Mar 26 17:19:57.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1318'
Mar 26 17:19:57.614: INFO: stderr: ""
Mar 26 17:19:57.614: INFO: stdout: "update-demo-nautilus-hhhsw update-demo-nautilus-jd8fb "
Mar 26 17:19:57.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-hhhsw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1318'
Mar 26 17:19:57.702: INFO: stderr: ""
Mar 26 17:19:57.702: INFO: stdout: "true"
Mar 26 17:19:57.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-hhhsw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1318'
Mar 26 17:19:57.793: INFO: stderr: ""
Mar 26 17:19:57.793: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:19:57.793: INFO: validating pod update-demo-nautilus-hhhsw
Mar 26 17:19:57.797: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:19:57.807: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:19:57.807: INFO: update-demo-nautilus-hhhsw is verified up and running
Mar 26 17:19:57.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jd8fb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1318'
Mar 26 17:19:57.918: INFO: stderr: ""
Mar 26 17:19:57.918: INFO: stdout: "true"
Mar 26 17:19:57.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods update-demo-nautilus-jd8fb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1318'
Mar 26 17:19:58.033: INFO: stderr: ""
Mar 26 17:19:58.033: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 17:19:58.033: INFO: validating pod update-demo-nautilus-jd8fb
Mar 26 17:19:58.041: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 17:19:58.041: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 17:19:58.041: INFO: update-demo-nautilus-jd8fb is verified up and running
STEP: using delete to clean up resources
Mar 26 17:19:58.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete --grace-period=0 --force -f - --namespace=kubectl-1318'
Mar 26 17:19:58.160: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 17:19:58.160: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 17:19:58.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1318'
Mar 26 17:19:58.265: INFO: stderr: "No resources found in kubectl-1318 namespace.\n"
Mar 26 17:19:58.265: INFO: stdout: ""
Mar 26 17:19:58.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -l name=update-demo --namespace=kubectl-1318 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 17:19:58.347: INFO: stderr: ""
Mar 26 17:19:58.347: INFO: stdout: "update-demo-nautilus-hhhsw\nupdate-demo-nautilus-jd8fb\n"
Mar 26 17:19:58.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1318'
Mar 26 17:19:59.159: INFO: stderr: "No resources found in kubectl-1318 namespace.\n"
Mar 26 17:19:59.159: INFO: stdout: ""
Mar 26 17:19:59.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pods -l name=update-demo --namespace=kubectl-1318 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 17:19:59.268: INFO: stderr: ""
Mar 26 17:19:59.268: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:19:59.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1318" for this suite.

â€¢ [SLOW TEST:8.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":275,"completed":169,"skipped":2587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:19:59.288: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-8ckc
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:19:59.452: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8ckc" in namespace "subpath-3852" to be "Succeeded or Failed"
Mar 26 17:19:59.458: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.953935ms
Mar 26 17:20:01.463: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010609088s
Mar 26 17:20:03.469: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 4.01642231s
Mar 26 17:20:05.474: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 6.021218202s
Mar 26 17:20:07.479: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 8.026168974s
Mar 26 17:20:09.483: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 10.029669988s
Mar 26 17:20:11.489: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 12.035958035s
Mar 26 17:20:13.494: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 14.041284199s
Mar 26 17:20:15.501: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 16.048118211s
Mar 26 17:20:17.509: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 18.055996051s
Mar 26 17:20:19.515: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Running", Reason="", readiness=true. Elapsed: 20.06262483s
Mar 26 17:20:21.519: INFO: Pod "pod-subpath-test-configmap-8ckc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06572535s
STEP: Saw pod success
Mar 26 17:20:21.519: INFO: Pod "pod-subpath-test-configmap-8ckc" satisfied condition "Succeeded or Failed"
Mar 26 17:20:21.520: INFO: Trying to get logs from node kubedee-test-worker-vf6bys pod pod-subpath-test-configmap-8ckc container test-container-subpath-configmap-8ckc: <nil>
STEP: delete the pod
Mar 26 17:20:21.575: INFO: Waiting for pod pod-subpath-test-configmap-8ckc to disappear
Mar 26 17:20:21.580: INFO: Pod pod-subpath-test-configmap-8ckc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8ckc
Mar 26 17:20:21.580: INFO: Deleting pod "pod-subpath-test-configmap-8ckc" in namespace "subpath-3852"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:20:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3852" for this suite.

â€¢ [SLOW TEST:22.310 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":275,"completed":170,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:20:21.603: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:20:22.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:20:24.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840022, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840022, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840022, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840022, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:20:27.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:20:27.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6751" for this suite.
STEP: Destroying namespace "webhook-6751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":275,"completed":171,"skipped":2724,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:20:27.756: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7486
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:20:27.962: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:20:33.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7486" for this suite.

â€¢ [SLOW TEST:5.836 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":275,"completed":172,"skipped":2733,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:20:33.594: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 17:20:36.416: INFO: Successfully updated pod "labelsupdate6310a140-db7e-408c-aeb4-4202abeabb39"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:20:40.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8437" for this suite.

â€¢ [SLOW TEST:6.863 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":173,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:20:40.464: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:20:41.039: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 26 17:20:43.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840041, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840041, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840041, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840041, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:20:46.083: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:20:46.088: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:20:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6495" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:11.856 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":275,"completed":174,"skipped":2759,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:20:52.320: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:20:52.486: INFO: Creating ReplicaSet my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e
Mar 26 17:20:52.511: INFO: Pod name my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e: Found 0 pods out of 1
Mar 26 17:20:57.522: INFO: Pod name my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e: Found 1 pods out of 1
Mar 26 17:20:57.523: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e" is running
Mar 26 17:20:57.527: INFO: Pod "my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e-vvd94" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:20:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:20:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:20:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:20:52 +0000 UTC Reason: Message:}])
Mar 26 17:20:57.527: INFO: Trying to dial the pod
Mar 26 17:21:02.570: INFO: Controller my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e: Got expected result from replica 1 [my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e-vvd94]: "my-hostname-basic-e874d7d4-2452-49aa-a3bd-5953a4cab36e-vvd94", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:21:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8056" for this suite.

â€¢ [SLOW TEST:10.267 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":175,"skipped":2759,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:21:02.589: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8439
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:21:02.784: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:22:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8439" for this suite.

â€¢ [SLOW TEST:62.013 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":275,"completed":176,"skipped":2760,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:22:04.604: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6512
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 26 17:22:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:22:29.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6512" for this suite.

â€¢ [SLOW TEST:24.542 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":275,"completed":177,"skipped":2772,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:22:29.148: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:22:29.412: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ed550c98-9169-47e1-9420-8d380230a160", Controller:(*bool)(0xc00407675a), BlockOwnerDeletion:(*bool)(0xc00407675b)}}
Mar 26 17:22:29.419: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4375f910-2db4-42d7-a664-4b12828f3352", Controller:(*bool)(0xc0040769b6), BlockOwnerDeletion:(*bool)(0xc0040769b7)}}
Mar 26 17:22:29.426: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9386dd23-0379-4122-8825-de479f8d6744", Controller:(*bool)(0xc00405152a), BlockOwnerDeletion:(*bool)(0xc00405152b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:22:34.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6438" for this suite.

â€¢ [SLOW TEST:5.321 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":275,"completed":178,"skipped":2777,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:22:34.469: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-sdjbx in namespace proxy-3203
I0326 17:22:34.765495      21 runners.go:190] Created replication controller with name: proxy-service-sdjbx, namespace: proxy-3203, replica count: 1
I0326 17:22:35.822977      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:22:36.823349      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:37.823699      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:38.824203      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:39.824486      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:40.824992      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:41.825325      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 17:22:42.825824      21 runners.go:190] proxy-service-sdjbx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:22:42.848: INFO: setup took 8.184979556s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 26 17:22:42.871: INFO: (0) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 15.226258ms)
Mar 26 17:22:42.878: INFO: (0) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 21.468286ms)
Mar 26 17:22:42.878: INFO: (0) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 22.1302ms)
Mar 26 17:22:42.879: INFO: (0) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 22.262374ms)
Mar 26 17:22:42.884: INFO: (0) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 27.251707ms)
Mar 26 17:22:42.887: INFO: (0) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 29.103381ms)
Mar 26 17:22:42.891: INFO: (0) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 33.356606ms)
Mar 26 17:22:42.892: INFO: (0) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 34.039466ms)
Mar 26 17:22:42.892: INFO: (0) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 33.976213ms)
Mar 26 17:22:42.892: INFO: (0) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 34.939191ms)
Mar 26 17:22:42.897: INFO: (0) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 39.632752ms)
Mar 26 17:22:42.898: INFO: (0) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 40.591135ms)
Mar 26 17:22:42.898: INFO: (0) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 41.905582ms)
Mar 26 17:22:42.899: INFO: (0) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 42.472815ms)
Mar 26 17:22:42.899: INFO: (0) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 42.19282ms)
Mar 26 17:22:42.902: INFO: (0) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 43.997044ms)
Mar 26 17:22:42.915: INFO: (1) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 12.463101ms)
Mar 26 17:22:42.915: INFO: (1) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 12.396235ms)
Mar 26 17:22:42.915: INFO: (1) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 12.651068ms)
Mar 26 17:22:42.915: INFO: (1) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 12.795663ms)
Mar 26 17:22:42.919: INFO: (1) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 16.369006ms)
Mar 26 17:22:42.919: INFO: (1) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 16.747204ms)
Mar 26 17:22:42.919: INFO: (1) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 17.071077ms)
Mar 26 17:22:42.919: INFO: (1) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 16.58054ms)
Mar 26 17:22:42.920: INFO: (1) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 17.161136ms)
Mar 26 17:22:42.922: INFO: (1) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 19.453661ms)
Mar 26 17:22:42.922: INFO: (1) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 19.438925ms)
Mar 26 17:22:42.922: INFO: (1) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 20.221293ms)
Mar 26 17:22:42.922: INFO: (1) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 20.490083ms)
Mar 26 17:22:42.923: INFO: (1) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 20.426254ms)
Mar 26 17:22:42.923: INFO: (1) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 20.998686ms)
Mar 26 17:22:42.924: INFO: (1) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 21.291145ms)
Mar 26 17:22:42.931: INFO: (2) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 7.231241ms)
Mar 26 17:22:42.933: INFO: (2) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 9.545488ms)
Mar 26 17:22:42.936: INFO: (2) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.1251ms)
Mar 26 17:22:42.937: INFO: (2) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 12.600123ms)
Mar 26 17:22:42.938: INFO: (2) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 13.908139ms)
Mar 26 17:22:42.938: INFO: (2) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 13.872347ms)
Mar 26 17:22:42.940: INFO: (2) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 14.870038ms)
Mar 26 17:22:42.940: INFO: (2) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 15.945046ms)
Mar 26 17:22:42.941: INFO: (2) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 16.641389ms)
Mar 26 17:22:42.943: INFO: (2) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 18.394024ms)
Mar 26 17:22:42.943: INFO: (2) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 18.554757ms)
Mar 26 17:22:42.944: INFO: (2) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 19.968433ms)
Mar 26 17:22:42.945: INFO: (2) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 19.670969ms)
Mar 26 17:22:42.945: INFO: (2) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 20.494412ms)
Mar 26 17:22:42.945: INFO: (2) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 20.567318ms)
Mar 26 17:22:42.945: INFO: (2) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 20.604875ms)
Mar 26 17:22:42.952: INFO: (3) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 6.336231ms)
Mar 26 17:22:42.952: INFO: (3) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 6.491498ms)
Mar 26 17:22:42.952: INFO: (3) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 6.926745ms)
Mar 26 17:22:42.952: INFO: (3) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 7.099046ms)
Mar 26 17:22:42.956: INFO: (3) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 9.830459ms)
Mar 26 17:22:42.957: INFO: (3) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 11.555903ms)
Mar 26 17:22:42.958: INFO: (3) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.334284ms)
Mar 26 17:22:42.959: INFO: (3) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 13.478809ms)
Mar 26 17:22:42.960: INFO: (3) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 14.093386ms)
Mar 26 17:22:42.962: INFO: (3) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 15.924877ms)
Mar 26 17:22:42.965: INFO: (3) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 19.134432ms)
Mar 26 17:22:42.967: INFO: (3) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 20.814025ms)
Mar 26 17:22:42.967: INFO: (3) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 21.775847ms)
Mar 26 17:22:42.968: INFO: (3) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 22.560971ms)
Mar 26 17:22:42.969: INFO: (3) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 23.299292ms)
Mar 26 17:22:42.969: INFO: (3) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 23.306756ms)
Mar 26 17:22:42.984: INFO: (4) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 14.515271ms)
Mar 26 17:22:42.985: INFO: (4) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 15.292459ms)
Mar 26 17:22:42.987: INFO: (4) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 17.959667ms)
Mar 26 17:22:42.990: INFO: (4) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 20.563144ms)
Mar 26 17:22:42.992: INFO: (4) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 23.228349ms)
Mar 26 17:22:42.993: INFO: (4) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 23.339263ms)
Mar 26 17:22:42.994: INFO: (4) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 24.115683ms)
Mar 26 17:22:42.995: INFO: (4) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 24.732653ms)
Mar 26 17:22:42.995: INFO: (4) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 25.385904ms)
Mar 26 17:22:42.995: INFO: (4) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 26.119381ms)
Mar 26 17:22:42.996: INFO: (4) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 25.91797ms)
Mar 26 17:22:42.997: INFO: (4) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 26.823613ms)
Mar 26 17:22:42.998: INFO: (4) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 28.268567ms)
Mar 26 17:22:42.998: INFO: (4) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 28.425331ms)
Mar 26 17:22:42.999: INFO: (4) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 28.635164ms)
Mar 26 17:22:42.999: INFO: (4) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 29.55787ms)
Mar 26 17:22:43.010: INFO: (5) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 10.26588ms)
Mar 26 17:22:43.011: INFO: (5) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 11.526237ms)
Mar 26 17:22:43.011: INFO: (5) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 11.015348ms)
Mar 26 17:22:43.011: INFO: (5) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 10.897725ms)
Mar 26 17:22:43.011: INFO: (5) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 10.918262ms)
Mar 26 17:22:43.012: INFO: (5) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 11.639032ms)
Mar 26 17:22:43.012: INFO: (5) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 11.806961ms)
Mar 26 17:22:43.013: INFO: (5) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 13.383727ms)
Mar 26 17:22:43.014: INFO: (5) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 13.979198ms)
Mar 26 17:22:43.014: INFO: (5) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 14.108832ms)
Mar 26 17:22:43.017: INFO: (5) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 17.100994ms)
Mar 26 17:22:43.017: INFO: (5) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 17.327323ms)
Mar 26 17:22:43.017: INFO: (5) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 17.837363ms)
Mar 26 17:22:43.018: INFO: (5) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 18.213158ms)
Mar 26 17:22:43.018: INFO: (5) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 18.975464ms)
Mar 26 17:22:43.019: INFO: (5) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 18.605768ms)
Mar 26 17:22:43.027: INFO: (6) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 8.841978ms)
Mar 26 17:22:43.028: INFO: (6) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 9.162472ms)
Mar 26 17:22:43.029: INFO: (6) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 10.074351ms)
Mar 26 17:22:43.029: INFO: (6) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 9.985022ms)
Mar 26 17:22:43.029: INFO: (6) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 10.228916ms)
Mar 26 17:22:43.029: INFO: (6) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 10.411344ms)
Mar 26 17:22:43.031: INFO: (6) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 12.534372ms)
Mar 26 17:22:43.034: INFO: (6) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 15.056854ms)
Mar 26 17:22:43.035: INFO: (6) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 16.118802ms)
Mar 26 17:22:43.037: INFO: (6) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 18.18647ms)
Mar 26 17:22:43.038: INFO: (6) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 18.528847ms)
Mar 26 17:22:43.041: INFO: (6) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 21.989988ms)
Mar 26 17:22:43.041: INFO: (6) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 22.085036ms)
Mar 26 17:22:43.041: INFO: (6) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 22.35801ms)
Mar 26 17:22:43.041: INFO: (6) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 22.086431ms)
Mar 26 17:22:43.042: INFO: (6) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 22.355939ms)
Mar 26 17:22:43.052: INFO: (7) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 10.400726ms)
Mar 26 17:22:43.053: INFO: (7) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 11.606772ms)
Mar 26 17:22:43.058: INFO: (7) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 15.542772ms)
Mar 26 17:22:43.058: INFO: (7) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 15.588601ms)
Mar 26 17:22:43.058: INFO: (7) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 15.694855ms)
Mar 26 17:22:43.065: INFO: (7) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 22.547199ms)
Mar 26 17:22:43.066: INFO: (7) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 23.628126ms)
Mar 26 17:22:43.066: INFO: (7) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 23.833622ms)
Mar 26 17:22:43.066: INFO: (7) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 23.888816ms)
Mar 26 17:22:43.067: INFO: (7) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 25.232056ms)
Mar 26 17:22:43.068: INFO: (7) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 25.572631ms)
Mar 26 17:22:43.068: INFO: (7) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 25.8013ms)
Mar 26 17:22:43.069: INFO: (7) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 26.971213ms)
Mar 26 17:22:43.071: INFO: (7) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 29.265365ms)
Mar 26 17:22:43.072: INFO: (7) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 29.560602ms)
Mar 26 17:22:43.073: INFO: (7) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 30.340866ms)
Mar 26 17:22:43.085: INFO: (8) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 11.536234ms)
Mar 26 17:22:43.085: INFO: (8) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 12.01036ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.518794ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 12.648886ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 12.917671ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 12.599698ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 12.475894ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.46728ms)
Mar 26 17:22:43.086: INFO: (8) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 12.604198ms)
Mar 26 17:22:43.094: INFO: (8) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 20.64397ms)
Mar 26 17:22:43.094: INFO: (8) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 20.962913ms)
Mar 26 17:22:43.095: INFO: (8) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 21.109546ms)
Mar 26 17:22:43.095: INFO: (8) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 21.196436ms)
Mar 26 17:22:43.095: INFO: (8) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 21.611642ms)
Mar 26 17:22:43.096: INFO: (8) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 23.138852ms)
Mar 26 17:22:43.097: INFO: (8) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 23.543882ms)
Mar 26 17:22:43.123: INFO: (9) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 25.699626ms)
Mar 26 17:22:43.124: INFO: (9) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 26.302938ms)
Mar 26 17:22:43.124: INFO: (9) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 26.901848ms)
Mar 26 17:22:43.126: INFO: (9) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 28.263461ms)
Mar 26 17:22:43.126: INFO: (9) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 28.39139ms)
Mar 26 17:22:43.126: INFO: (9) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 28.744869ms)
Mar 26 17:22:43.127: INFO: (9) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 29.113168ms)
Mar 26 17:22:43.129: INFO: (9) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 31.253523ms)
Mar 26 17:22:43.129: INFO: (9) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 31.050641ms)
Mar 26 17:22:43.132: INFO: (9) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 34.483205ms)
Mar 26 17:22:43.132: INFO: (9) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 34.578874ms)
Mar 26 17:22:43.133: INFO: (9) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 34.795592ms)
Mar 26 17:22:43.133: INFO: (9) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 35.430469ms)
Mar 26 17:22:43.134: INFO: (9) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 35.705051ms)
Mar 26 17:22:43.134: INFO: (9) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 36.02797ms)
Mar 26 17:22:43.135: INFO: (9) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 36.893885ms)
Mar 26 17:22:43.149: INFO: (10) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 14.147374ms)
Mar 26 17:22:43.150: INFO: (10) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 14.335814ms)
Mar 26 17:22:43.150: INFO: (10) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 14.762377ms)
Mar 26 17:22:43.150: INFO: (10) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 15.283093ms)
Mar 26 17:22:43.150: INFO: (10) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 14.952231ms)
Mar 26 17:22:43.151: INFO: (10) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 16.088557ms)
Mar 26 17:22:43.151: INFO: (10) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 15.799884ms)
Mar 26 17:22:43.151: INFO: (10) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 15.670074ms)
Mar 26 17:22:43.151: INFO: (10) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 15.890924ms)
Mar 26 17:22:43.151: INFO: (10) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 15.79165ms)
Mar 26 17:22:43.154: INFO: (10) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 18.121152ms)
Mar 26 17:22:43.155: INFO: (10) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 20.48192ms)
Mar 26 17:22:43.157: INFO: (10) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 21.24907ms)
Mar 26 17:22:43.157: INFO: (10) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 21.593784ms)
Mar 26 17:22:43.158: INFO: (10) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 22.481333ms)
Mar 26 17:22:43.158: INFO: (10) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 23.113163ms)
Mar 26 17:22:43.176: INFO: (11) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 16.976548ms)
Mar 26 17:22:43.176: INFO: (11) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 17.085548ms)
Mar 26 17:22:43.178: INFO: (11) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 19.387545ms)
Mar 26 17:22:43.179: INFO: (11) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 19.970917ms)
Mar 26 17:22:43.179: INFO: (11) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 20.618396ms)
Mar 26 17:22:43.183: INFO: (11) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 24.223118ms)
Mar 26 17:22:43.183: INFO: (11) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 24.57214ms)
Mar 26 17:22:43.184: INFO: (11) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 25.087815ms)
Mar 26 17:22:43.184: INFO: (11) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 25.258815ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 26.012995ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 26.397277ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 27.060821ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 26.685899ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 26.651836ms)
Mar 26 17:22:43.185: INFO: (11) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 26.359463ms)
Mar 26 17:22:43.186: INFO: (11) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 27.060375ms)
Mar 26 17:22:43.201: INFO: (12) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 14.510227ms)
Mar 26 17:22:43.201: INFO: (12) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 14.510179ms)
Mar 26 17:22:43.202: INFO: (12) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 15.61114ms)
Mar 26 17:22:43.202: INFO: (12) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 15.848563ms)
Mar 26 17:22:43.202: INFO: (12) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 16.135771ms)
Mar 26 17:22:43.202: INFO: (12) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 16.25948ms)
Mar 26 17:22:43.202: INFO: (12) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 16.712599ms)
Mar 26 17:22:43.203: INFO: (12) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 16.318086ms)
Mar 26 17:22:43.203: INFO: (12) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 16.829826ms)
Mar 26 17:22:43.205: INFO: (12) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 19.036553ms)
Mar 26 17:22:43.206: INFO: (12) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 19.738157ms)
Mar 26 17:22:43.206: INFO: (12) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 20.053324ms)
Mar 26 17:22:43.207: INFO: (12) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 20.394246ms)
Mar 26 17:22:43.207: INFO: (12) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 21.052258ms)
Mar 26 17:22:43.207: INFO: (12) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 20.471844ms)
Mar 26 17:22:43.207: INFO: (12) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 20.808446ms)
Mar 26 17:22:43.218: INFO: (13) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 10.580985ms)
Mar 26 17:22:43.220: INFO: (13) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 11.850029ms)
Mar 26 17:22:43.220: INFO: (13) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 12.753788ms)
Mar 26 17:22:43.226: INFO: (13) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 18.578463ms)
Mar 26 17:22:43.226: INFO: (13) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 18.849119ms)
Mar 26 17:22:43.227: INFO: (13) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 19.295962ms)
Mar 26 17:22:43.227: INFO: (13) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 19.572384ms)
Mar 26 17:22:43.228: INFO: (13) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 20.187124ms)
Mar 26 17:22:43.228: INFO: (13) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 19.870228ms)
Mar 26 17:22:43.228: INFO: (13) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 20.272229ms)
Mar 26 17:22:43.229: INFO: (13) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 20.858884ms)
Mar 26 17:22:43.229: INFO: (13) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 21.21525ms)
Mar 26 17:22:43.230: INFO: (13) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 22.568084ms)
Mar 26 17:22:43.230: INFO: (13) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 23.086505ms)
Mar 26 17:22:43.231: INFO: (13) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 23.808867ms)
Mar 26 17:22:43.231: INFO: (13) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 23.721969ms)
Mar 26 17:22:43.239: INFO: (14) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 7.221956ms)
Mar 26 17:22:43.271: INFO: (14) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 39.046448ms)
Mar 26 17:22:43.271: INFO: (14) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 39.569893ms)
Mar 26 17:22:43.271: INFO: (14) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 39.556757ms)
Mar 26 17:22:43.271: INFO: (14) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 39.522112ms)
Mar 26 17:22:43.271: INFO: (14) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 39.735584ms)
Mar 26 17:22:43.273: INFO: (14) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 41.371783ms)
Mar 26 17:22:43.274: INFO: (14) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 42.609125ms)
Mar 26 17:22:43.279: INFO: (14) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 46.97031ms)
Mar 26 17:22:43.279: INFO: (14) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 47.215954ms)
Mar 26 17:22:43.282: INFO: (14) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 50.491829ms)
Mar 26 17:22:43.283: INFO: (14) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 51.158192ms)
Mar 26 17:22:43.283: INFO: (14) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 50.971806ms)
Mar 26 17:22:43.283: INFO: (14) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 51.084192ms)
Mar 26 17:22:43.283: INFO: (14) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 51.55752ms)
Mar 26 17:22:43.284: INFO: (14) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 53.012526ms)
Mar 26 17:22:43.290: INFO: (15) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 5.618947ms)
Mar 26 17:22:43.298: INFO: (15) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.711917ms)
Mar 26 17:22:43.298: INFO: (15) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 13.107541ms)
Mar 26 17:22:43.299: INFO: (15) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 13.544145ms)
Mar 26 17:22:43.300: INFO: (15) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 14.881918ms)
Mar 26 17:22:43.300: INFO: (15) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 15.209703ms)
Mar 26 17:22:43.301: INFO: (15) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 16.280094ms)
Mar 26 17:22:43.302: INFO: (15) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 16.920323ms)
Mar 26 17:22:43.302: INFO: (15) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 16.971425ms)
Mar 26 17:22:43.302: INFO: (15) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 17.338718ms)
Mar 26 17:22:43.303: INFO: (15) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 17.968042ms)
Mar 26 17:22:43.303: INFO: (15) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 18.014323ms)
Mar 26 17:22:43.304: INFO: (15) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 18.801231ms)
Mar 26 17:22:43.304: INFO: (15) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 19.049808ms)
Mar 26 17:22:43.305: INFO: (15) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 19.417573ms)
Mar 26 17:22:43.305: INFO: (15) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 19.43693ms)
Mar 26 17:22:43.336: INFO: (16) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 30.393515ms)
Mar 26 17:22:43.336: INFO: (16) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 30.806367ms)
Mar 26 17:22:43.337: INFO: (16) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 31.486755ms)
Mar 26 17:22:43.337: INFO: (16) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 31.614302ms)
Mar 26 17:22:43.337: INFO: (16) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 31.368161ms)
Mar 26 17:22:43.337: INFO: (16) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 32.159223ms)
Mar 26 17:22:43.337: INFO: (16) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 32.504176ms)
Mar 26 17:22:43.338: INFO: (16) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 32.607858ms)
Mar 26 17:22:43.338: INFO: (16) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 32.595265ms)
Mar 26 17:22:43.338: INFO: (16) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 32.623657ms)
Mar 26 17:22:43.339: INFO: (16) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 33.299935ms)
Mar 26 17:22:43.340: INFO: (16) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 34.106488ms)
Mar 26 17:22:43.340: INFO: (16) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 34.89083ms)
Mar 26 17:22:43.341: INFO: (16) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 35.754142ms)
Mar 26 17:22:43.342: INFO: (16) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 36.880266ms)
Mar 26 17:22:43.343: INFO: (16) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 37.053711ms)
Mar 26 17:22:43.349: INFO: (17) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 5.949827ms)
Mar 26 17:22:43.352: INFO: (17) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 8.225947ms)
Mar 26 17:22:43.358: INFO: (17) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 14.878564ms)
Mar 26 17:22:43.358: INFO: (17) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 15.650162ms)
Mar 26 17:22:43.358: INFO: (17) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 15.71773ms)
Mar 26 17:22:43.359: INFO: (17) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 15.738551ms)
Mar 26 17:22:43.359: INFO: (17) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 15.347912ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 17.719377ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 17.639291ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 18.237202ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 18.131282ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 18.332594ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 18.106198ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 18.304545ms)
Mar 26 17:22:43.361: INFO: (17) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 18.192489ms)
Mar 26 17:22:43.362: INFO: (17) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 18.204111ms)
Mar 26 17:22:43.370: INFO: (18) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 8.023545ms)
Mar 26 17:22:43.370: INFO: (18) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 8.335497ms)
Mar 26 17:22:43.373: INFO: (18) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 11.426615ms)
Mar 26 17:22:43.373: INFO: (18) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 11.716314ms)
Mar 26 17:22:43.373: INFO: (18) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 11.466666ms)
Mar 26 17:22:43.375: INFO: (18) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 12.780241ms)
Mar 26 17:22:43.376: INFO: (18) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 13.421709ms)
Mar 26 17:22:43.377: INFO: (18) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 14.6723ms)
Mar 26 17:22:43.378: INFO: (18) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 15.155201ms)
Mar 26 17:22:43.381: INFO: (18) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 18.774004ms)
Mar 26 17:22:43.382: INFO: (18) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 20.072025ms)
Mar 26 17:22:43.382: INFO: (18) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 20.489291ms)
Mar 26 17:22:43.383: INFO: (18) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 20.567245ms)
Mar 26 17:22:43.383: INFO: (18) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 20.574748ms)
Mar 26 17:22:43.383: INFO: (18) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 20.712647ms)
Mar 26 17:22:43.384: INFO: (18) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 21.490469ms)
Mar 26 17:22:43.400: INFO: (19) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj/proxy/rewriteme">test</a> (200; 16.044053ms)
Mar 26 17:22:43.403: INFO: (19) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">... (200; 18.843437ms)
Mar 26 17:22:43.405: INFO: (19) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 20.470971ms)
Mar 26 17:22:43.405: INFO: (19) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 20.808412ms)
Mar 26 17:22:43.405: INFO: (19) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:443/proxy/tlsrewritem... (200; 20.925114ms)
Mar 26 17:22:43.405: INFO: (19) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:462/proxy/: tls qux (200; 21.325565ms)
Mar 26 17:22:43.405: INFO: (19) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:162/proxy/: bar (200; 21.449844ms)
Mar 26 17:22:43.406: INFO: (19) /api/v1/namespaces/proxy-3203/pods/https:proxy-service-sdjbx-prtnj:460/proxy/: tls baz (200; 22.341232ms)
Mar 26 17:22:43.407: INFO: (19) /api/v1/namespaces/proxy-3203/pods/http:proxy-service-sdjbx-prtnj:160/proxy/: foo (200; 22.869487ms)
Mar 26 17:22:43.407: INFO: (19) /api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/: <a href="/api/v1/namespaces/proxy-3203/pods/proxy-service-sdjbx-prtnj:1080/proxy/rewriteme">test<... (200; 22.824527ms)
Mar 26 17:22:43.410: INFO: (19) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname1/proxy/: foo (200; 25.450428ms)
Mar 26 17:22:43.410: INFO: (19) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname2/proxy/: tls qux (200; 25.98598ms)
Mar 26 17:22:43.411: INFO: (19) /api/v1/namespaces/proxy-3203/services/proxy-service-sdjbx:portname2/proxy/: bar (200; 27.096277ms)
Mar 26 17:22:43.411: INFO: (19) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname2/proxy/: bar (200; 27.074981ms)
Mar 26 17:22:43.411: INFO: (19) /api/v1/namespaces/proxy-3203/services/https:proxy-service-sdjbx:tlsportname1/proxy/: tls baz (200; 27.398387ms)
Mar 26 17:22:43.412: INFO: (19) /api/v1/namespaces/proxy-3203/services/http:proxy-service-sdjbx:portname1/proxy/: foo (200; 27.703784ms)
STEP: deleting ReplicationController proxy-service-sdjbx in namespace proxy-3203, will wait for the garbage collector to delete the pods
Mar 26 17:22:43.471: INFO: Deleting ReplicationController proxy-service-sdjbx took: 6.914195ms
Mar 26 17:22:43.576: INFO: Terminating ReplicationController proxy-service-sdjbx pods took: 104.56072ms
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:22:48.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3203" for this suite.

â€¢ [SLOW TEST:13.718 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":275,"completed":179,"skipped":2790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:22:48.197: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 17:22:48.356: INFO: Waiting up to 5m0s for pod "pod-85cb261d-489c-4477-8346-79d6649a7aff" in namespace "emptydir-2883" to be "Succeeded or Failed"
Mar 26 17:22:48.361: INFO: Pod "pod-85cb261d-489c-4477-8346-79d6649a7aff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.791186ms
Mar 26 17:22:50.368: INFO: Pod "pod-85cb261d-489c-4477-8346-79d6649a7aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011674552s
Mar 26 17:22:52.371: INFO: Pod "pod-85cb261d-489c-4477-8346-79d6649a7aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014916847s
STEP: Saw pod success
Mar 26 17:22:52.371: INFO: Pod "pod-85cb261d-489c-4477-8346-79d6649a7aff" satisfied condition "Succeeded or Failed"
Mar 26 17:22:52.373: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-85cb261d-489c-4477-8346-79d6649a7aff container test-container: <nil>
STEP: delete the pod
Mar 26 17:22:52.417: INFO: Waiting for pod pod-85cb261d-489c-4477-8346-79d6649a7aff to disappear
Mar 26 17:22:52.423: INFO: Pod pod-85cb261d-489c-4477-8346-79d6649a7aff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:22:52.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2883" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":180,"skipped":2879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:22:52.434: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:22:52.694: INFO: Create a RollingUpdate DaemonSet
Mar 26 17:22:52.701: INFO: Check that daemon pods launch on every node of the cluster
Mar 26 17:22:52.703: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:22:52.709: INFO: Number of nodes with available pods: 0
Mar 26 17:22:52.709: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:22:53.752: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:22:53.760: INFO: Number of nodes with available pods: 0
Mar 26 17:22:53.760: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:22:54.714: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:22:54.717: INFO: Number of nodes with available pods: 1
Mar 26 17:22:54.717: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:22:55.714: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:22:55.718: INFO: Number of nodes with available pods: 2
Mar 26 17:22:55.718: INFO: Number of running nodes: 2, number of available pods: 2
Mar 26 17:22:55.718: INFO: Update the DaemonSet to trigger a rollout
Mar 26 17:22:55.731: INFO: Updating DaemonSet daemon-set
Mar 26 17:23:04.759: INFO: Roll back the DaemonSet before rollout is complete
Mar 26 17:23:04.769: INFO: Updating DaemonSet daemon-set
Mar 26 17:23:04.769: INFO: Make sure DaemonSet rollback is complete
Mar 26 17:23:04.784: INFO: Wrong image for pod: daemon-set-zvssn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 26 17:23:04.784: INFO: Pod daemon-set-zvssn is not available
Mar 26 17:23:04.793: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:23:05.800: INFO: Wrong image for pod: daemon-set-zvssn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 26 17:23:05.801: INFO: Pod daemon-set-zvssn is not available
Mar 26 17:23:05.809: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:23:06.802: INFO: Pod daemon-set-45ffg is not available
Mar 26 17:23:06.809: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4123, will wait for the garbage collector to delete the pods
Mar 26 17:23:06.892: INFO: Deleting DaemonSet.extensions daemon-set took: 17.042612ms
Mar 26 17:23:07.392: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.596345ms
Mar 26 17:23:18.299: INFO: Number of nodes with available pods: 0
Mar 26 17:23:18.299: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 17:23:18.326: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4123/daemonsets","resourceVersion":"20238"},"items":null}

Mar 26 17:23:18.339: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4123/pods","resourceVersion":"20238"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:23:18.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4123" for this suite.

â€¢ [SLOW TEST:25.933 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":275,"completed":181,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:23:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-50cd2aa3-5485-4dd9-912c-b6f9b95b4a07
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:23:18.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2759" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":275,"completed":182,"skipped":2929,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:23:18.572: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5051
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 26 17:23:22.881: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5051 PodName:pod-sharedvolume-80447888-0dc5-4384-9af9-064583d50aac ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:23:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:23:23.034: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:23:23.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5051" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":275,"completed":183,"skipped":2938,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:23:23.046: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 26 17:23:27.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:27.326: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:29.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:29.331: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:31.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:31.331: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:33.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:33.334: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:35.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:35.334: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:37.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:37.332: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:23:39.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:23:39.331: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:23:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4858" for this suite.

â€¢ [SLOW TEST:16.318 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":275,"completed":184,"skipped":2941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:23:39.367: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Mar 26 17:23:40.111: INFO: created pod pod-service-account-defaultsa
Mar 26 17:23:40.112: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 26 17:23:40.128: INFO: created pod pod-service-account-mountsa
Mar 26 17:23:40.129: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 26 17:23:40.150: INFO: created pod pod-service-account-nomountsa
Mar 26 17:23:40.150: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 26 17:23:40.169: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 26 17:23:40.170: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 26 17:23:40.210: INFO: created pod pod-service-account-mountsa-mountspec
Mar 26 17:23:40.210: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 26 17:23:40.222: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 26 17:23:40.222: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 26 17:23:40.247: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 26 17:23:40.248: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 26 17:23:40.285: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 26 17:23:40.285: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 26 17:23:40.368: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 26 17:23:40.368: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:23:40.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7948" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":275,"completed":185,"skipped":2964,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:23:40.501: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:24:41.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3113" for this suite.

â€¢ [SLOW TEST:60.706 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":275,"completed":186,"skipped":2966,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:24:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-cefbd773-4226-4c94-89a4-17581c38727e
STEP: Creating a pod to test consume secrets
Mar 26 17:24:41.364: INFO: Waiting up to 5m0s for pod "pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea" in namespace "secrets-174" to be "Succeeded or Failed"
Mar 26 17:24:41.368: INFO: Pod "pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206301ms
Mar 26 17:24:43.376: INFO: Pod "pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012026797s
Mar 26 17:24:45.382: INFO: Pod "pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017852954s
STEP: Saw pod success
Mar 26 17:24:45.382: INFO: Pod "pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea" satisfied condition "Succeeded or Failed"
Mar 26 17:24:45.385: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:24:45.425: INFO: Waiting for pod pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea to disappear
Mar 26 17:24:45.427: INFO: Pod pod-secrets-0dd6556c-3eb1-4f9f-91ea-1fa41bb274ea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:24:45.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-174" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":187,"skipped":2984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:24:45.435: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1330
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:24:45.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1330" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":275,"completed":188,"skipped":3023,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:24:45.666: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 17:24:45.813: INFO: Waiting up to 5m0s for pod "pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d" in namespace "emptydir-6074" to be "Succeeded or Failed"
Mar 26 17:24:45.816: INFO: Pod "pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923339ms
Mar 26 17:24:47.822: INFO: Pod "pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009750333s
Mar 26 17:24:49.831: INFO: Pod "pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018687344s
STEP: Saw pod success
Mar 26 17:24:49.832: INFO: Pod "pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d" satisfied condition "Succeeded or Failed"
Mar 26 17:24:49.835: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d container test-container: <nil>
STEP: delete the pod
Mar 26 17:24:49.856: INFO: Waiting for pod pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d to disappear
Mar 26 17:24:49.860: INFO: Pod pod-dbdb7104-2f4a-485e-a3b5-d292f5a48e6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:24:49.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6074" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":189,"skipped":3036,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:24:49.868: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 26 17:24:50.038: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20759 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:24:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:24:50.038: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20760 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:24:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:24:50.039: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20761 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:24:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 26 17:25:00.069: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20815 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:25:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:25:00.069: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20816 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:25:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:25:00.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5170 /api/v1/namespaces/watch-5170/configmaps/e2e-watch-test-label-changed b1902199-abd4-4b4b-994d-1ad3b74bbc37 20817 0 2020-03-26 17:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 17:25:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:25:00.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5170" for this suite.

â€¢ [SLOW TEST:10.219 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":275,"completed":190,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:25:00.088: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Mar 26 17:25:00.241: INFO: Waiting up to 5m0s for pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d" in namespace "containers-2763" to be "Succeeded or Failed"
Mar 26 17:25:00.244: INFO: Pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334272ms
Mar 26 17:25:02.251: INFO: Pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009883814s
Mar 26 17:25:04.285: INFO: Pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04446804s
Mar 26 17:25:06.288: INFO: Pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047391569s
STEP: Saw pod success
Mar 26 17:25:06.288: INFO: Pod "client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d" satisfied condition "Succeeded or Failed"
Mar 26 17:25:06.290: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d container test-container: <nil>
STEP: delete the pod
Mar 26 17:25:06.318: INFO: Waiting for pod client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d to disappear
Mar 26 17:25:06.322: INFO: Pod client-containers-ffba49b9-9c76-4be2-a776-7bd6a7715e7d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:25:06.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2763" for this suite.

â€¢ [SLOW TEST:6.240 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":275,"completed":191,"skipped":3072,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:25:06.328: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:25:06.547: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 17:25:06.557: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:06.574: INFO: Number of nodes with available pods: 0
Mar 26 17:25:06.574: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:25:07.583: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:07.594: INFO: Number of nodes with available pods: 0
Mar 26 17:25:07.594: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:25:08.596: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:08.600: INFO: Number of nodes with available pods: 1
Mar 26 17:25:08.600: INFO: Node kubedee-test-worker-vf6bys is running more than one daemon pod
Mar 26 17:25:09.580: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:09.584: INFO: Number of nodes with available pods: 2
Mar 26 17:25:09.584: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 26 17:25:09.630: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:09.630: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:09.645: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:10.657: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:10.657: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:10.663: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:11.649: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:11.649: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:11.652: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:12.655: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:12.655: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:12.655: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:12.662: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:13.656: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:13.656: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:13.656: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:13.674: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:14.652: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:14.652: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:14.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:14.658: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:15.651: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:15.652: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:15.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:15.658: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:16.651: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:16.651: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:16.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:16.656: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:17.652: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:17.653: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:17.653: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:17.658: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:18.652: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:18.652: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:18.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:18.659: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:19.651: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:19.651: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:19.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:19.657: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:20.651: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:20.651: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:20.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:20.657: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:21.648: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:21.648: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:21.648: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:21.653: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:22.649: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:22.649: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:22.649: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:22.659: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:23.657: INFO: Wrong image for pod: daemon-set-7bnmw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:23.659: INFO: Pod daemon-set-7bnmw is not available
Mar 26 17:25:23.659: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:23.664: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:24.649: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:24.649: INFO: Pod daemon-set-qmd7h is not available
Mar 26 17:25:24.652: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:25.649: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:25.653: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:26.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:26.652: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:26.666: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:27.653: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:27.653: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:27.660: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:28.654: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:28.654: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:28.658: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:29.649: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:29.649: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:29.652: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:30.653: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:30.653: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:30.661: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:31.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:31.652: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:31.659: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:32.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:32.652: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:32.658: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:33.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:33.651: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:33.657: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:34.652: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:34.653: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:34.659: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:35.651: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:35.651: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:35.657: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:36.653: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:36.653: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:36.659: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:37.650: INFO: Wrong image for pod: daemon-set-flpfj. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 17:25:37.650: INFO: Pod daemon-set-flpfj is not available
Mar 26 17:25:37.657: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:38.677: INFO: Pod daemon-set-fszbt is not available
Mar 26 17:25:38.698: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 26 17:25:38.706: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:38.713: INFO: Number of nodes with available pods: 1
Mar 26 17:25:38.713: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:25:39.720: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:25:39.725: INFO: Number of nodes with available pods: 2
Mar 26 17:25:39.725: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9440, will wait for the garbage collector to delete the pods
Mar 26 17:25:39.805: INFO: Deleting DaemonSet.extensions daemon-set took: 13.646384ms
Mar 26 17:25:40.205: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.294227ms
Mar 26 17:25:48.214: INFO: Number of nodes with available pods: 0
Mar 26 17:25:48.214: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 17:25:48.216: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9440/daemonsets","resourceVersion":"21054"},"items":null}

Mar 26 17:25:48.218: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9440/pods","resourceVersion":"21054"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:25:48.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9440" for this suite.

â€¢ [SLOW TEST:41.908 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":275,"completed":192,"skipped":3084,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:25:48.237: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1
Mar 26 17:25:48.480: INFO: Pod name my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1: Found 0 pods out of 1
Mar 26 17:25:53.485: INFO: Pod name my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1: Found 1 pods out of 1
Mar 26 17:25:53.486: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1" are running
Mar 26 17:25:53.488: INFO: Pod "my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1-kd6vn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:25:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:25:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:25:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 17:25:48 +0000 UTC Reason: Message:}])
Mar 26 17:25:53.488: INFO: Trying to dial the pod
Mar 26 17:25:58.555: INFO: Controller my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1: Got expected result from replica 1 [my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1-kd6vn]: "my-hostname-basic-dad072d1-9dba-4c0b-8996-10acaa7b95b1-kd6vn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:25:58.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3426" for this suite.

â€¢ [SLOW TEST:10.326 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":193,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:25:58.569: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:25:58.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31" in namespace "downward-api-8087" to be "Succeeded or Failed"
Mar 26 17:25:58.805: INFO: Pod "downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31": Phase="Pending", Reason="", readiness=false. Elapsed: 24.644909ms
Mar 26 17:26:00.822: INFO: Pod "downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041543439s
STEP: Saw pod success
Mar 26 17:26:00.822: INFO: Pod "downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31" satisfied condition "Succeeded or Failed"
Mar 26 17:26:00.830: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31 container client-container: <nil>
STEP: delete the pod
Mar 26 17:26:00.881: INFO: Waiting for pod downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31 to disappear
Mar 26 17:26:00.885: INFO: Pod downwardapi-volume-4cda2034-d825-4c40-afe9-49870e9e4f31 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8087" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":194,"skipped":3113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:00.905: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:26:01.201: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 26 17:26:01.218: INFO: Number of nodes with available pods: 0
Mar 26 17:26:01.218: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 26 17:26:01.254: INFO: Number of nodes with available pods: 0
Mar 26 17:26:01.254: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:02.266: INFO: Number of nodes with available pods: 0
Mar 26 17:26:02.266: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:03.260: INFO: Number of nodes with available pods: 1
Mar 26 17:26:03.260: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 26 17:26:03.298: INFO: Number of nodes with available pods: 1
Mar 26 17:26:03.298: INFO: Number of running nodes: 0, number of available pods: 1
Mar 26 17:26:04.305: INFO: Number of nodes with available pods: 0
Mar 26 17:26:04.305: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 26 17:26:04.358: INFO: Number of nodes with available pods: 0
Mar 26 17:26:04.358: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:05.383: INFO: Number of nodes with available pods: 0
Mar 26 17:26:05.383: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:06.369: INFO: Number of nodes with available pods: 0
Mar 26 17:26:06.369: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:07.361: INFO: Number of nodes with available pods: 0
Mar 26 17:26:07.361: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:08.367: INFO: Number of nodes with available pods: 0
Mar 26 17:26:08.368: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:26:09.363: INFO: Number of nodes with available pods: 1
Mar 26 17:26:09.364: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6243, will wait for the garbage collector to delete the pods
Mar 26 17:26:09.436: INFO: Deleting DaemonSet.extensions daemon-set took: 11.202517ms
Mar 26 17:26:09.836: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.62027ms
Mar 26 17:26:12.740: INFO: Number of nodes with available pods: 0
Mar 26 17:26:12.740: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 17:26:12.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6243/daemonsets","resourceVersion":"21257"},"items":null}

Mar 26 17:26:12.745: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6243/pods","resourceVersion":"21257"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:12.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6243" for this suite.

â€¢ [SLOW TEST:11.880 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":275,"completed":195,"skipped":3163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:12.786: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:26:12.982: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-950eb830-8f37-4e09-a680-f8680541fc93" in namespace "security-context-test-1644" to be "Succeeded or Failed"
Mar 26 17:26:13.008: INFO: Pod "busybox-privileged-false-950eb830-8f37-4e09-a680-f8680541fc93": Phase="Pending", Reason="", readiness=false. Elapsed: 25.786268ms
Mar 26 17:26:15.015: INFO: Pod "busybox-privileged-false-950eb830-8f37-4e09-a680-f8680541fc93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03243568s
Mar 26 17:26:15.015: INFO: Pod "busybox-privileged-false-950eb830-8f37-4e09-a680-f8680541fc93" satisfied condition "Succeeded or Failed"
Mar 26 17:26:15.027: INFO: Got logs for pod "busybox-privileged-false-950eb830-8f37-4e09-a680-f8680541fc93": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:15.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1644" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":196,"skipped":3188,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:26:15.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431" in namespace "projected-618" to be "Succeeded or Failed"
Mar 26 17:26:15.229: INFO: Pod "downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431": Phase="Pending", Reason="", readiness=false. Elapsed: 10.54912ms
Mar 26 17:26:17.235: INFO: Pod "downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016638881s
STEP: Saw pod success
Mar 26 17:26:17.235: INFO: Pod "downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431" satisfied condition "Succeeded or Failed"
Mar 26 17:26:17.240: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431 container client-container: <nil>
STEP: delete the pod
Mar 26 17:26:17.278: INFO: Waiting for pod downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431 to disappear
Mar 26 17:26:17.287: INFO: Pod downwardapi-volume-cafb42a5-2262-44f7-bcef-beb967c2c431 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-618" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":197,"skipped":3200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:17.302: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Mar 26 17:26:17.468: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-194310689 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5697" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":275,"completed":198,"skipped":3238,"failed":0}
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:17.824: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 26 17:26:26.149: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.239: INFO: Exec stderr: ""
Mar 26 17:26:26.239: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.239: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.324: INFO: Exec stderr: ""
Mar 26 17:26:26.324: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.324: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.390: INFO: Exec stderr: ""
Mar 26 17:26:26.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.390: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.469: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 26 17:26:26.469: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.470: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.545: INFO: Exec stderr: ""
Mar 26 17:26:26.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.636: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 26 17:26:26.636: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.636: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.708: INFO: Exec stderr: ""
Mar 26 17:26:26.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.708: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.789: INFO: Exec stderr: ""
Mar 26 17:26:26.790: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.790: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.889: INFO: Exec stderr: ""
Mar 26 17:26:26.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2257 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:26:26.889: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:26:26.951: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:26.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2257" for this suite.

â€¢ [SLOW TEST:9.135 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":199,"skipped":3245,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 26 17:26:29.681: INFO: Successfully updated pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de"
Mar 26 17:26:29.681: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de" in namespace "pods-533" to be "terminated due to deadline exceeded"
Mar 26 17:26:29.689: INFO: Pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de": Phase="Running", Reason="", readiness=true. Elapsed: 7.158505ms
Mar 26 17:26:31.692: INFO: Pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de": Phase="Running", Reason="", readiness=true. Elapsed: 2.01067669s
Mar 26 17:26:33.700: INFO: Pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.018153697s
Mar 26 17:26:33.700: INFO: Pod "pod-update-activedeadlineseconds-af365436-05fb-494f-b727-bb9cd60065de" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:26:33.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-533" for this suite.

â€¢ [SLOW TEST:6.757 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":275,"completed":200,"skipped":3251,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:26:33.717: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 26 17:26:33.879: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21472 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:33 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:26:33.879: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21472 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:33 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 26 17:26:43.894: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21511 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:26:43.895: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21511 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 26 17:26:53.910: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21536 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:26:53.911: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21536 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 26 17:27:03.926: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21563 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:27:03.926: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-a a90397f6-8515-4c2a-9937-93126ced32f5 21563 0 2020-03-26 17:26:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 17:26:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 26 17:27:13.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-b 26effa60-e9ed-43a0-8682-a94b8feb0fd6 21594 0 2020-03-26 17:27:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 17:27:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:27:13.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-b 26effa60-e9ed-43a0-8682-a94b8feb0fd6 21594 0 2020-03-26 17:27:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 17:27:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 26 17:27:23.949: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-b 26effa60-e9ed-43a0-8682-a94b8feb0fd6 21619 0 2020-03-26 17:27:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 17:27:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 17:27:23.949: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1747 /api/v1/namespaces/watch-1747/configmaps/e2e-watch-test-configmap-b 26effa60-e9ed-43a0-8682-a94b8feb0fd6 21619 0 2020-03-26 17:27:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 17:27:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1747" for this suite.

â€¢ [SLOW TEST:60.258 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":275,"completed":201,"skipped":3258,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:33.977: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0326 17:27:44.738931      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 17:27:44.739: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:44.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7387" for this suite.

â€¢ [SLOW TEST:10.774 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":275,"completed":202,"skipped":3269,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:44.752: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-32ba8200-759e-4049-8178-21b9bd4122c7
STEP: Creating a pod to test consume configMaps
Mar 26 17:27:44.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87" in namespace "configmap-9315" to be "Succeeded or Failed"
Mar 26 17:27:44.961: INFO: Pod "pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87": Phase="Pending", Reason="", readiness=false. Elapsed: 17.315108ms
Mar 26 17:27:46.968: INFO: Pod "pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024029555s
Mar 26 17:27:48.974: INFO: Pod "pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030071885s
STEP: Saw pod success
Mar 26 17:27:48.974: INFO: Pod "pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87" satisfied condition "Succeeded or Failed"
Mar 26 17:27:48.978: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:27:49.025: INFO: Waiting for pod pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87 to disappear
Mar 26 17:27:49.027: INFO: Pod pod-configmaps-bde4e848-a65c-4476-9367-501a33dcec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:49.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9315" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":203,"skipped":3274,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-7a3e5b01-ff19-4e8f-8e13-c476b9c78813
STEP: Creating a pod to test consume secrets
Mar 26 17:27:49.213: INFO: Waiting up to 5m0s for pod "pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8" in namespace "secrets-6217" to be "Succeeded or Failed"
Mar 26 17:27:49.218: INFO: Pod "pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799905ms
Mar 26 17:27:51.239: INFO: Pod "pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025504008s
Mar 26 17:27:53.246: INFO: Pod "pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032343671s
STEP: Saw pod success
Mar 26 17:27:53.246: INFO: Pod "pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8" satisfied condition "Succeeded or Failed"
Mar 26 17:27:53.252: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:27:53.291: INFO: Waiting for pod pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8 to disappear
Mar 26 17:27:53.294: INFO: Pod pod-secrets-207ac9f8-ebf8-430e-ab99-624d0bc5aca8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:53.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6217" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":204,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:53.306: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Mar 26 17:27:53.503: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-194310689 proxy --unix-socket=/tmp/kubectl-proxy-unix929376332/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:53.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2243" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":275,"completed":205,"skipped":3336,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:53.643: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:55.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7435" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":206,"skipped":3346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:55.838: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-3f18ceab-931b-4e27-be5b-403c2a971451
STEP: Creating a pod to test consume secrets
Mar 26 17:27:56.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0" in namespace "projected-8180" to be "Succeeded or Failed"
Mar 26 17:27:56.037: INFO: Pod "pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.564318ms
Mar 26 17:27:58.044: INFO: Pod "pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023964018s
STEP: Saw pod success
Mar 26 17:27:58.044: INFO: Pod "pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0" satisfied condition "Succeeded or Failed"
Mar 26 17:27:58.048: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:27:58.079: INFO: Waiting for pod pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0 to disappear
Mar 26 17:27:58.088: INFO: Pod pod-projected-secrets-1fc8e108-372e-4318-b570-c6b8439ce8a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:27:58.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8180" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":207,"skipped":3389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:27:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9219
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2918
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:28:04.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4400" for this suite.
STEP: Destroying namespace "nsdeletetest-9219" for this suite.
Mar 26 17:28:04.784: INFO: Namespace nsdeletetest-9219 was already deleted
STEP: Destroying namespace "nsdeletetest-2918" for this suite.

â€¢ [SLOW TEST:6.689 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":275,"completed":208,"skipped":3427,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:28:04.791: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:28:05.259: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 17:28:07.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840485, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840485, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840485, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840485, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:28:10.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:28:10.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4098" for this suite.
STEP: Destroying namespace "webhook-4098-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.818 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":275,"completed":209,"skipped":3441,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:28:10.610: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-43c1f7a5-779f-4532-b980-3afd7720355a in namespace container-probe-850
Mar 26 17:28:12.819: INFO: Started pod liveness-43c1f7a5-779f-4532-b980-3afd7720355a in namespace container-probe-850
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:28:12.822: INFO: Initial restart count of pod liveness-43c1f7a5-779f-4532-b980-3afd7720355a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:13.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-850" for this suite.

â€¢ [SLOW TEST:243.192 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":275,"completed":210,"skipped":3454,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:13.806: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:32:14.135: INFO: Waiting up to 5m0s for pod "downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986" in namespace "downward-api-1642" to be "Succeeded or Failed"
Mar 26 17:32:14.166: INFO: Pod "downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986": Phase="Pending", Reason="", readiness=false. Elapsed: 31.240048ms
Mar 26 17:32:16.171: INFO: Pod "downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03556111s
STEP: Saw pod success
Mar 26 17:32:16.171: INFO: Pod "downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986" satisfied condition "Succeeded or Failed"
Mar 26 17:32:16.175: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986 container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:32:16.292: INFO: Waiting for pod downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986 to disappear
Mar 26 17:32:16.298: INFO: Pod downward-api-e9472b30-34bd-41e1-a0d7-8bab16471986 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:16.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1642" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":275,"completed":211,"skipped":3457,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:16.325: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Mar 26 17:32:16.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 cluster-info'
Mar 26 17:32:17.301: INFO: stderr: ""
Mar 26 17:32:17.301: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-930" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":275,"completed":212,"skipped":3462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:17.327: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:32:17.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede" in namespace "projected-908" to be "Succeeded or Failed"
Mar 26 17:32:17.549: INFO: Pod "downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede": Phase="Pending", Reason="", readiness=false. Elapsed: 15.491203ms
Mar 26 17:32:19.554: INFO: Pod "downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020253371s
STEP: Saw pod success
Mar 26 17:32:19.554: INFO: Pod "downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede" satisfied condition "Succeeded or Failed"
Mar 26 17:32:19.557: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede container client-container: <nil>
STEP: delete the pod
Mar 26 17:32:19.640: INFO: Waiting for pod downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede to disappear
Mar 26 17:32:19.643: INFO: Pod downwardapi-volume-375abe41-60b9-4426-b90c-944424d84ede no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:19.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-908" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":213,"skipped":3493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:19.670: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:43.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8786" for this suite.

â€¢ [SLOW TEST:23.669 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":275,"completed":214,"skipped":3522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:43.340: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 26 17:32:45.590: INFO: &Pod{ObjectMeta:{send-events-de1c5584-c272-4197-8a83-384a79869c62  events-6037 /api/v1/namespaces/events-6037/pods/send-events-de1c5584-c272-4197-8a83-384a79869c62 d3940583-af4b-440e-ad5f-86b82cd95982 22997 0 2020-03-26 17:32:43 +0000 UTC <nil> <nil> map[name:foo time:562718029] map[] [] []  [{e2e.test Update v1 2020-03-26 17:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:32:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 49 46 50 53 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vnz6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vnz6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vnz6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kubedee-test-worker-dqhapg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.107.34.127,PodIP:10.244.1.250,StartTime:2020-03-26 17:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:32:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:cri-o://ab1682ca7d6ded48329958050c362d9feb5d08dca930ae4eef04ecfeda8acdb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 26 17:32:47.637: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 26 17:32:49.644: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:32:49.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6037" for this suite.

â€¢ [SLOW TEST:6.339 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":275,"completed":215,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:32:49.682: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6507
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:32:49.850: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Creating first CR 
Mar 26 17:32:55.560: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:32:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:32:55Z]] name:name1 resourceVersion:23067 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:9af2fddf-62c0-4ff5-b925-856c95da159f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 26 17:33:05.575: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:33:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:33:05Z]] name:name2 resourceVersion:23093 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:cc8213c5-7f4b-4a42-9eed-e86045415ab7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 26 17:33:15.587: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:32:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:33:15Z]] name:name1 resourceVersion:23118 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:9af2fddf-62c0-4ff5-b925-856c95da159f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 26 17:33:25.603: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:33:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:33:25Z]] name:name2 resourceVersion:23142 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:cc8213c5-7f4b-4a42-9eed-e86045415ab7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 26 17:33:35.624: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:32:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:33:15Z]] name:name1 resourceVersion:23178 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:9af2fddf-62c0-4ff5-b925-856c95da159f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 26 17:33:45.640: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T17:33:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T17:33:25Z]] name:name2 resourceVersion:23211 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:cc8213c5-7f4b-4a42-9eed-e86045415ab7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:33:56.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6507" for this suite.

â€¢ [SLOW TEST:66.487 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":275,"completed":216,"skipped":3569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:33:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7689
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:34:09.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3476" for this suite.
STEP: Destroying namespace "nsdeletetest-671" for this suite.
Mar 26 17:34:09.659: INFO: Namespace nsdeletetest-671 was already deleted
STEP: Destroying namespace "nsdeletetest-7689" for this suite.

â€¢ [SLOW TEST:13.489 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":275,"completed":217,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:34:09.670: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:34:09.915: INFO: (0) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 28.02412ms)
Mar 26 17:34:09.922: INFO: (1) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.378474ms)
Mar 26 17:34:09.927: INFO: (2) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.21815ms)
Mar 26 17:34:09.934: INFO: (3) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.646487ms)
Mar 26 17:34:09.937: INFO: (4) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.348388ms)
Mar 26 17:34:09.941: INFO: (5) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.469731ms)
Mar 26 17:34:09.961: INFO: (6) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 20.575921ms)
Mar 26 17:34:09.965: INFO: (7) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.84807ms)
Mar 26 17:34:09.969: INFO: (8) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.316228ms)
Mar 26 17:34:09.972: INFO: (9) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.339952ms)
Mar 26 17:34:09.975: INFO: (10) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.625449ms)
Mar 26 17:34:09.977: INFO: (11) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.658347ms)
Mar 26 17:34:09.980: INFO: (12) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.810275ms)
Mar 26 17:34:09.983: INFO: (13) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.62294ms)
Mar 26 17:34:09.985: INFO: (14) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.232542ms)
Mar 26 17:34:09.987: INFO: (15) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.022377ms)
Mar 26 17:34:09.989: INFO: (16) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.146917ms)
Mar 26 17:34:09.992: INFO: (17) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.047798ms)
Mar 26 17:34:09.994: INFO: (18) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.156251ms)
Mar 26 17:34:09.996: INFO: (19) /api/v1/nodes/kubedee-test-worker-vf6bys:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 2.07046ms)
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:34:09.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7255" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":275,"completed":218,"skipped":3697,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:34:10.003: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-1471
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 17:34:10.168: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 17:34:10.242: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:34:12.251: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:14.249: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:16.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:18.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:20.251: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:22.259: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:24.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:34:26.246: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 17:34:26.256: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 26 17:34:30.391: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.252:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1471 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:34:30.391: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:34:30.566: INFO: Found all expected endpoints: [netserver-0]
Mar 26 17:34:30.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.95:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1471 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:34:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:34:30.678: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:34:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1471" for this suite.

â€¢ [SLOW TEST:20.684 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":219,"skipped":3714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:34:30.688: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2403
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-2403
Mar 26 17:34:30.903: INFO: Found 0 stateful pods, waiting for 1
Mar 26 17:34:40.914: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:34:40.964: INFO: Deleting all statefulset in ns statefulset-2403
Mar 26 17:34:40.983: INFO: Scaling statefulset ss to 0
Mar 26 17:35:01.033: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:35:01.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:35:01.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2403" for this suite.

â€¢ [SLOW TEST:30.394 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":275,"completed":220,"skipped":3738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:35:01.086: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 17:35:01.240: INFO: PodSpec: initContainers in spec.initContainers
Mar 26 17:35:46.367: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7e6fa8d8-290d-47aa-bcf5-dfddab6c8f65", GenerateName:"", Namespace:"init-container-4373", SelfLink:"/api/v1/namespaces/init-container-4373/pods/pod-init-7e6fa8d8-290d-47aa-bcf5-dfddab6c8f65", UID:"3fb666fb-2326-46fb-89fe-55c0a48aeeb3", ResourceVersion:"23770", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63720840901, loc:(*time.Location)(0x7b4e1c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"240632907"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006c06560), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006c06580)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006c065a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006c065c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wpk5z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00371be80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wpk5z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wpk5z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wpk5z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00392e9f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kubedee-test-worker-dqhapg", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004ca4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00392ea70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00392ea90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00392ea98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00392ea9c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840901, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840901, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840901, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720840901, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.107.34.127", PodIP:"10.244.1.254", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.254"}}, StartTime:(*v1.Time)(0xc006c065e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004ca620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004ca700)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://b0165f262b60a415f4f761c6633934d0da2040df7948cf26662262cc2d0503f1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006c06620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006c06600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00392eb1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:35:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4373" for this suite.

â€¢ [SLOW TEST:45.299 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":275,"completed":221,"skipped":3781,"failed":0}
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:35:46.386: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:35:46.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 version'
Mar 26 17:35:46.871: INFO: stderr: ""
Mar 26 17:35:46.871: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:50:46Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:35:46.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3636" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":275,"completed":222,"skipped":3781,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:35:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 17:35:47.092: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:47.103: INFO: Number of nodes with available pods: 0
Mar 26 17:35:47.103: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:48.108: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:48.114: INFO: Number of nodes with available pods: 0
Mar 26 17:35:48.114: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:49.119: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:49.124: INFO: Number of nodes with available pods: 1
Mar 26 17:35:49.124: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:50.113: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:50.118: INFO: Number of nodes with available pods: 2
Mar 26 17:35:50.118: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 26 17:35:50.135: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:50.137: INFO: Number of nodes with available pods: 1
Mar 26 17:35:50.137: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:51.147: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:51.152: INFO: Number of nodes with available pods: 1
Mar 26 17:35:51.152: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:52.142: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:52.145: INFO: Number of nodes with available pods: 1
Mar 26 17:35:52.145: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:53.147: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:53.153: INFO: Number of nodes with available pods: 1
Mar 26 17:35:53.153: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:54.145: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:54.168: INFO: Number of nodes with available pods: 1
Mar 26 17:35:54.168: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:55.145: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:55.151: INFO: Number of nodes with available pods: 1
Mar 26 17:35:55.151: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:56.143: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:56.148: INFO: Number of nodes with available pods: 1
Mar 26 17:35:56.148: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:57.143: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:57.147: INFO: Number of nodes with available pods: 1
Mar 26 17:35:57.147: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:58.143: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:58.147: INFO: Number of nodes with available pods: 1
Mar 26 17:35:58.147: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:35:59.141: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:35:59.144: INFO: Number of nodes with available pods: 1
Mar 26 17:35:59.144: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:36:00.143: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:36:00.146: INFO: Number of nodes with available pods: 2
Mar 26 17:36:00.146: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-767, will wait for the garbage collector to delete the pods
Mar 26 17:36:00.217: INFO: Deleting DaemonSet.extensions daemon-set took: 11.49385ms
Mar 26 17:36:00.617: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.518068ms
Mar 26 17:36:13.926: INFO: Number of nodes with available pods: 0
Mar 26 17:36:13.926: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 17:36:13.929: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-767/daemonsets","resourceVersion":"23920"},"items":null}

Mar 26 17:36:13.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-767/pods","resourceVersion":"23920"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:36:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-767" for this suite.

â€¢ [SLOW TEST:27.068 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":275,"completed":223,"skipped":3790,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:36:13.949: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-f666
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:36:14.110: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f666" in namespace "subpath-3063" to be "Succeeded or Failed"
Mar 26 17:36:14.112: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586563ms
Mar 26 17:36:16.118: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008515572s
Mar 26 17:36:18.123: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 4.013667423s
Mar 26 17:36:20.127: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 6.017620969s
Mar 26 17:36:22.133: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 8.022839505s
Mar 26 17:36:24.138: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 10.028380623s
Mar 26 17:36:26.144: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 12.03384254s
Mar 26 17:36:28.149: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 14.039744772s
Mar 26 17:36:30.163: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 16.053299786s
Mar 26 17:36:32.168: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 18.058689651s
Mar 26 17:36:34.175: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Running", Reason="", readiness=true. Elapsed: 20.064813079s
Mar 26 17:36:36.180: INFO: Pod "pod-subpath-test-downwardapi-f666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06991127s
STEP: Saw pod success
Mar 26 17:36:36.180: INFO: Pod "pod-subpath-test-downwardapi-f666" satisfied condition "Succeeded or Failed"
Mar 26 17:36:36.184: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-subpath-test-downwardapi-f666 container test-container-subpath-downwardapi-f666: <nil>
STEP: delete the pod
Mar 26 17:36:36.307: INFO: Waiting for pod pod-subpath-test-downwardapi-f666 to disappear
Mar 26 17:36:36.315: INFO: Pod pod-subpath-test-downwardapi-f666 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-f666
Mar 26 17:36:36.315: INFO: Deleting pod "pod-subpath-test-downwardapi-f666" in namespace "subpath-3063"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:36:36.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3063" for this suite.

â€¢ [SLOW TEST:22.400 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":275,"completed":224,"skipped":3801,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:36:36.350: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 17:36:40.539: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:36:40.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1368" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":225,"skipped":3808,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:36:40.574: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 in namespace container-probe-4089
Mar 26 17:36:42.732: INFO: Started pod liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 in namespace container-probe-4089
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:36:42.736: INFO: Initial restart count of pod liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is 0
Mar 26 17:37:00.799: INFO: Restart count of pod container-probe-4089/liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is now 1 (18.062552111s elapsed)
Mar 26 17:37:20.853: INFO: Restart count of pod container-probe-4089/liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is now 2 (38.116378977s elapsed)
Mar 26 17:37:40.906: INFO: Restart count of pod container-probe-4089/liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is now 3 (58.169645397s elapsed)
Mar 26 17:38:01.004: INFO: Restart count of pod container-probe-4089/liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is now 4 (1m18.267568119s elapsed)
Mar 26 17:39:05.253: INFO: Restart count of pod container-probe-4089/liveness-3b3576f2-7fb4-4b19-b8f8-aab86a918f34 is now 5 (2m22.516665787s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:05.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4089" for this suite.

â€¢ [SLOW TEST:144.729 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":275,"completed":226,"skipped":3812,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:39:05.637: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:07.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3028" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":275,"completed":227,"skipped":3827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:07.832: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5663
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:39:08.065: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 17:39:15.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-5663 create -f -'
Mar 26 17:39:16.999: INFO: stderr: ""
Mar 26 17:39:16.999: INFO: stdout: "e2e-test-crd-publish-openapi-402-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 26 17:39:17.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-5663 delete e2e-test-crd-publish-openapi-402-crds test-cr'
Mar 26 17:39:17.115: INFO: stderr: ""
Mar 26 17:39:17.115: INFO: stdout: "e2e-test-crd-publish-openapi-402-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 26 17:39:17.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-5663 apply -f -'
Mar 26 17:39:18.692: INFO: stderr: ""
Mar 26 17:39:18.692: INFO: stdout: "e2e-test-crd-publish-openapi-402-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 26 17:39:18.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-5663 delete e2e-test-crd-publish-openapi-402-crds test-cr'
Mar 26 17:39:18.792: INFO: stderr: ""
Mar 26 17:39:18.792: INFO: stdout: "e2e-test-crd-publish-openapi-402-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 26 17:39:18.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-402-crds'
Mar 26 17:39:20.304: INFO: stderr: ""
Mar 26 17:39:20.304: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-402-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:23.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5663" for this suite.

â€¢ [SLOW TEST:16.027 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":275,"completed":228,"skipped":3855,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 26 17:39:24.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9792'
Mar 26 17:39:24.234: INFO: stderr: ""
Mar 26 17:39:24.234: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 26 17:39:29.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 get pod e2e-test-httpd-pod --namespace=kubectl-9792 -o json'
Mar 26 17:39:29.426: INFO: stderr: ""
Mar 26 17:39:29.426: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-26T17:39:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-03-26T17:39:24Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.1.9\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-03-26T17:39:25Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9792\",\n        \"resourceVersion\": \"24574\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9792/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1a0f73ed-3bca-49e8-b4ec-9f4e933a7858\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zhzxr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kubedee-test-worker-dqhapg\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zhzxr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zhzxr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T17:39:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T17:39:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T17:39:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T17:39:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://d60064226f13033d208e457f55ef0af42911a255d8598ff7cca093950a953f15\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-26T17:39:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.107.34.127\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.9\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.9\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-26T17:39:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 26 17:39:29.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 replace -f - --namespace=kubectl-9792'
Mar 26 17:39:30.907: INFO: stderr: ""
Mar 26 17:39:30.907: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar 26 17:39:30.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 delete pods e2e-test-httpd-pod --namespace=kubectl-9792'
Mar 26 17:39:38.213: INFO: stderr: ""
Mar 26 17:39:38.213: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:38.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9792" for this suite.

â€¢ [SLOW TEST:14.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":275,"completed":229,"skipped":3858,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-9350/secret-test-854fc1aa-0a46-43d6-9f4b-82af13bf0c03
STEP: Creating a pod to test consume secrets
Mar 26 17:39:38.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56" in namespace "secrets-9350" to be "Succeeded or Failed"
Mar 26 17:39:38.529: INFO: Pod "pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56": Phase="Pending", Reason="", readiness=false. Elapsed: 23.446645ms
Mar 26 17:39:40.600: INFO: Pod "pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.094793508s
STEP: Saw pod success
Mar 26 17:39:40.601: INFO: Pod "pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56" satisfied condition "Succeeded or Failed"
Mar 26 17:39:40.613: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56 container env-test: <nil>
STEP: delete the pod
Mar 26 17:39:40.719: INFO: Waiting for pod pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56 to disappear
Mar 26 17:39:40.722: INFO: Pod pod-configmaps-04e16eea-4e01-4181-adf4-d34d4e262b56 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:40.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9350" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":230,"skipped":3861,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:40.736: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:43.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8877" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":275,"completed":231,"skipped":3863,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:43.192: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:39:43.465: INFO: Waiting up to 5m0s for pod "downward-api-d1882310-358c-43a1-b21e-feb2383b01ad" in namespace "downward-api-1344" to be "Succeeded or Failed"
Mar 26 17:39:43.488: INFO: Pod "downward-api-d1882310-358c-43a1-b21e-feb2383b01ad": Phase="Pending", Reason="", readiness=false. Elapsed: 20.687285ms
Mar 26 17:39:45.507: INFO: Pod "downward-api-d1882310-358c-43a1-b21e-feb2383b01ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038913364s
STEP: Saw pod success
Mar 26 17:39:45.507: INFO: Pod "downward-api-d1882310-358c-43a1-b21e-feb2383b01ad" satisfied condition "Succeeded or Failed"
Mar 26 17:39:45.509: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downward-api-d1882310-358c-43a1-b21e-feb2383b01ad container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:39:45.600: INFO: Waiting for pod downward-api-d1882310-358c-43a1-b21e-feb2383b01ad to disappear
Mar 26 17:39:45.639: INFO: Pod downward-api-d1882310-358c-43a1-b21e-feb2383b01ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:45.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1344" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":275,"completed":232,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:45.695: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:39:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2637" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":275,"completed":233,"skipped":3913,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:39:45.941: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9536
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-65098c6d-4905-48fc-a327-976fc63927bf in namespace container-probe-9536
Mar 26 17:39:48.105: INFO: Started pod liveness-65098c6d-4905-48fc-a327-976fc63927bf in namespace container-probe-9536
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:39:48.110: INFO: Initial restart count of pod liveness-65098c6d-4905-48fc-a327-976fc63927bf is 0
Mar 26 17:40:14.206: INFO: Restart count of pod container-probe-9536/liveness-65098c6d-4905-48fc-a327-976fc63927bf is now 1 (26.09504494s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:14.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9536" for this suite.

â€¢ [SLOW TEST:28.296 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":234,"skipped":3933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:14.240: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4115
STEP: Creating secret with name secret-test-3d777c20-20f2-49e0-a3c3-e5babcbcfdf8
STEP: Creating a pod to test consume secrets
Mar 26 17:40:14.587: INFO: Waiting up to 5m0s for pod "pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2" in namespace "secrets-7560" to be "Succeeded or Failed"
Mar 26 17:40:14.590: INFO: Pod "pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469249ms
Mar 26 17:40:16.596: INFO: Pod "pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00846639s
STEP: Saw pod success
Mar 26 17:40:16.596: INFO: Pod "pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2" satisfied condition "Succeeded or Failed"
Mar 26 17:40:16.601: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:40:16.626: INFO: Waiting for pod pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2 to disappear
Mar 26 17:40:16.628: INFO: Pod pod-secrets-f7d9fc8f-0e41-4178-818f-529753e5aee2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:16.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7560" for this suite.
STEP: Destroying namespace "secret-namespace-4115" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":275,"completed":235,"skipped":4007,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:16.644: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 26 17:40:16.787: INFO: Waiting up to 5m0s for pod "pod-609f528b-5ac0-44b1-8e6f-078ea467db5c" in namespace "emptydir-6507" to be "Succeeded or Failed"
Mar 26 17:40:16.790: INFO: Pod "pod-609f528b-5ac0-44b1-8e6f-078ea467db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785327ms
Mar 26 17:40:18.794: INFO: Pod "pod-609f528b-5ac0-44b1-8e6f-078ea467db5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006515998s
STEP: Saw pod success
Mar 26 17:40:18.794: INFO: Pod "pod-609f528b-5ac0-44b1-8e6f-078ea467db5c" satisfied condition "Succeeded or Failed"
Mar 26 17:40:18.797: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-609f528b-5ac0-44b1-8e6f-078ea467db5c container test-container: <nil>
STEP: delete the pod
Mar 26 17:40:18.818: INFO: Waiting for pod pod-609f528b-5ac0-44b1-8e6f-078ea467db5c to disappear
Mar 26 17:40:18.820: INFO: Pod pod-609f528b-5ac0-44b1-8e6f-078ea467db5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:18.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6507" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":236,"skipped":4018,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:18.826: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-71
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 17:40:18.961: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 17:40:18.978: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 17:40:18.982: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-dqhapg before test
Mar 26 17:40:19.011: INFO: kube-flannel-ds-amd64-vblvg from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:40:19.011: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:40:19.011: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-slkc5 from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:40:19.011: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 26 17:40:19.011: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:40:19.011: INFO: sonobuoy from sonobuoy started at 2020-03-26 16:32:42 +0000 UTC (1 container statuses recorded)
Mar 26 17:40:19.011: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 26 17:40:19.011: INFO: 
Logging pods the kubelet thinks is on node kubedee-test-worker-vf6bys before test
Mar 26 17:40:19.037: INFO: sonobuoy-systemd-logs-daemon-set-e2640173c95746cf-fjlmt from sonobuoy started at 2020-03-26 16:32:52 +0000 UTC (2 container statuses recorded)
Mar 26 17:40:19.037: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 26 17:40:19.037: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 26 17:40:19.037: INFO: coredns-6ccf845bfb-c9f7x from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:40:19.037: INFO: 	Container coredns ready: true, restart count 0
Mar 26 17:40:19.037: INFO: kube-flannel-ds-amd64-4db44 from kube-system started at 2020-03-26 16:31:12 +0000 UTC (1 container statuses recorded)
Mar 26 17:40:19.037: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 26 17:40:19.037: INFO: coredns-6ccf845bfb-jklrr from kube-system started at 2020-03-26 16:31:36 +0000 UTC (1 container statuses recorded)
Mar 26 17:40:19.037: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c7db9e6b-cafa-4268-ad00-fb2d495a8e07 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c7db9e6b-cafa-4268-ad00-fb2d495a8e07 off the node kubedee-test-worker-dqhapg
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c7db9e6b-cafa-4268-ad00-fb2d495a8e07
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:41.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-71" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:22.441 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":275,"completed":237,"skipped":4032,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 26 17:40:45.574: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:45.585: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 17:40:47.585: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:47.589: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 17:40:49.585: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:49.593: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 17:40:51.585: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:51.592: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 17:40:53.585: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:53.595: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 17:40:55.585: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 17:40:55.593: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:55.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3089" for this suite.

â€¢ [SLOW TEST:14.343 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":275,"completed":238,"skipped":4040,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:55.615: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 26 17:41:02.039: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 17:41:02.051: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 17:41:04.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 17:41:04.077: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 17:41:06.051: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 17:41:06.056: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 17:41:08.051: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 17:41:08.062: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 17:41:10.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 17:41:10.057: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:10.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-782" for this suite.

â€¢ [SLOW TEST:14.499 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":275,"completed":239,"skipped":4040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:10.119: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-37
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:41:10.839: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 26 17:41:12.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841270, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841270, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841270, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841270, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:41:15.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:41:15.934: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5491-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-37" for this suite.
STEP: Destroying namespace "webhook-37-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:12.234 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":275,"completed":240,"skipped":4073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:22.357: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-855
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-632a184f-9aee-4e6c-bcb1-b492e9a1e252
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-632a184f-9aee-4e6c-bcb1-b492e9a1e252
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:26.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-855" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":241,"skipped":4158,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:26.694: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-31206177-403d-4ac9-9e2f-30619292f712
STEP: Creating a pod to test consume secrets
Mar 26 17:41:26.866: INFO: Waiting up to 5m0s for pod "pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e" in namespace "secrets-1420" to be "Succeeded or Failed"
Mar 26 17:41:26.886: INFO: Pod "pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.67575ms
Mar 26 17:41:28.891: INFO: Pod "pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024488958s
Mar 26 17:41:30.895: INFO: Pod "pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028301733s
STEP: Saw pod success
Mar 26 17:41:30.895: INFO: Pod "pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e" satisfied condition "Succeeded or Failed"
Mar 26 17:41:30.897: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:41:30.927: INFO: Waiting for pod pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e to disappear
Mar 26 17:41:30.944: INFO: Pod pod-secrets-5d7a235e-6cf9-436c-a87b-a0fa556fba7e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:30.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1420" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":242,"skipped":4160,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:30.951: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-ed31dec3-c911-4bdf-922c-ab8388cbce77
STEP: Creating a pod to test consume secrets
Mar 26 17:41:31.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354" in namespace "projected-8878" to be "Succeeded or Failed"
Mar 26 17:41:31.157: INFO: Pod "pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354": Phase="Pending", Reason="", readiness=false. Elapsed: 9.469392ms
Mar 26 17:41:33.162: INFO: Pod "pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014598195s
Mar 26 17:41:35.169: INFO: Pod "pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021349359s
STEP: Saw pod success
Mar 26 17:41:35.169: INFO: Pod "pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354" satisfied condition "Succeeded or Failed"
Mar 26 17:41:35.173: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:41:35.208: INFO: Waiting for pod pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354 to disappear
Mar 26 17:41:35.212: INFO: Pod pod-projected-secrets-2f7112a5-945e-4507-83ba-8a3a93768354 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:35.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8878" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":243,"skipped":4172,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-221.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-221.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-221.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-221.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-221.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-221.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:41:39.505: INFO: DNS probes using dns-221/dns-test-faccf40f-e5ef-42c8-b682-f4e629d72def succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-221" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":275,"completed":244,"skipped":4173,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:39.670: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:41:40.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52" in namespace "projected-505" to be "Succeeded or Failed"
Mar 26 17:41:40.089: INFO: Pod "downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.178455ms
Mar 26 17:41:42.096: INFO: Pod "downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020974097s
Mar 26 17:41:44.105: INFO: Pod "downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029895702s
STEP: Saw pod success
Mar 26 17:41:44.105: INFO: Pod "downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52" satisfied condition "Succeeded or Failed"
Mar 26 17:41:44.128: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52 container client-container: <nil>
STEP: delete the pod
Mar 26 17:41:44.172: INFO: Waiting for pod downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52 to disappear
Mar 26 17:41:44.219: INFO: Pod downwardapi-volume-f0114d2e-08af-4143-b342-a2f26f1a8d52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:44.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-505" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":245,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-8448
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8448 to expose endpoints map[]
Mar 26 17:41:44.598: INFO: successfully validated that service multi-endpoint-test in namespace services-8448 exposes endpoints map[] (32.177703ms elapsed)
STEP: Creating pod pod1 in namespace services-8448
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8448 to expose endpoints map[pod1:[100]]
Mar 26 17:41:47.673: INFO: successfully validated that service multi-endpoint-test in namespace services-8448 exposes endpoints map[pod1:[100]] (3.054232284s elapsed)
STEP: Creating pod pod2 in namespace services-8448
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8448 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 26 17:41:49.752: INFO: successfully validated that service multi-endpoint-test in namespace services-8448 exposes endpoints map[pod1:[100] pod2:[101]] (2.066049759s elapsed)
STEP: Deleting pod pod1 in namespace services-8448
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8448 to expose endpoints map[pod2:[101]]
Mar 26 17:41:50.813: INFO: successfully validated that service multi-endpoint-test in namespace services-8448 exposes endpoints map[pod2:[101]] (1.049812608s elapsed)
STEP: Deleting pod pod2 in namespace services-8448
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8448 to expose endpoints map[]
Mar 26 17:41:51.833: INFO: successfully validated that service multi-endpoint-test in namespace services-8448 exposes endpoints map[] (1.007868803s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8448" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:7.707 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":275,"completed":246,"skipped":4215,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:51.949: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:41:52.091: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 26 17:41:54.130: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:55.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6392" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":275,"completed":247,"skipped":4225,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:55.159: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:41:55.299: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:57.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9272" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":275,"completed":248,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:57.381: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 26 17:42:00.056: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-5f882adb-ae04-4fba-9d64-199ce39d7541 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 26 17:42:00.823: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-5f882adb-ae04-4fba-9d64-199ce39d7541 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 26 17:42:01.108: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-5f882adb-ae04-4fba-9d64-199ce39d7541 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:01.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1172" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":275,"completed":249,"skipped":4268,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:01.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2596" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":275,"completed":250,"skipped":4273,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:01.450: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:42:01.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab" in namespace "projected-3526" to be "Succeeded or Failed"
Mar 26 17:42:01.625: INFO: Pod "downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270575ms
Mar 26 17:42:03.630: INFO: Pod "downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00999789s
Mar 26 17:42:05.637: INFO: Pod "downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016737762s
STEP: Saw pod success
Mar 26 17:42:05.637: INFO: Pod "downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab" satisfied condition "Succeeded or Failed"
Mar 26 17:42:05.642: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab container client-container: <nil>
STEP: delete the pod
Mar 26 17:42:05.671: INFO: Waiting for pod downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab to disappear
Mar 26 17:42:05.678: INFO: Pod downwardapi-volume-d843e80e-b79f-4e65-9557-fa3b8cabc8ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:05.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3526" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":251,"skipped":4285,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:05.687: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:42:06.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 26 17:42:08.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841326, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841326, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841326, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841326, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:42:11.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:11.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1094" for this suite.
STEP: Destroying namespace "webhook-1094-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.083 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":275,"completed":252,"skipped":4286,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:28.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6974" for this suite.

â€¢ [SLOW TEST:16.373 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":275,"completed":253,"skipped":4288,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:28.154: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-820
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:42:28.357: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 17:42:36.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-820 create -f -'
Mar 26 17:42:39.154: INFO: stderr: ""
Mar 26 17:42:39.154: INFO: stdout: "e2e-test-crd-publish-openapi-6657-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 26 17:42:39.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-820 delete e2e-test-crd-publish-openapi-6657-crds test-cr'
Mar 26 17:42:39.264: INFO: stderr: ""
Mar 26 17:42:39.264: INFO: stdout: "e2e-test-crd-publish-openapi-6657-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 26 17:42:39.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-820 apply -f -'
Mar 26 17:42:40.395: INFO: stderr: ""
Mar 26 17:42:40.395: INFO: stdout: "e2e-test-crd-publish-openapi-6657-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 26 17:42:40.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 --namespace=crd-publish-openapi-820 delete e2e-test-crd-publish-openapi-6657-crds test-cr'
Mar 26 17:42:40.504: INFO: stderr: ""
Mar 26 17:42:40.504: INFO: stdout: "e2e-test-crd-publish-openapi-6657-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 26 17:42:40.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 explain e2e-test-crd-publish-openapi-6657-crds'
Mar 26 17:42:41.257: INFO: stderr: ""
Mar 26 17:42:41.257: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6657-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:44.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-820" for this suite.

â€¢ [SLOW TEST:16.127 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":275,"completed":254,"skipped":4298,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:44.281: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:42:44.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780" in namespace "downward-api-7854" to be "Succeeded or Failed"
Mar 26 17:42:44.478: INFO: Pod "downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780": Phase="Pending", Reason="", readiness=false. Elapsed: 12.676181ms
Mar 26 17:42:46.486: INFO: Pod "downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020697654s
STEP: Saw pod success
Mar 26 17:42:46.486: INFO: Pod "downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780" satisfied condition "Succeeded or Failed"
Mar 26 17:42:46.489: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780 container client-container: <nil>
STEP: delete the pod
Mar 26 17:42:46.538: INFO: Waiting for pod downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780 to disappear
Mar 26 17:42:46.541: INFO: Pod downwardapi-volume-4717d060-e6a2-4810-ae42-f09a3a56f780 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:46.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7854" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":255,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:46.551: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 26 17:42:51.283: INFO: Successfully updated pod "pod-update-fa5f1b64-824e-46fd-aebf-3207c53a6785"
STEP: verifying the updated pod is in kubernetes
Mar 26 17:42:51.291: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:51.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-562" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":275,"completed":256,"skipped":4337,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9656
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 26 17:42:51.482: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
Mar 26 17:43:00.068: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:17.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9656" for this suite.

â€¢ [SLOW TEST:26.020 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":275,"completed":257,"skipped":4345,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:17.322: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:43:17.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6" in namespace "projected-7874" to be "Succeeded or Failed"
Mar 26 17:43:17.477: INFO: Pod "downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476396ms
Mar 26 17:43:19.482: INFO: Pod "downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009609085s
Mar 26 17:43:21.486: INFO: Pod "downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013931736s
STEP: Saw pod success
Mar 26 17:43:21.486: INFO: Pod "downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6" satisfied condition "Succeeded or Failed"
Mar 26 17:43:21.489: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6 container client-container: <nil>
STEP: delete the pod
Mar 26 17:43:21.506: INFO: Waiting for pod downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6 to disappear
Mar 26 17:43:21.510: INFO: Pod downwardapi-volume-c70cea66-9f2a-459f-aac0-84b5bdac92c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7874" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":258,"skipped":4353,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:21.518: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:43:21.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38" in namespace "projected-2299" to be "Succeeded or Failed"
Mar 26 17:43:21.666: INFO: Pod "downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181904ms
Mar 26 17:43:23.672: INFO: Pod "downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012325757s
Mar 26 17:43:25.678: INFO: Pod "downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017764386s
STEP: Saw pod success
Mar 26 17:43:25.678: INFO: Pod "downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38" satisfied condition "Succeeded or Failed"
Mar 26 17:43:25.682: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38 container client-container: <nil>
STEP: delete the pod
Mar 26 17:43:25.704: INFO: Waiting for pod downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38 to disappear
Mar 26 17:43:25.708: INFO: Pod downwardapi-volume-c41b622a-58ce-4e36-bcc7-c51bf9141e38 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2299" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":259,"skipped":4363,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:25.715: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:43:25.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f" in namespace "downward-api-1909" to be "Succeeded or Failed"
Mar 26 17:43:25.859: INFO: Pod "downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005961ms
Mar 26 17:43:27.865: INFO: Pod "downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009518865s
STEP: Saw pod success
Mar 26 17:43:27.865: INFO: Pod "downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f" satisfied condition "Succeeded or Failed"
Mar 26 17:43:27.870: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f container client-container: <nil>
STEP: delete the pod
Mar 26 17:43:27.900: INFO: Waiting for pod downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f to disappear
Mar 26 17:43:27.910: INFO: Pod downwardapi-volume-26f11223-263a-4dec-8954-8d4ee089967f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:27.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1909" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":260,"skipped":4381,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:27.920: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 17:43:32.602: INFO: Successfully updated pod "annotationupdate56953927-4862-40bc-b2d7-5384311c31cc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:34.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1612" for this suite.

â€¢ [SLOW TEST:6.716 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":261,"skipped":4388,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:34.638: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-jsmp
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:43:34.811: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jsmp" in namespace "subpath-1869" to be "Succeeded or Failed"
Mar 26 17:43:34.835: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Pending", Reason="", readiness=false. Elapsed: 24.245431ms
Mar 26 17:43:36.841: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.029928679s
Mar 26 17:43:38.847: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.036352089s
Mar 26 17:43:40.853: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 6.042215603s
Mar 26 17:43:42.860: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.04872219s
Mar 26 17:43:44.867: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 10.055645759s
Mar 26 17:43:46.873: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 12.061905006s
Mar 26 17:43:48.878: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 14.067216211s
Mar 26 17:43:50.885: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 16.073585351s
Mar 26 17:43:52.891: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 18.079983833s
Mar 26 17:43:54.898: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Running", Reason="", readiness=true. Elapsed: 20.086950051s
Mar 26 17:43:56.904: INFO: Pod "pod-subpath-test-secret-jsmp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.093395884s
STEP: Saw pod success
Mar 26 17:43:56.905: INFO: Pod "pod-subpath-test-secret-jsmp" satisfied condition "Succeeded or Failed"
Mar 26 17:43:56.909: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-subpath-test-secret-jsmp container test-container-subpath-secret-jsmp: <nil>
STEP: delete the pod
Mar 26 17:43:56.941: INFO: Waiting for pod pod-subpath-test-secret-jsmp to disappear
Mar 26 17:43:56.946: INFO: Pod pod-subpath-test-secret-jsmp no longer exists
STEP: Deleting pod pod-subpath-test-secret-jsmp
Mar 26 17:43:56.946: INFO: Deleting pod "pod-subpath-test-secret-jsmp" in namespace "subpath-1869"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1869" for this suite.

â€¢ [SLOW TEST:22.331 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":275,"completed":262,"skipped":4399,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:56.970: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar 26 17:43:58.258: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:43:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0326 17:43:58.247597      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1950" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":275,"completed":263,"skipped":4400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:43:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:44:02.598: INFO: DNS probes using dns-test-b4be8a5e-eddf-4416-9009-4f29ba660456 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:44:06.765: INFO: DNS probes using dns-test-d24483f3-108c-4873-858c-f24d5bb7bc63 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6782.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6782.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:44:10.987: INFO: DNS probes using dns-test-f08e46a0-e208-4d8e-9566-722fcb868296 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:44:11.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6782" for this suite.

â€¢ [SLOW TEST:12.818 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":275,"completed":264,"skipped":4463,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:44:11.129: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-f7c5590a-ccd5-4fac-97c4-3c0597fda290
STEP: Creating a pod to test consume configMaps
Mar 26 17:44:11.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1" in namespace "configmap-6068" to be "Succeeded or Failed"
Mar 26 17:44:11.472: INFO: Pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882473ms
Mar 26 17:44:13.495: INFO: Pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02553804s
Mar 26 17:44:15.511: INFO: Pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041735564s
Mar 26 17:44:17.519: INFO: Pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049403873s
STEP: Saw pod success
Mar 26 17:44:17.520: INFO: Pod "pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1" satisfied condition "Succeeded or Failed"
Mar 26 17:44:17.524: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:44:17.570: INFO: Waiting for pod pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1 to disappear
Mar 26 17:44:17.587: INFO: Pod pod-configmaps-0582168a-29a2-490e-8d15-9dc0b840b5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:44:17.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6068" for this suite.

â€¢ [SLOW TEST:6.472 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":265,"skipped":4466,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:44:17.603: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-fdb99307-822e-4d82-b26c-93e5839d25a1 in namespace container-probe-7387
Mar 26 17:44:19.869: INFO: Started pod busybox-fdb99307-822e-4d82-b26c-93e5839d25a1 in namespace container-probe-7387
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:44:19.881: INFO: Initial restart count of pod busybox-fdb99307-822e-4d82-b26c-93e5839d25a1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:20.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7387" for this suite.

â€¢ [SLOW TEST:243.139 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":266,"skipped":4480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-d6ec376e-a319-4587-a679-08d781ff2791
STEP: Creating a pod to test consume configMaps
Mar 26 17:48:21.066: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7" in namespace "projected-6656" to be "Succeeded or Failed"
Mar 26 17:48:21.070: INFO: Pod "pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.409858ms
Mar 26 17:48:23.086: INFO: Pod "pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019909884s
Mar 26 17:48:25.107: INFO: Pod "pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040059214s
STEP: Saw pod success
Mar 26 17:48:25.107: INFO: Pod "pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7" satisfied condition "Succeeded or Failed"
Mar 26 17:48:25.125: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:48:25.158: INFO: Waiting for pod pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7 to disappear
Mar 26 17:48:25.163: INFO: Pod pod-projected-configmaps-9c7539f9-de9f-48a3-9dbb-8c3887b8c0c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6656" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":267,"skipped":4509,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:25.172: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-1686/configmap-test-b8deb098-7dc0-43a3-944c-90b8f3eca1f7
STEP: Creating a pod to test consume configMaps
Mar 26 17:48:25.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586" in namespace "configmap-1686" to be "Succeeded or Failed"
Mar 26 17:48:25.353: INFO: Pod "pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581529ms
Mar 26 17:48:27.359: INFO: Pod "pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008881375s
STEP: Saw pod success
Mar 26 17:48:27.359: INFO: Pod "pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586" satisfied condition "Succeeded or Failed"
Mar 26 17:48:27.362: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586 container env-test: <nil>
STEP: delete the pod
Mar 26 17:48:27.382: INFO: Waiting for pod pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586 to disappear
Mar 26 17:48:27.384: INFO: Pod pod-configmaps-d6a3c561-3a3e-4a46-9fd2-d16547e2f586 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:27.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1686" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":275,"completed":268,"skipped":4517,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 26 17:48:37.624: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0326 17:48:37.624369      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:37.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3985" for this suite.

â€¢ [SLOW TEST:10.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":275,"completed":269,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:37.643: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:48:38.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:48:41.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:48:41.639: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Mar 26 17:48:47.197: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:47.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8431" for this suite.
STEP: Destroying namespace "webhook-8431-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.675 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":275,"completed":270,"skipped":4574,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:48.319: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-8186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
STEP: Verifying LimitRange creation was observed
Mar 26 17:48:48.610: INFO: observed the limitRanges list
STEP: Fetching the LimitRange to ensure it has proper values
Mar 26 17:48:48.653: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 26 17:48:48.653: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 26 17:48:48.670: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 26 17:48:48.670: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 26 17:48:48.764: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 26 17:48:48.764: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 26 17:48:55.849: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:55.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8186" for this suite.

â€¢ [SLOW TEST:7.587 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":275,"completed":271,"skipped":4580,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:55.911: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Mar 26 17:48:56.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194310689 api-versions'
Mar 26 17:48:56.261: INFO: stderr: ""
Mar 26 17:48:56.261: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:56.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2867" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":275,"completed":272,"skipped":4599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:56.283: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 26 17:48:57.617: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0326 17:48:57.616887      21 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:57.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2868" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":275,"completed":273,"skipped":4671,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:57.696: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 17:48:58.028: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:48:58.031: INFO: Number of nodes with available pods: 0
Mar 26 17:48:58.031: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:48:59.043: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:48:59.048: INFO: Number of nodes with available pods: 0
Mar 26 17:48:59.048: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:00.041: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:00.046: INFO: Number of nodes with available pods: 0
Mar 26 17:49:00.046: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:01.084: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:01.093: INFO: Number of nodes with available pods: 1
Mar 26 17:49:01.093: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:02.039: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:02.045: INFO: Number of nodes with available pods: 2
Mar 26 17:49:02.045: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 26 17:49:02.095: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:02.105: INFO: Number of nodes with available pods: 1
Mar 26 17:49:02.105: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:03.128: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:03.143: INFO: Number of nodes with available pods: 1
Mar 26 17:49:03.143: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:04.109: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:04.111: INFO: Number of nodes with available pods: 1
Mar 26 17:49:04.111: INFO: Node kubedee-test-worker-dqhapg is running more than one daemon pod
Mar 26 17:49:05.111: INFO: DaemonSet pods can't tolerate node kubedee-test-controller with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 17:49:05.114: INFO: Number of nodes with available pods: 2
Mar 26 17:49:05.114: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7897, will wait for the garbage collector to delete the pods
Mar 26 17:49:05.190: INFO: Deleting DaemonSet.extensions daemon-set took: 19.015195ms
Mar 26 17:49:05.296: INFO: Terminating DaemonSet.extensions daemon-set pods took: 106.346774ms
Mar 26 17:49:14.007: INFO: Number of nodes with available pods: 0
Mar 26 17:49:14.010: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 17:49:14.019: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7897/daemonsets","resourceVersion":"27876"},"items":null}

Mar 26 17:49:14.027: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7897/pods","resourceVersion":"27876"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:14.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7897" for this suite.

â€¢ [SLOW TEST:16.344 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":275,"completed":274,"skipped":4674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:14.042: INFO: >>> kubeConfig: /tmp/kubeconfig-194310689
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:49:16.371: INFO: Waiting up to 5m0s for pod "client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661" in namespace "pods-376" to be "Succeeded or Failed"
Mar 26 17:49:16.391: INFO: Pod "client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661": Phase="Pending", Reason="", readiness=false. Elapsed: 20.47657ms
Mar 26 17:49:18.397: INFO: Pod "client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025871069s
STEP: Saw pod success
Mar 26 17:49:18.397: INFO: Pod "client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661" satisfied condition "Succeeded or Failed"
Mar 26 17:49:18.401: INFO: Trying to get logs from node kubedee-test-worker-dqhapg pod client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661 container env3cont: <nil>
STEP: delete the pod
Mar 26 17:49:18.430: INFO: Waiting for pod client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661 to disappear
Mar 26 17:49:18.436: INFO: Pod client-envvars-4c47f5f8-ebe0-4b7e-93ba-baeaaf596661 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-376" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":275,"completed":275,"skipped":4702,"failed":0}
SSSSSSSSSSSSSSSMar 26 17:49:18.444: INFO: Running AfterSuite actions on all nodes
Mar 26 17:49:18.444: INFO: Running AfterSuite actions on node 1
Mar 26 17:49:18.444: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":275,"completed":275,"skipped":4717,"failed":0}

Ran 275 of 4992 Specs in 4558.304 seconds
SUCCESS! -- 275 Passed | 0 Failed | 0 Pending | 4717 Skipped
PASS

Ginkgo ran 1 suite in 1h16m2.043595982s
Test Suite Passed
