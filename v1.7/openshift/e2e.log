Oct 31 09:18:36.843: INFO: Overriding default scale value of zero to 1
Oct 31 09:18:36.843: INFO: Overriding default milliseconds value of zero to 5000
I1031 09:18:36.878620    7114 e2e.go:344] Starting e2e run "799cb00b-be1c-11e7-9459-0e11a30959be" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1509441516 - Will randomize all specs
Will run 148 of 651 specs

I1031 09:18:36.898114    7114 e2e.go:69] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
Oct 31 09:18:36.898: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:18:36.900: INFO: Waiting up to 4h0m0s for all (but 1) nodes to be schedulable
Oct 31 09:18:36.964: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 31 09:18:37.009: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 31 09:18:37.009: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 31 09:18:37.024: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Oct 31 09:18:37.024: INFO: Dumping network health container logs from all nodes to file /data/src/github.com/openshift/origin/_output/scripts/conformance-k8s/artifacts/nethealth.txt
I1031 09:18:37.039607    7114 e2e.go:69] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
SS
------------------------------
[k8s.io] Projected 
  should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:935
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:18:37.039: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:935
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:18:37.183: INFO: Waiting up to 5m0s for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:18:37.198: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.742256ms elapsed)
Oct 31 09:18:39.214: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (2.031246195s elapsed)
Oct 31 09:18:41.229: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (4.046616129s elapsed)
Oct 31 09:18:43.245: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (6.061996102s elapsed)
Oct 31 09:18:45.260: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (8.077191482s elapsed)
Oct 31 09:18:47.275: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gdlzp' status to be 'success or failure'(found phase: "Pending", readiness: false) (10.092285614s elapsed)
STEP: Saw pod success
Oct 31 09:18:49.306: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:18:49.377: INFO: Waiting for pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be to disappear
Oct 31 09:18:49.391: INFO: Pod downwardapi-volume-79e75dd0-be1c-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:18:49.391: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gdlzp" for this suite.
Oct 31 09:18:56.003: INFO: namespace: e2e-tests-projected-gdlzp, resource: bindings, ignored listing per whitelist
Oct 31 09:18:56.726: INFO: namespace e2e-tests-projected-gdlzp deletion completed in 7.318861997s

• [SLOW TEST:19.686 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:935
------------------------------
SSSS
------------------------------
[k8s.io] Downward API volume 
  should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:49
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:18:56.726: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:49
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:18:56.846: INFO: Waiting up to 5m0s for pod downwardapi-volume-859fe0be-be1c-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:18:56.861: INFO: Waiting for pod downwardapi-volume-859fe0be-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-2jjg4' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.954056ms elapsed)
STEP: Saw pod success
Oct 31 09:18:58.892: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-859fe0be-be1c-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:18:58.955: INFO: Waiting for pod downwardapi-volume-859fe0be-be1c-11e7-9459-0e11a30959be to disappear
Oct 31 09:18:58.971: INFO: Pod downwardapi-volume-859fe0be-be1c-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:18:58.971: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2jjg4" for this suite.
Oct 31 09:19:05.564: INFO: namespace: e2e-tests-downward-api-2jjg4, resource: bindings, ignored listing per whitelist
Oct 31 09:19:06.300: INFO: namespace e2e-tests-downward-api-2jjg4 deletion completed in 7.312616414s

• [SLOW TEST:9.574 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:49
------------------------------
SSSS
------------------------------
[k8s.io] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance] [Flaky]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:193
[BeforeEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:19:06.300: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:47
[It] should be submitted and removed [Conformance] [Flaky]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:193
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Oct 31 09:19:12.505: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:19:21.236: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xb49t" for this suite.
Oct 31 09:19:27.929: INFO: namespace: e2e-tests-pods-xb49t, resource: bindings, ignored listing per whitelist
Oct 31 09:19:28.556: INFO: namespace e2e-tests-pods-xb49t deletion completed in 7.305499021s

• [SLOW TEST:22.257 seconds]
[k8s.io] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should be submitted and removed [Conformance] [Flaky]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:193
------------------------------
[k8s.io] DNS 
  should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:341
[BeforeEach] [k8s.io] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:19:28.556: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:341
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-q6kgm.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-q6kgm.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-q6kgm.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-q6kgm.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-q6kgm.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-q6kgm.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 31 09:19:47.077: INFO: DNS probes using dns-test-989c7db0-be1c-11e7-9459-0e11a30959be succeeded

STEP: deleting the pod
[AfterEach] [k8s.io] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:19:47.096: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-q6kgm" for this suite.
Oct 31 09:19:53.936: INFO: namespace: e2e-tests-dns-q6kgm, resource: bindings, ignored listing per whitelist
Oct 31 09:19:54.416: INFO: namespace e2e-tests-dns-q6kgm deletion completed in 7.304174305s

• [SLOW TEST:25.859 seconds]
[k8s.io] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:341
------------------------------
[k8s.io] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:257
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:19:54.416: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:108
Oct 31 09:19:54.523: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 31 09:20:54.626: INFO: Waiting for terminating namespaces to be deleted...
Oct 31 09:20:54.657: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 31 09:20:54.702: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 31 09:20:54.702: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 31 09:20:54.718: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Oct 31 09:20:54.718: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-2mxx before test
Oct 31 09:20:54.750: INFO: registry-console-1-rhw5g from default started at 2017-10-31 09:12:05 +0000 UTC (1 container statuses recorded)
Oct 31 09:20:54.750: INFO: 	Container registry-console ready: true, restart count 0
Oct 31 09:20:54.750: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-cfzz before test
Oct 31 09:20:54.784: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-sttk before test
[It] validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:257
STEP: verifying the node has the label node ci-prtest-cc63063-94-ig-n-2mxx
STEP: verifying the node has the label node ci-prtest-cc63063-94-ig-n-cfzz
STEP: verifying the node has the label node ci-prtest-cc63063-94-ig-n-sttk
Oct 31 09:20:54.946: INFO: Pod registry-console-1-rhw5g requesting resource cpu=0m on Node ci-prtest-cc63063-94-ig-n-2mxx
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d30160647d0], Reason = [Scheduled], Message = [Successfully assigned filler-pod-ci-prtest-cc63063-94-ig-n-2mxx to ci-prtest-cc63063-94-ig-n-2mxx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d301fa7e799], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-29vzl" ]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d303e41f2f5], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d304e4e0c78], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d3052f88f26], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-2mxx.14f29d305a3106fa], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d3017d4aba8], Reason = [Scheduled], Message = [Successfully assigned filler-pod-ci-prtest-cc63063-94-ig-n-cfzz to ci-prtest-cc63063-94-ig-n-cfzz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d301f585195], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-29vzl" ]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d3040ce1d4f], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d305032230c], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d3054816ac3], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-cfzz.14f29d305bcad7f6], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d3019a8c004], Reason = [Scheduled], Message = [Successfully assigned filler-pod-ci-prtest-cc63063-94-ig-n-sttk to ci-prtest-cc63063-94-ig-n-sttk]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d30293d2700], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-29vzl" ]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d30448868dd], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d30529acdfa], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d305703c1b9], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ci-prtest-cc63063-94-ig-n-sttk.14f29d305dd0fa40], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.14f29d3097ebc61d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ci-prtest-cc63063-94-ig-n-2mxx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ci-prtest-cc63063-94-ig-n-cfzz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ci-prtest-cc63063-94-ig-n-sttk
STEP: verifying the node doesn't have the label node
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:20:58.320: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-lggmv" for this suite.
Oct 31 09:21:21.573: INFO: namespace: e2e-tests-sched-pred-lggmv, resource: bindings, ignored listing per whitelist
Oct 31 09:21:21.662: INFO: namespace e2e-tests-sched-pred-lggmv deletion completed in 23.32556512s
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:87.246 seconds]
[k8s.io] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:257
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1060
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:21:21.662: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1006
STEP: creating an rc
Oct 31 09:21:21.788: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-h5j7f'
Oct 31 09:21:22.674: INFO: stderr: ""
Oct 31 09:21:22.674: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
[It] should be able to retrieve and filter logs [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1060
STEP: Waiting for Redis master to start.
Oct 31 09:21:23.704: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:21:23.704: INFO: Found 0 / 1
Oct 31 09:21:24.704: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:21:24.704: INFO: Found 0 / 1
Oct 31 09:21:25.704: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:21:25.704: INFO: Found 1 / 1
Oct 31 09:21:25.704: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 31 09:21:25.720: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:21:25.720: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Oct 31 09:21:25.720: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f'
Oct 31 09:21:25.925: INFO: stderr: ""
Oct 31 09:21:25.925: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.5 (c72176ef/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n1:M 31 Oct 09:21:24.141 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Oct 09:21:24.141 # Server started, Redis version 3.2.5\n1:M 31 Oct 09:21:24.142 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Oct 09:21:24.142 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Oct 31 09:21:25.926: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f --tail=1'
Oct 31 09:21:26.125: INFO: stderr: ""
Oct 31 09:21:26.125: INFO: stdout: "1:M 31 Oct 09:21:24.142 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Oct 31 09:21:26.125: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f --limit-bytes=1'
Oct 31 09:21:26.328: INFO: stderr: ""
Oct 31 09:21:26.328: INFO: stdout: " "
STEP: exposing timestamps
Oct 31 09:21:26.328: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f --tail=1 --timestamps'
Oct 31 09:21:26.530: INFO: stderr: ""
Oct 31 09:21:26.530: INFO: stdout: "2017-10-31T09:21:24.146761000Z 1:M 31 Oct 09:21:24.142 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Oct 31 09:21:29.030: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f --since=1s'
Oct 31 09:21:29.233: INFO: stderr: ""
Oct 31 09:21:29.233: INFO: stdout: ""
Oct 31 09:21:29.233: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-7xssx redis-master --namespace=e2e-tests-kubectl-h5j7f --since=24h'
Oct 31 09:21:29.436: INFO: stderr: ""
Oct 31 09:21:29.436: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.5 (c72176ef/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n1:M 31 Oct 09:21:24.141 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Oct 09:21:24.141 # Server started, Redis version 3.2.5\n1:M 31 Oct 09:21:24.142 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Oct 09:21:24.142 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1009
STEP: using delete to clean up resources
Oct 31 09:21:29.436: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-h5j7f'
Oct 31 09:21:29.708: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 09:21:29.708: INFO: stdout: "replicationcontroller \"redis-master\" deleted\n"
Oct 31 09:21:29.708: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-h5j7f'
Oct 31 09:21:29.918: INFO: stderr: "No resources found.\n"
Oct 31 09:21:29.918: INFO: stdout: ""
Oct 31 09:21:29.918: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-h5j7f -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 31 09:21:30.110: INFO: stderr: ""
Oct 31 09:21:30.110: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:21:30.110: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h5j7f" for this suite.
Oct 31 09:21:36.658: INFO: namespace: e2e-tests-kubectl-h5j7f, resource: bindings, ignored listing per whitelist
Oct 31 09:21:37.437: INFO: namespace e2e-tests-kubectl-h5j7f deletion completed in 7.310926596s

• [SLOW TEST:15.775 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should be able to retrieve and filter logs [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1060
------------------------------
SSSS
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:60
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:21:37.437: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:60
STEP: Creating configMap with name configmap-test-volume-map-e56bb087-be1c-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:21:37.583: INFO: Waiting up to 5m0s for pod pod-configmaps-e56e1e57-be1c-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:21:37.598: INFO: Waiting for pod pod-configmaps-e56e1e57-be1c-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-gkfz4' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.792898ms elapsed)
STEP: Saw pod success
Oct 31 09:21:39.629: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-configmaps-e56e1e57-be1c-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:21:39.668: INFO: Waiting for pod pod-configmaps-e56e1e57-be1c-11e7-9459-0e11a30959be to disappear
Oct 31 09:21:39.683: INFO: Pod pod-configmaps-e56e1e57-be1c-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:21:39.683: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gkfz4" for this suite.
Oct 31 09:21:47.020: INFO: namespace: e2e-tests-configmap-gkfz4, resource: bindings, ignored listing per whitelist
Oct 31 09:21:47.021: INFO: namespace e2e-tests-configmap-gkfz4 deletion completed in 7.321545386s

• [SLOW TEST:9.584 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:60
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:150
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:21:47.021: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:150
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-l4fl2
Oct 31 09:21:49.174: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-l4fl2
STEP: checking the pod's current state and verifying that restartCount is present
Oct 31 09:21:49.189: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:23:50.138: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-l4fl2" for this suite.
Oct 31 09:23:56.905: INFO: namespace: e2e-tests-container-probe-l4fl2, resource: bindings, ignored listing per whitelist
Oct 31 09:23:57.472: INFO: namespace e2e-tests-container-probe-l4fl2 deletion completed in 7.318558263s

• [SLOW TEST:130.451 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:150
------------------------------
[k8s.io] Projected 
  should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:171
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:23:57.472: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:171
STEP: Creating secret with name projected-secret-test-38e027d2-be1d-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:23:57.597: INFO: Waiting up to 5m0s for pod pod-projected-secrets-38e2950e-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:23:57.612: INFO: Waiting for pod pod-projected-secrets-38e2950e-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-8ggsw' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.494797ms elapsed)
STEP: Saw pod success
Oct 31 09:23:59.642: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-projected-secrets-38e2950e-be1d-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:23:59.682: INFO: Waiting for pod pod-projected-secrets-38e2950e-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:23:59.697: INFO: Pod pod-projected-secrets-38e2950e-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:23:59.697: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8ggsw" for this suite.
Oct 31 09:24:06.577: INFO: namespace: e2e-tests-projected-8ggsw, resource: bindings, ignored listing per whitelist
Oct 31 09:24:07.020: INFO: namespace e2e-tests-projected-8ggsw deletion completed in 7.307903868s

• [SLOW TEST:9.548 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:171
------------------------------
SSSSSSS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:385
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:24:07.020: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:385
STEP: Creating secret with name secret-test-3e97deac-be1d-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:24:07.194: INFO: Waiting up to 5m0s for pod pod-secrets-3e9aa83c-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:24:07.209: INFO: Waiting for pod pod-secrets-3e9aa83c-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-qpzhw' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.0814ms elapsed)
STEP: Saw pod success
Oct 31 09:24:09.240: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-secrets-3e9aa83c-be1d-11e7-9459-0e11a30959be container secret-env-test: <nil>
STEP: delete the pod
Oct 31 09:24:09.279: INFO: Waiting for pod pod-secrets-3e9aa83c-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:24:09.294: INFO: Pod pod-secrets-3e9aa83c-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:24:09.294: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qpzhw" for this suite.
Oct 31 09:24:15.852: INFO: namespace: e2e-tests-secrets-qpzhw, resource: bindings, ignored listing per whitelist
Oct 31 09:24:16.646: INFO: namespace e2e-tests-secrets-qpzhw deletion completed in 7.335523875s

• [SLOW TEST:9.626 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:385
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:145
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:24:16.646: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:128
[It] should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:145
STEP: creating pod
Oct 31 09:24:18.862: INFO: Pod pod-hostip-4452ca49-be1d-11e7-9459-0e11a30959be has hostIP: 10.142.0.2
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:24:18.862: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rcj7l" for this suite.
Oct 31 09:24:41.465: INFO: namespace: e2e-tests-pods-rcj7l, resource: bindings, ignored listing per whitelist
Oct 31 09:24:42.225: INFO: namespace e2e-tests-pods-rcj7l deletion completed in 23.347268462s

• [SLOW TEST:25.580 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:145
------------------------------
S
------------------------------
[k8s.io] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:277
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:24:42.226: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:108
Oct 31 09:24:42.343: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 31 09:25:42.448: INFO: Waiting for terminating namespaces to be deleted...
Oct 31 09:25:42.479: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 31 09:25:42.524: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 31 09:25:42.524: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 31 09:25:42.541: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Oct 31 09:25:42.541: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-2mxx before test
Oct 31 09:25:42.575: INFO: registry-console-1-rhw5g from default started at 2017-10-31 09:12:05 +0000 UTC (1 container statuses recorded)
Oct 31 09:25:42.575: INFO: 	Container registry-console ready: true, restart count 0
Oct 31 09:25:42.575: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-cfzz before test
Oct 31 09:25:42.606: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-sttk before test
[It] validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:277
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.14f29d7314a3fff9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 MatchNodeSelector.]
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:25:43.737: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-49b2n" for this suite.
Oct 31 09:26:06.888: INFO: namespace: e2e-tests-sched-pred-49b2n, resource: bindings, ignored listing per whitelist
Oct 31 09:26:07.080: INFO: namespace e2e-tests-sched-pred-49b2n deletion completed in 23.326959179s
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:84.854 seconds]
[k8s.io] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:277
------------------------------
SSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1426
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:26:07.080: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should create a job from an image, then delete the job [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1426
STEP: executing a command with run --rm and attach with stdin
Oct 31 09:26:07.197: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig --namespace=e2e-tests-kubectl-hmkqm run e2e-test-rm-busybox-job --image=gcr.io/google_containers/busybox:1.24 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Oct 31 09:26:11.961: INFO: stderr: "If you don't see a command prompt, try pressing enter.\n"
Oct 31 09:26:11.961: INFO: stdout: "abcd1234stdin closed\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:26:11.976: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hmkqm" for this suite.
Oct 31 09:26:18.770: INFO: namespace: e2e-tests-kubectl-hmkqm, resource: bindings, ignored listing per whitelist
Oct 31 09:26:19.323: INFO: namespace e2e-tests-kubectl-hmkqm deletion completed in 7.329818401s

• [SLOW TEST:12.243 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run --rm job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create a job from an image, then delete the job [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1426
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1233
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:26:19.323: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1209
[It] should support rolling-update to same image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1233
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:26:19.428: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=gcr.io/google_containers/nginx-slim:0.7 --generator=run/v1 --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:20.092: INFO: stderr: ""
Oct 31 09:26:20.092: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Oct 31 09:26:20.122: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=gcr.io/google_containers/nginx-slim:0.7 --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:34.001: INFO: stderr: ""
Oct 31 09:26:34.001: INFO: stdout: "Created e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12\nScaling up e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12 to e2e-test-nginx-rc\nreplicationcontroller \"e2e-test-nginx-rc\" rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Oct 31 09:26:34.001: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:34.194: INFO: stderr: ""
Oct 31 09:26:34.194: INFO: stdout: "e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12-d5dqs "
Oct 31 09:26:34.194: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12-d5dqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:34.385: INFO: stderr: ""
Oct 31 09:26:34.385: INFO: stdout: "true"
Oct 31 09:26:34.385: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12-d5dqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:34.574: INFO: stderr: ""
Oct 31 09:26:34.574: INFO: stdout: "gcr.io/google_containers/nginx-slim:0.7"
Oct 31 09:26:34.574: INFO: e2e-test-nginx-rc-44f7c0c76d807e0a7716f707f85aba12-d5dqs is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1213
Oct 31 09:26:34.574: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m99bg'
Oct 31 09:26:34.856: INFO: stderr: ""
Oct 31 09:26:34.856: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:26:34.856: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m99bg" for this suite.
Oct 31 09:26:58.120: INFO: namespace: e2e-tests-kubectl-m99bg, resource: bindings, ignored listing per whitelist
Oct 31 09:26:58.210: INFO: namespace e2e-tests-kubectl-m99bg deletion completed in 23.33706362s

• [SLOW TEST:38.886 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should support rolling-update to same image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1233
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1101
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:26:58.210: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should check is all data is printed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1101
Oct 31 09:26:58.320: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version'
Oct 31 09:26:58.496: INFO: stderr: ""
Oct 31 09:26:58.496: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"7+\", GitVersion:\"v1.7.10-beta.0.33+3f573516fbbf05\", GitCommit:\"3f573516fbbf05bf28a463773192773debf060a7\", GitTreeState:\"clean\", BuildDate:\"2017-10-31T09:14:09Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.6+a08f5eeb62\", GitCommit:\"c84beff\", GitTreeState:\"clean\", BuildDate:\"2017-10-31T08:38:59Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:26:58.496: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6vd7t" for this suite.
Oct 31 09:27:05.230: INFO: namespace: e2e-tests-kubectl-6vd7t, resource: bindings, ignored listing per whitelist
Oct 31 09:27:05.841: INFO: namespace e2e-tests-kubectl-6vd7t deletion completed in 7.329425571s

• [SLOW TEST:7.631 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl version
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should check is all data is printed [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1101
------------------------------
S
------------------------------
[k8s.io] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:238
[BeforeEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:27:05.842: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:200
[It] should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:238
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:27:05.988: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wknv2" for this suite.
Oct 31 09:27:28.614: INFO: namespace: e2e-tests-pods-wknv2, resource: bindings, ignored listing per whitelist
Oct 31 09:27:29.325: INFO: namespace e2e-tests-pods-wknv2 deletion completed in 23.321022726s

• [SLOW TEST:23.483 seconds]
[k8s.io] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should be submitted and removed [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:238
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:676
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:27:29.325: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should check if v1 is in available api versions [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:676
STEP: validating api verions
Oct 31 09:27:29.433: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig api-versions'
Oct 31 09:27:29.642: INFO: stderr: ""
Oct 31 09:27:29.642: INFO: stdout: "apiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1beta1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nbatch/v1\nbatch/v2alpha1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsecurity.openshift.io/v1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:27:29.642: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qv8k8" for this suite.
Oct 31 09:27:36.315: INFO: namespace: e2e-tests-kubectl-qv8k8, resource: bindings, ignored listing per whitelist
Oct 31 09:27:36.988: INFO: namespace e2e-tests-kubectl-qv8k8 deletion completed in 7.330294186s

• [SLOW TEST:7.663 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl api-versions
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should check if v1 is in available api versions [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:676
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:27:36.988: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
STEP: Creating configMap with name projected-configmap-test-volume-bbbc0d15-be1d-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:27:37.141: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-bbbe6e59-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:27:37.156: INFO: Waiting for pod pod-projected-configmaps-bbbe6e59-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-crt57' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.833055ms elapsed)
STEP: Saw pod success
Oct 31 09:27:39.187: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-configmaps-bbbe6e59-be1d-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:27:39.225: INFO: Waiting for pod pod-projected-configmaps-bbbe6e59-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:27:39.240: INFO: Pod pod-projected-configmaps-bbbe6e59-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:27:39.240: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-crt57" for this suite.
Oct 31 09:27:46.251: INFO: namespace: e2e-tests-projected-crt57, resource: bindings, ignored listing per whitelist
Oct 31 09:27:46.581: INFO: namespace e2e-tests-projected-crt57 deletion completed in 7.324708793s

• [SLOW TEST:9.592 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:51
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:27:46.581: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:51
STEP: Creating secret with name secret-test-c176a76d-be1d-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:27:46.753: INFO: Waiting up to 5m0s for pod pod-secrets-c1791387-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:27:46.768: INFO: Waiting for pod pod-secrets-c1791387-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-ngq84' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.092673ms elapsed)
STEP: Saw pod success
Oct 31 09:27:48.800: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-secrets-c1791387-be1d-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:27:48.842: INFO: Waiting for pod pod-secrets-c1791387-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:27:48.858: INFO: Pod pod-secrets-c1791387-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:27:48.858: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ngq84" for this suite.
Oct 31 09:27:55.402: INFO: namespace: e2e-tests-secrets-ngq84, resource: bindings, ignored listing per whitelist
Oct 31 09:27:56.213: INFO: namespace e2e-tests-secrets-ngq84 deletion completed in 7.338273137s

• [SLOW TEST:9.633 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:51
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:315
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:27:56.214: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:128
[It] should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:315
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 31 09:27:58.941: INFO: Successfully updated pod "pod-update-c730cd2c-be1d-11e7-9459-0e11a30959be"
STEP: verifying the updated pod is in kubernetes
Oct 31 09:27:58.971: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:27:58.971: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-k7wll" for this suite.
Oct 31 09:28:21.911: INFO: namespace: e2e-tests-pods-k7wll, resource: bindings, ignored listing per whitelist
Oct 31 09:28:22.310: INFO: namespace e2e-tests-pods-k7wll deletion completed in 23.323436426s

• [SLOW TEST:26.097 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:315
------------------------------
S
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1399
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:28:22.311: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1361
[It] should update a single-container pod's image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1399
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:28:22.420: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=gcr.io/google_containers/nginx-slim:0.7 --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-qk6t4'
Oct 31 09:28:23.089: INFO: stderr: ""
Oct 31 09:28:23.090: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Oct 31 09:28:28.090: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-qk6t4 -o json'
Oct 31 09:28:28.283: INFO: stderr: ""
Oct 31 09:28:28.283: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"privileged\"\n        },\n        \"creationTimestamp\": \"2017-10-31T09:28:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-qk6t4\",\n        \"resourceVersion\": \"4943\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-qk6t4/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d7225073-be1d-11e7-99d7-42010a8e0005\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"gcr.io/google_containers/nginx-slim:0.7\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"privileged\": false\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-nzdvr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-v4pbb\"\n            }\n        ],\n        \"nodeName\": \"ci-prtest-cc63063-94-ig-n-2mxx\",\n        \"nodeSelector\": {\n            \"role\": \"app\"\n        },\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-nzdvr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-nzdvr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2017-10-31T09:28:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2017-10-31T09:28:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2017-10-31T09:28:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://3fae6be4af56ed0a147236d9a568cb167898c2ecff093c1c4b60fccfd9ee9c14\",\n                \"image\": \"gcr.io/google_containers/nginx-slim:0.7\",\n                \"imageID\": \"docker-pullable://gcr.io/google_containers/nginx-slim@sha256:dd4efd4c13bec2c6f3fe855deeab9524efe434505568421d4f31820485b3a795\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2017-10-31T09:28:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.142.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.2.9\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2017-10-31T09:28:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 31 09:28:28.283: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-qk6t4'
Oct 31 09:28:28.530: INFO: stderr: ""
Oct 31 09:28:28.530: INFO: stdout: "pod \"e2e-test-nginx-pod\" replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image gcr.io/google_containers/busybox:1.24
[AfterEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1365
Oct 31 09:28:28.546: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-qk6t4'
Oct 31 09:28:28.760: INFO: stderr: ""
Oct 31 09:28:28.760: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:28:28.760: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qk6t4" for this suite.
Oct 31 09:28:52.075: INFO: namespace: e2e-tests-kubectl-qk6t4, resource: bindings, ignored listing per whitelist
Oct 31 09:28:52.121: INFO: namespace e2e-tests-kubectl-qk6t4 deletion completed in 23.344803584s

• [SLOW TEST:29.810 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should update a single-container pod's image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1399
------------------------------
SSSSS
------------------------------
[k8s.io] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:129
[BeforeEach] [k8s.io] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:28:52.121: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:129
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-e885b986-be1d-11e7-9459-0e11a30959be,GenerateName:,Namespace:e2e-tests-events-trb7f,SelfLink:/api/v1/namespaces/e2e-tests-events-trb7f/pods/send-events-e885b986-be1d-11e7-9459-0e11a30959be,UID:e8872409-be1d-11e7-99d7-42010a8e0005,ResourceVersion:5067,Generation:0,CreationTimestamp:2017-10-31 09:28:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 249647358,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mxr76 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mxr76,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/google_containers/serve_hostname:v1.4 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mxr76 true /var/run/secrets/kubernetes.io/serviceaccount }] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{role: app,},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci-prtest-cc63063-94-ig-n-sttk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[{default-dockercfg-wdp97}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2017-10-31 09:28:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2017-10-31 09:28:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2017-10-31 09:28:52 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:172.16.4.9,StartTime:2017-10-31 09:28:52 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2017-10-31 09:28:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/google_containers/serve_hostname:v1.4 docker-pullable://gcr.io/google_containers/serve_hostname@sha256:a49737ee84a3b94f0b977f32e60c5daf11f0b5636f1f7503a2981524f351c57a docker://bd2498388673967e7f5df437a3bf045cb1c0ebab8a8871114a6837f614485d17}],QOSClass:BestEffort,InitContainerStatuses:[],},}
STEP: checking for scheduler event about the pod
Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:28:58.379: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-trb7f" for this suite.
Oct 31 09:29:04.940: INFO: namespace: e2e-tests-events-trb7f, resource: bindings, ignored listing per whitelist
Oct 31 09:29:05.737: INFO: namespace e2e-tests-events-trb7f deletion completed in 7.342148398s

• [SLOW TEST:13.616 seconds]
[k8s.io] Events
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:129
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:44
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:29:05.738: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:44
STEP: Creating secret with name secret-test-f0a13094-be1d-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:29:05.889: INFO: Waiting up to 5m0s for pod pod-secrets-f0a44bb4-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:29:05.905: INFO: Waiting for pod pod-secrets-f0a44bb4-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-dnxwx' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.849548ms elapsed)
STEP: Saw pod success
Oct 31 09:29:07.936: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-secrets-f0a44bb4-be1d-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:29:07.979: INFO: Waiting for pod pod-secrets-f0a44bb4-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:29:07.994: INFO: Pod pod-secrets-f0a44bb4-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:29:07.994: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dnxwx" for this suite.
Oct 31 09:29:14.960: INFO: namespace: e2e-tests-secrets-dnxwx, resource: bindings, ignored listing per whitelist
Oct 31 09:29:15.350: INFO: namespace e2e-tests-secrets-dnxwx deletion completed in 7.340084726s

• [SLOW TEST:9.613 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:44
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:822
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:29:15.350: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:822
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:29:15.534: INFO: Waiting up to 5m0s for pod downwardapi-volume-f663f201-be1d-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:29:15.549: INFO: Waiting for pod downwardapi-volume-f663f201-be1d-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-nczpn' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.798223ms elapsed)
STEP: Saw pod success
Oct 31 09:29:17.580: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-f663f201-be1d-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:29:17.627: INFO: Waiting for pod downwardapi-volume-f663f201-be1d-11e7-9459-0e11a30959be to disappear
Oct 31 09:29:17.642: INFO: Pod downwardapi-volume-f663f201-be1d-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:29:17.642: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nczpn" for this suite.
Oct 31 09:29:24.884: INFO: namespace: e2e-tests-projected-nczpn, resource: bindings, ignored listing per whitelist
Oct 31 09:29:25.002: INFO: namespace e2e-tests-projected-nczpn deletion completed in 7.344857678s

• [SLOW TEST:9.652 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:822
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:994
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:29:25.003: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:970
STEP: creating the pod
Oct 31 09:29:25.113: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:25.460: INFO: stderr: ""
Oct 31 09:29:25.460: INFO: stdout: "pod \"pause\" created\n"
Oct 31 09:29:25.460: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 31 09:29:25.460: INFO: Waiting up to 5m0s for pod pause status to be running and ready
Oct 31 09:29:25.476: INFO: Waiting for pod pause in namespace 'e2e-tests-kubectl-xhsmk' status to be 'running and ready'(found phase: "Pending", readiness: false) (15.633957ms elapsed)
Oct 31 09:29:27.492: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:994
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 31 09:29:27.492: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:27.706: INFO: stderr: ""
Oct 31 09:29:27.706: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 31 09:29:27.706: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:27.902: INFO: stderr: ""
Oct 31 09:29:27.902: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          2s        testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 31 09:29:27.902: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:28.114: INFO: stderr: ""
Oct 31 09:29:28.114: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 31 09:29:28.114: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:28.307: INFO: stderr: ""
Oct 31 09:29:28.307: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          3s        \n"
[AfterEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:973
STEP: using delete to clean up resources
Oct 31 09:29:28.307: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:28.525: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 09:29:28.525: INFO: stdout: "pod \"pause\" deleted\n"
Oct 31 09:29:28.525: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-xhsmk'
Oct 31 09:29:28.738: INFO: stderr: "No resources found.\n"
Oct 31 09:29:28.738: INFO: stdout: ""
Oct 31 09:29:28.738: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-xhsmk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 31 09:29:28.934: INFO: stderr: ""
Oct 31 09:29:28.934: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:29:28.934: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xhsmk" for this suite.
Oct 31 09:29:35.463: INFO: namespace: e2e-tests-kubectl-xhsmk, resource: bindings, ignored listing per whitelist
Oct 31 09:29:36.292: INFO: namespace e2e-tests-kubectl-xhsmk deletion completed in 7.343010944s

• [SLOW TEST:11.290 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should update the label on a resource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:994
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1351
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:29:36.293: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1328
[It] should create a pod from an image when restart is Never [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1351
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:29:36.406: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=gcr.io/google_containers/nginx-slim:0.7 --namespace=e2e-tests-kubectl-f44tm'
Oct 31 09:29:37.053: INFO: stderr: ""
Oct 31 09:29:37.053: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1332
Oct 31 09:29:37.069: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-f44tm'
Oct 31 09:29:37.281: INFO: stderr: ""
Oct 31 09:29:37.281: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:29:37.281: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f44tm" for this suite.
Oct 31 09:29:44.047: INFO: namespace: e2e-tests-kubectl-f44tm, resource: bindings, ignored listing per whitelist
Oct 31 09:29:44.629: INFO: namespace e2e-tests-kubectl-f44tm deletion completed in 7.332589286s

• [SLOW TEST:8.337 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create a pod from an image when restart is Never [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1351
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:236
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:29:44.630: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:236
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-qr2f5
Oct 31 09:29:46.790: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-qr2f5
STEP: checking the pod's current state and verifying that restartCount is present
Oct 31 09:29:46.805: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:31:47.778: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qr2f5" for this suite.
Oct 31 09:31:54.676: INFO: namespace: e2e-tests-container-probe-qr2f5, resource: bindings, ignored listing per whitelist
Oct 31 09:31:55.122: INFO: namespace e2e-tests-container-probe-qr2f5 deletion completed in 7.328524809s

• [SLOW TEST:130.493 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:236
------------------------------
SSS
------------------------------
[k8s.io] Downward API 
  should provide pod and host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:94
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:31:55.122: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod and host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:94
STEP: Creating a pod to test downward api env vars
Oct 31 09:31:55.269: INFO: Waiting up to 5m0s for pod downward-api-5599a3f1-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:31:55.284: INFO: Waiting for pod downward-api-5599a3f1-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-hsv9s' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.678645ms elapsed)
STEP: Saw pod success
Oct 31 09:31:57.316: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downward-api-5599a3f1-be1e-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:31:57.365: INFO: Waiting for pod downward-api-5599a3f1-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:31:57.380: INFO: Pod downward-api-5599a3f1-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:31:57.380: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hsv9s" for this suite.
Oct 31 09:32:04.049: INFO: namespace: e2e-tests-downward-api-hsv9s, resource: bindings, ignored listing per whitelist
Oct 31 09:32:04.748: INFO: namespace e2e-tests-downward-api-hsv9s deletion completed in 7.352528153s

• [SLOW TEST:9.626 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide pod and host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:94
------------------------------
S
------------------------------
[k8s.io] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:960
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:32:04.749: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:960
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:32:04.873: INFO: Waiting up to 5m0s for pod downwardapi-volume-5b530f1b-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:32:04.889: INFO: Waiting for pod downwardapi-volume-5b530f1b-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-p7l6s' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.226277ms elapsed)
STEP: Saw pod success
Oct 31 09:32:06.921: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-5b530f1b-be1e-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:32:06.962: INFO: Waiting for pod downwardapi-volume-5b530f1b-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:32:06.977: INFO: Pod downwardapi-volume-5b530f1b-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:32:06.977: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p7l6s" for this suite.
Oct 31 09:32:14.251: INFO: namespace: e2e-tests-projected-p7l6s, resource: bindings, ignored listing per whitelist
Oct 31 09:32:14.325: INFO: namespace e2e-tests-projected-p7l6s deletion completed in 7.331795674s

• [SLOW TEST:9.576 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:960
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:61
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:32:14.325: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:61
STEP: Creating projection with secret that has name projected-secret-test-map-610dd257-be1e-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:32:14.501: INFO: Waiting up to 5m0s for pod pod-projected-secrets-61103920-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:32:14.516: INFO: Waiting for pod pod-projected-secrets-61103920-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-dknlr' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.972299ms elapsed)
STEP: Saw pod success
Oct 31 09:32:16.547: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-secrets-61103920-be1e-11e7-9459-0e11a30959be container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:32:16.590: INFO: Waiting for pod pod-projected-secrets-61103920-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:32:16.606: INFO: Pod pod-projected-secrets-61103920-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:32:16.606: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dknlr" for this suite.
Oct 31 09:32:23.468: INFO: namespace: e2e-tests-projected-dknlr, resource: bindings, ignored listing per whitelist
Oct 31 09:32:23.981: INFO: namespace e2e-tests-projected-dknlr deletion completed in 7.359399681s

• [SLOW TEST:9.656 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:61
------------------------------
S
------------------------------
[k8s.io] Downward API 
  should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:185
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:32:23.981: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:185
STEP: Creating a pod to test downward api env vars
Oct 31 09:32:24.126: INFO: Waiting up to 5m0s for pod downward-api-66cc10e4-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:32:24.142: INFO: Waiting for pod downward-api-66cc10e4-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-sfpgk' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.658837ms elapsed)
STEP: Saw pod success
Oct 31 09:32:26.173: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downward-api-66cc10e4-be1e-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:32:26.215: INFO: Waiting for pod downward-api-66cc10e4-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:32:26.230: INFO: Pod downward-api-66cc10e4-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:32:26.230: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sfpgk" for this suite.
Oct 31 09:32:33.413: INFO: namespace: e2e-tests-downward-api-sfpgk, resource: bindings, ignored listing per whitelist
Oct 31 09:32:33.628: INFO: namespace e2e-tests-downward-api-sfpgk deletion completed in 7.381412344s

• [SLOW TEST:9.647 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:185
------------------------------
SSS
------------------------------
[k8s.io] ConfigMap 
  updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:155
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:32:33.628: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:155
STEP: Creating configMap with name configmap-test-upd-6c8dcc5c-be1e-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-6c8dcc5c-be1e-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:32:37.920: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-g88bk" for this suite.
Oct 31 09:33:00.522: INFO: namespace: e2e-tests-configmap-g88bk, resource: bindings, ignored listing per whitelist
Oct 31 09:33:01.269: INFO: namespace e2e-tests-configmap-g88bk deletion completed in 23.332912843s

• [SLOW TEST:27.641 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:155
------------------------------
SSSSSSS
------------------------------
[k8s.io] Projected 
  should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:832
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:33:01.269: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:832
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:33:01.395: INFO: Waiting up to 5m0s for pod downwardapi-volume-7d03a9c7-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:33:01.410: INFO: Waiting for pod downwardapi-volume-7d03a9c7-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-68mtp' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.066576ms elapsed)
STEP: Saw pod success
Oct 31 09:33:03.442: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-7d03a9c7-be1e-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:33:03.480: INFO: Waiting for pod downwardapi-volume-7d03a9c7-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:33:03.495: INFO: Pod downwardapi-volume-7d03a9c7-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:33:03.495: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-68mtp" for this suite.
Oct 31 09:33:10.633: INFO: namespace: e2e-tests-projected-68mtp, resource: bindings, ignored listing per whitelist
Oct 31 09:33:10.844: INFO: namespace e2e-tests-projected-68mtp deletion completed in 7.333330149s

• [SLOW TEST:9.575 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:832
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1197
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:33:10.844: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1153
[It] should create an rc from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1197
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:33:10.998: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=gcr.io/google_containers/nginx-slim:0.7 --generator=run/v1 --namespace=e2e-tests-kubectl-5q8fl'
Oct 31 09:33:12.218: INFO: stderr: ""
Oct 31 09:33:12.218: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Oct 31 09:33:12.254: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-z2wqv]
Oct 31 09:33:12.254: INFO: Waiting up to 5m0s for pod e2e-test-nginx-rc-z2wqv status to be running and ready
Oct 31 09:33:12.270: INFO: Waiting for pod e2e-test-nginx-rc-z2wqv in namespace 'e2e-tests-kubectl-5q8fl' status to be 'running and ready'(found phase: "Pending", readiness: false) (15.612399ms elapsed)
Oct 31 09:33:14.286: INFO: Waiting for pod e2e-test-nginx-rc-z2wqv in namespace 'e2e-tests-kubectl-5q8fl' status to be 'running and ready'(found phase: "Pending", readiness: false) (2.031644897s elapsed)
Oct 31 09:33:16.302: INFO: Waiting for pod e2e-test-nginx-rc-z2wqv in namespace 'e2e-tests-kubectl-5q8fl' status to be 'running and ready'(found phase: "Pending", readiness: false) (4.048040104s elapsed)
Oct 31 09:33:18.318: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-z2wqv]
Oct 31 09:33:18.318: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5q8fl'
Oct 31 09:33:18.538: INFO: stderr: ""
[AfterEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1157
Oct 31 09:33:18.538: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5q8fl'
Oct 31 09:33:18.819: INFO: stderr: ""
Oct 31 09:33:18.819: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:33:18.819: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5q8fl" for this suite.
Oct 31 09:33:41.882: INFO: namespace: e2e-tests-kubectl-5q8fl, resource: bindings, ignored listing per whitelist
Oct 31 09:33:42.185: INFO: namespace e2e-tests-kubectl-5q8fl deletion completed in 23.349705447s

• [SLOW TEST:31.340 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create an rc from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1197
------------------------------
S
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:65
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:33:42.185: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:65
STEP: Creating configMap with name configmap-test-volume-map-95668a57-be1e-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:33:42.325: INFO: Waiting up to 5m0s for pod pod-configmaps-95690e8c-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:33:42.342: INFO: Waiting for pod pod-configmaps-95690e8c-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-dnrv6' status to be 'success or failure'(found phase: "Pending", readiness: false) (16.568961ms elapsed)
STEP: Saw pod success
Oct 31 09:33:44.373: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-configmaps-95690e8c-be1e-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:33:44.414: INFO: Waiting for pod pod-configmaps-95690e8c-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:33:44.429: INFO: Pod pod-configmaps-95690e8c-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:33:44.429: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dnrv6" for this suite.
Oct 31 09:33:51.138: INFO: namespace: e2e-tests-configmap-dnrv6, resource: bindings, ignored listing per whitelist
Oct 31 09:33:51.783: INFO: namespace e2e-tests-configmap-dnrv6 deletion completed in 7.338301307s

• [SLOW TEST:9.598 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:65
------------------------------
S
------------------------------
[k8s.io] ReplicationController 
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/rc.go:42
[BeforeEach] [k8s.io] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:33:51.783: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/rc.go:42
STEP: Creating replication controller my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be
Oct 31 09:33:51.946: INFO: Pod name my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be: Found 1 pods out of 1
Oct 31 09:33:51.946: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be" are running
Oct 31 09:33:53.978: INFO: Pod "my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be-dckkp" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2017-10-31 09:33:51 +0000 UTC Reason: Message:}])
Oct 31 09:33:53.978: INFO: Trying to dial the pod
Oct 31 09:33:59.043: INFO: Controller my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be: Got expected result from replica 1 [my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be-dckkp]: "my-hostname-basic-9b2237b4-be1e-11e7-9459-0e11a30959be-dckkp", 1 of 1 required successes so far
[AfterEach] [k8s.io] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:33:59.043: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-v5lk2" for this suite.
Oct 31 09:34:06.060: INFO: namespace: e2e-tests-replication-controller-v5lk2, resource: bindings, ignored listing per whitelist
Oct 31 09:34:06.410: INFO: namespace e2e-tests-replication-controller-v5lk2 deletion completed in 7.351154521s

• [SLOW TEST:14.626 seconds]
[k8s.io] ReplicationController
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/rc.go:42
------------------------------
SSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:90
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:34:06.410: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:90
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 31 09:34:06.543: INFO: Waiting up to 5m0s for pod pod-a3d86dbb-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:34:06.559: INFO: Waiting for pod pod-a3d86dbb-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-kc2qp' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.827741ms elapsed)
STEP: Saw pod success
Oct 31 09:34:08.591: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-a3d86dbb-be1e-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:34:08.631: INFO: Waiting for pod pod-a3d86dbb-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:34:08.646: INFO: Pod pod-a3d86dbb-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:34:08.646: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kc2qp" for this suite.
Oct 31 09:34:15.177: INFO: namespace: e2e-tests-emptydir-kc2qp, resource: bindings, ignored listing per whitelist
Oct 31 09:34:15.997: INFO: namespace e2e-tests-emptydir-kc2qp deletion completed in 7.334937582s

• [SLOW TEST:9.587 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:90
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:55
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:34:15.997: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:55
STEP: Creating a pod to test override command
Oct 31 09:34:16.137: INFO: Waiting up to 5m0s for pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:34:16.152: INFO: Waiting for pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-containers-2b795' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.078871ms elapsed)
Oct 31 09:34:18.168: INFO: Waiting for pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-containers-2b795' status to be 'success or failure'(found phase: "Pending", readiness: false) (2.030643949s elapsed)
STEP: Saw pod success
Oct 31 09:34:20.199: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:34:20.240: INFO: Waiting for pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:34:20.255: INFO: Pod client-containers-a9906416-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:34:20.255: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2b795" for this suite.
Oct 31 09:34:27.272: INFO: namespace: e2e-tests-containers-2b795, resource: bindings, ignored listing per whitelist
Oct 31 09:34:27.624: INFO: namespace e2e-tests-containers-2b795 deletion completed in 7.353406945s

• [SLOW TEST:11.627 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:55
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:163
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:34:27.624: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:163
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:34:27.767: INFO: Waiting up to 5m0s for pod downwardapi-volume-b07e22ba-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:34:27.782: INFO: Waiting for pod downwardapi-volume-b07e22ba-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-zp2j7' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.413451ms elapsed)
STEP: Saw pod success
Oct 31 09:34:29.814: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-b07e22ba-be1e-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:34:29.855: INFO: Waiting for pod downwardapi-volume-b07e22ba-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:34:29.870: INFO: Pod downwardapi-volume-b07e22ba-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:34:29.870: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zp2j7" for this suite.
Oct 31 09:34:37.033: INFO: namespace: e2e-tests-downward-api-zp2j7, resource: bindings, ignored listing per whitelist
Oct 31 09:34:37.260: INFO: namespace e2e-tests-downward-api-zp2j7 deletion completed in 7.373640618s

• [SLOW TEST:9.636 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:163
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:266
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:34:37.260: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:128
[It] should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:266
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Oct 31 09:34:39.483: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-b63b13c0-be1e-11e7-9459-0e11a30959be", GenerateName:"", Namespace:"e2e-tests-pods-vvphf", SelfLink:"/api/v1/namespaces/e2e-tests-pods-vvphf/pods/pod-submit-remove-b63b13c0-be1e-11e7-9459-0e11a30959be", UID:"b6412cc9-be1e-11e7-99d7-42010a8e0005", ResourceVersion:"6820", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:63645039277, nsec:0, loc:(*time.Location)(0x5786480)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"371085486"}, Annotations:map[string]string{"openshift.io/scc":"privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4v2s9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc421019080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"gcr.io/google_containers/nginx-slim:0.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4v2s9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc420bf1560), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc4201b5058), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"role":"app"}, ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ci-prtest-cc63063-94-ig-n-sttk", HostNetwork:false, HostPID:false, HostIPC:false, SecurityContext:(*v1.PodSecurityContext)(0xc421019100), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-n2tc8"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645039277, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645039279, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645039277, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.142.0.2", PodIP:"172.16.4.15", StartTime:(*v1.Time)(0xc42114c9c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc42114ca00), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"gcr.io/google_containers/nginx-slim:0.7", ImageID:"docker-pullable://gcr.io/google_containers/nginx-slim@sha256:dd4efd4c13bec2c6f3fe855deeab9524efe434505568421d4f31820485b3a795", ContainerID:"docker://c17ee8d2d82ab01461ded561f84bad350766b00c5823734839e55e847e878156"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:34:51.339: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vvphf" for this suite.
Oct 31 09:34:58.696: INFO: namespace: e2e-tests-pods-vvphf, resource: bindings, ignored listing per whitelist
Oct 31 09:34:58.696: INFO: namespace e2e-tests-pods-vvphf deletion completed in 7.341376225s

• [SLOW TEST:21.436 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:266
------------------------------
S
------------------------------
[k8s.io] ReplicaSet 
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/replica_set.go:82
[BeforeEach] [k8s.io] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:34:58.696: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/replica_set.go:82
Oct 31 09:34:58.785: INFO: Creating ReplicaSet my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be
Oct 31 09:34:58.819: INFO: Pod name my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be: Found 1 pods out of 1
Oct 31 09:34:58.819: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be" is running
Oct 31 09:35:00.850: INFO: Pod "my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be-48h8p" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2017-10-31 09:34:58 +0000 UTC Reason: Message:}])
Oct 31 09:35:00.850: INFO: Trying to dial the pod
Oct 31 09:35:05.918: INFO: Controller my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be: Got expected result from replica 1 [my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be-48h8p]: "my-hostname-basic-c2fea061-be1e-11e7-9459-0e11a30959be-48h8p", 1 of 1 required successes so far
[AfterEach] [k8s.io] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:05.918: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-f9gzz" for this suite.
Oct 31 09:35:12.834: INFO: namespace: e2e-tests-replicaset-f9gzz, resource: bindings, ignored listing per whitelist
Oct 31 09:35:13.264: INFO: namespace e2e-tests-replicaset-f9gzz deletion completed in 7.330013423s

• [SLOW TEST:14.568 seconds]
[k8s.io] ReplicaSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/replica_set.go:82
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:356
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:35:13.264: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:128
[It] should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:356
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 31 09:35:15.980: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cbaff4e5-be1e-11e7-9459-0e11a30959be"
Oct 31 09:35:15.980: INFO: Waiting up to 5m0s for pod pod-update-activedeadlineseconds-cbaff4e5-be1e-11e7-9459-0e11a30959be status to be terminated due to deadline exceeded
Oct 31 09:35:15.996: INFO: Waiting for pod pod-update-activedeadlineseconds-cbaff4e5-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-pods-2nh46' status to be 'terminated due to deadline exceeded'(found phase: "Running", readiness: true) (15.164178ms elapsed)
Oct 31 09:35:18.011: INFO: Waiting for pod pod-update-activedeadlineseconds-cbaff4e5-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-pods-2nh46' status to be 'terminated due to deadline exceeded'(found phase: "Running", readiness: true) (2.031103478s elapsed)
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:20.027: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2nh46" for this suite.
Oct 31 09:35:26.900: INFO: namespace: e2e-tests-pods-2nh46, resource: bindings, ignored listing per whitelist
Oct 31 09:35:27.382: INFO: namespace e2e-tests-pods-2nh46 deletion completed in 7.339560764s

• [SLOW TEST:14.118 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:356
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:45
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:35:27.382: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:45
STEP: Creating projection with secret that has name projected-secret-test-d41aba14-be1e-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:35:27.525: INFO: Waiting up to 5m0s for pod pod-projected-secrets-d41d1dd5-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:35:27.541: INFO: Waiting for pod pod-projected-secrets-d41d1dd5-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-n96c4' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.285663ms elapsed)
STEP: Saw pod success
Oct 31 09:35:29.572: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-projected-secrets-d41d1dd5-be1e-11e7-9459-0e11a30959be container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:35:29.615: INFO: Waiting for pod pod-projected-secrets-d41d1dd5-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:35:29.630: INFO: Pod pod-projected-secrets-d41d1dd5-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:29.630: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n96c4" for this suite.
Oct 31 09:35:36.905: INFO: namespace: e2e-tests-projected-n96c4, resource: bindings, ignored listing per whitelist
Oct 31 09:35:36.978: INFO: namespace e2e-tests-projected-n96c4 deletion completed in 7.32926789s

• [SLOW TEST:9.596 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:45
------------------------------
S
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1318
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:35:36.978: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1295
[It] should create a job from an image when restart is OnFailure [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1318
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:35:37.091: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=gcr.io/google_containers/nginx-slim:0.7 --namespace=e2e-tests-kubectl-rwcrb'
Oct 31 09:35:37.836: INFO: stderr: ""
Oct 31 09:35:37.836: INFO: stdout: "job \"e2e-test-nginx-job\" created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1299
Oct 31 09:35:37.854: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-rwcrb'
Oct 31 09:35:40.165: INFO: stderr: ""
Oct 31 09:35:40.165: INFO: stdout: "job \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:40.165: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rwcrb" for this suite.
Oct 31 09:35:47.419: INFO: namespace: e2e-tests-kubectl-rwcrb, resource: bindings, ignored listing per whitelist
Oct 31 09:35:47.527: INFO: namespace e2e-tests-kubectl-rwcrb deletion completed in 7.345745667s

• [SLOW TEST:10.549 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create a job from an image when restart is OnFailure [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1318
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:52
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:35:47.527: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:52
STEP: Creating projection with secret that has name projected-secret-test-e01a3531-be1e-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:35:47.655: INFO: Waiting up to 5m0s for pod pod-projected-secrets-e01cc5f1-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:35:47.671: INFO: Waiting for pod pod-projected-secrets-e01cc5f1-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-5f6sx' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.212205ms elapsed)
STEP: Saw pod success
Oct 31 09:35:49.701: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-secrets-e01cc5f1-be1e-11e7-9459-0e11a30959be container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:35:49.739: INFO: Waiting for pod pod-projected-secrets-e01cc5f1-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:35:49.754: INFO: Pod pod-projected-secrets-e01cc5f1-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:49.754: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5f6sx" for this suite.
Oct 31 09:35:56.594: INFO: namespace: e2e-tests-projected-5f6sx, resource: bindings, ignored listing per whitelist
Oct 31 09:35:57.104: INFO: namespace e2e-tests-projected-5f6sx deletion completed in 7.334590158s

• [SLOW TEST:9.577 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:52
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:44
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:35:57.104: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:44
STEP: Creating a pod to test override arguments
Oct 31 09:35:57.211: INFO: Waiting up to 5m0s for pod client-containers-e5cf09c2-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:35:57.226: INFO: Waiting for pod client-containers-e5cf09c2-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-containers-6lvnd' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.907455ms elapsed)
STEP: Saw pod success
Oct 31 09:35:59.257: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod client-containers-e5cf09c2-be1e-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:35:59.295: INFO: Waiting for pod client-containers-e5cf09c2-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:35:59.310: INFO: Pod client-containers-e5cf09c2-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:35:59.310: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-6lvnd" for this suite.
Oct 31 09:36:06.073: INFO: namespace: e2e-tests-containers-6lvnd, resource: bindings, ignored listing per whitelist
Oct 31 09:36:06.657: INFO: namespace e2e-tests-containers-6lvnd deletion completed in 7.331016144s

• [SLOW TEST:9.552 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:44
------------------------------
S
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:52
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:36:06.657: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:52
STEP: Creating configMap with name configmap-test-volume-eb890a16-be1e-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:36:06.835: INFO: Waiting up to 5m0s for pod pod-configmaps-eb8b7c47-be1e-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:36:06.850: INFO: Waiting for pod pod-configmaps-eb8b7c47-be1e-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-d7hfh' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.867648ms elapsed)
STEP: Saw pod success
Oct 31 09:36:08.881: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-configmaps-eb8b7c47-be1e-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:36:08.920: INFO: Waiting for pod pod-configmaps-eb8b7c47-be1e-11e7-9459-0e11a30959be to disappear
Oct 31 09:36:08.935: INFO: Pod pod-configmaps-eb8b7c47-be1e-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:36:08.935: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-d7hfh" for this suite.
Oct 31 09:36:16.036: INFO: namespace: e2e-tests-configmap-d7hfh, resource: bindings, ignored listing per whitelist
Oct 31 09:36:16.274: INFO: namespace e2e-tests-configmap-d7hfh deletion completed in 7.322668197s

• [SLOW TEST:9.617 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:52
------------------------------
SS
------------------------------
[k8s.io] Projected 
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:710
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:36:16.274: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:710
STEP: Creating configMap with name cm-test-opt-del-f14439ab-be1e-11e7-9459-0e11a30959be
STEP: Creating configMap with name cm-test-opt-upd-f1443a0a-be1e-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f14439ab-be1e-11e7-9459-0e11a30959be
STEP: Updating configmap cm-test-opt-upd-f1443a0a-be1e-11e7-9459-0e11a30959be
STEP: Creating configMap with name cm-test-opt-create-f1443a33-be1e-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:37:37.471: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rws6g" for this suite.
Oct 31 09:38:00.516: INFO: namespace: e2e-tests-projected-rws6g, resource: bindings, ignored listing per whitelist
Oct 31 09:38:00.815: INFO: namespace e2e-tests-projected-rws6g deletion completed in 23.328607221s

• [SLOW TEST:104.541 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:710
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:442
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:38:00.815: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:128
[It] should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:442
Oct 31 09:38:03.019: INFO: Waiting up to 5m0s for pod client-envvars-30cba179-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:38:03.043: INFO: Waiting for pod client-envvars-30cba179-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-pods-k7pwl' status to be 'success or failure'(found phase: "Pending", readiness: false) (23.867409ms elapsed)
STEP: Saw pod success
Oct 31 09:38:05.075: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod client-envvars-30cba179-be1f-11e7-9459-0e11a30959be container env3cont: <nil>
STEP: delete the pod
Oct 31 09:38:05.123: INFO: Waiting for pod client-envvars-30cba179-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:38:05.139: INFO: Pod client-envvars-30cba179-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:38:05.139: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-k7pwl" for this suite.
Oct 31 09:38:27.794: INFO: namespace: e2e-tests-pods-k7pwl, resource: bindings, ignored listing per whitelist
Oct 31 09:38:28.493: INFO: namespace e2e-tests-pods-k7pwl deletion completed in 23.324922922s

• [SLOW TEST:27.678 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:442
------------------------------
SSSSSSS
------------------------------
[k8s.io] ConfigMap 
  should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:381
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:38:28.493: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:381
STEP: Creating configMap e2e-tests-configmap-srng9/configmap-test-400e0729-be1f-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:38:28.637: INFO: Waiting up to 5m0s for pod pod-configmaps-401089b2-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:38:28.653: INFO: Waiting for pod pod-configmaps-401089b2-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-srng9' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.179786ms elapsed)
STEP: Saw pod success
Oct 31 09:38:30.683: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-configmaps-401089b2-be1f-11e7-9459-0e11a30959be container env-test: <nil>
STEP: delete the pod
Oct 31 09:38:30.720: INFO: Waiting for pod pod-configmaps-401089b2-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:38:30.735: INFO: Pod pod-configmaps-401089b2-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:38:30.735: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-srng9" for this suite.
Oct 31 09:38:37.778: INFO: namespace: e2e-tests-configmap-srng9, resource: bindings, ignored listing per whitelist
Oct 31 09:38:38.075: INFO: namespace e2e-tests-configmap-srng9 deletion completed in 7.312037273s

• [SLOW TEST:9.583 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:381
------------------------------
SS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:958
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:38:38.076: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should create services for rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:958
STEP: creating Redis RC
Oct 31 09:38:38.168: INFO: namespace e2e-tests-kubectl-mgg6n
Oct 31 09:38:38.168: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-mgg6n'
Oct 31 09:38:38.474: INFO: stderr: ""
Oct 31 09:38:38.474: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Oct 31 09:38:39.490: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:38:39.490: INFO: Found 0 / 1
Oct 31 09:38:40.490: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:38:40.490: INFO: Found 1 / 1
Oct 31 09:38:40.490: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 31 09:38:40.506: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 09:38:40.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 31 09:38:40.506: INFO: wait on redis-master startup in e2e-tests-kubectl-mgg6n 
Oct 31 09:38:40.506: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-x5v62 redis-master --namespace=e2e-tests-kubectl-mgg6n'
Oct 31 09:38:40.710: INFO: stderr: ""
Oct 31 09:38:40.710: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.5 (c72176ef/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n1:M 31 Oct 09:38:39.902 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Oct 09:38:39.902 # Server started, Redis version 3.2.5\n1:M 31 Oct 09:38:39.902 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Oct 09:38:39.902 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Oct 31 09:38:40.710: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-mgg6n'
Oct 31 09:38:40.928: INFO: stderr: ""
Oct 31 09:38:40.928: INFO: stdout: "service \"rm2\" exposed\n"
Oct 31 09:38:40.951: INFO: Service rm2 in namespace e2e-tests-kubectl-mgg6n found.
STEP: exposing service
Oct 31 09:38:42.982: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-mgg6n'
Oct 31 09:38:43.193: INFO: stderr: ""
Oct 31 09:38:43.193: INFO: stdout: "service \"rm3\" exposed\n"
Oct 31 09:38:43.218: INFO: Service rm3 in namespace e2e-tests-kubectl-mgg6n found.
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:38:45.249: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mgg6n" for this suite.
Oct 31 09:39:08.097: INFO: namespace: e2e-tests-kubectl-mgg6n, resource: bindings, ignored listing per whitelist
Oct 31 09:39:08.598: INFO: namespace e2e-tests-kubectl-mgg6n deletion completed in 23.320562175s

• [SLOW TEST:30.522 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl expose
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create services for rc [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:958
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:39:08.598: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
STEP: Creating configMap with name projected-configmap-test-volume-map-57f85f44-be1f-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:39:08.759: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-57fab823-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:39:08.774: INFO: Waiting for pod pod-projected-configmaps-57fab823-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-2p8wk' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.790474ms elapsed)
STEP: Saw pod success
Oct 31 09:39:10.805: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-projected-configmaps-57fab823-be1f-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:39:10.843: INFO: Waiting for pod pod-projected-configmaps-57fab823-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:39:10.858: INFO: Pod pod-projected-configmaps-57fab823-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:39:10.858: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2p8wk" for this suite.
Oct 31 09:39:17.707: INFO: namespace: e2e-tests-projected-2p8wk, resource: bindings, ignored listing per whitelist
Oct 31 09:39:18.211: INFO: namespace e2e-tests-projected-2p8wk deletion completed in 7.324205846s

• [SLOW TEST:9.613 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
------------------------------
S
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:43
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:39:18.211: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:43
STEP: Creating configMap with name configmap-test-volume-5db21360-be1f-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:39:18.365: INFO: Waiting up to 5m0s for pod pod-configmaps-5db4a5b9-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:39:18.380: INFO: Waiting for pod pod-configmaps-5db4a5b9-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-6kw7n' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.934974ms elapsed)
STEP: Saw pod success
Oct 31 09:39:20.411: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-configmaps-5db4a5b9-be1f-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:39:20.449: INFO: Waiting for pod pod-configmaps-5db4a5b9-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:39:20.464: INFO: Pod pod-configmaps-5db4a5b9-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:39:20.464: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6kw7n" for this suite.
Oct 31 09:39:27.443: INFO: namespace: e2e-tests-configmap-6kw7n, resource: bindings, ignored listing per whitelist
Oct 31 09:39:27.817: INFO: namespace e2e-tests-configmap-6kw7n deletion completed in 7.323626346s

• [SLOW TEST:9.605 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with defaultMode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:43
------------------------------
SSS
------------------------------
[k8s.io] Projected 
  should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:812
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:39:27.817: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:812
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:39:27.965: INFO: Waiting up to 5m0s for pod downwardapi-volume-636da2d9-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:39:27.980: INFO: Waiting for pod downwardapi-volume-636da2d9-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-ttgjz' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.917239ms elapsed)
STEP: Saw pod success
Oct 31 09:39:30.011: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downwardapi-volume-636da2d9-be1f-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:39:30.050: INFO: Waiting for pod downwardapi-volume-636da2d9-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:39:30.065: INFO: Pod downwardapi-volume-636da2d9-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:39:30.065: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ttgjz" for this suite.
Oct 31 09:39:36.943: INFO: namespace: e2e-tests-projected-ttgjz, resource: bindings, ignored listing per whitelist
Oct 31 09:39:37.416: INFO: namespace e2e-tests-projected-ttgjz deletion completed in 7.322321611s

• [SLOW TEST:9.599 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide podname only [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:812
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:140
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:39:37.416: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:140
STEP: Creating a pod to test downward api env vars
Oct 31 09:39:37.539: INFO: Waiting up to 5m0s for pod downward-api-69223a6b-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:39:37.554: INFO: Waiting for pod downward-api-69223a6b-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-cm8s9' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.502751ms elapsed)
STEP: Saw pod success
Oct 31 09:39:39.585: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downward-api-69223a6b-be1f-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:39:39.638: INFO: Waiting for pod downward-api-69223a6b-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:39:39.653: INFO: Pod downward-api-69223a6b-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:39:39.654: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cm8s9" for this suite.
Oct 31 09:39:46.835: INFO: namespace: e2e-tests-downward-api-cm8s9, resource: bindings, ignored listing per whitelist
Oct 31 09:39:46.998: INFO: namespace e2e-tests-downward-api-cm8s9 deletion completed in 7.315577869s

• [SLOW TEST:9.582 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:140
------------------------------
SSSSSSS
------------------------------
[k8s.io] DNS 
  should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:389
[BeforeEach] [k8s.io] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:39:46.998: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:389
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2hn9t;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2hn9t;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2hn9t.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 96.19.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.19.96_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 96.19.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.19.96_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2hn9t;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2hn9t;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2hn9t.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-2hn9t.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2hn9t.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 96.19.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.19.96_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 96.19.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.19.96_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 31 09:40:05.682: INFO: DNS probes using dns-test-6edd2791-be1f-11e7-9459-0e11a30959be succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [k8s.io] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:40:05.780: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-2hn9t" for this suite.
Oct 31 09:40:12.876: INFO: namespace: e2e-tests-dns-2hn9t, resource: bindings, ignored listing per whitelist
Oct 31 09:40:13.129: INFO: namespace e2e-tests-dns-2hn9t deletion completed in 7.331250314s

• [SLOW TEST:26.131 seconds]
[k8s.io] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/dns.go:389
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:118
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:40:13.129: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:118
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 31 09:40:13.256: INFO: Waiting up to 5m0s for pod pod-7e6c529d-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:40:13.271: INFO: Waiting for pod pod-7e6c529d-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-s29gk' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.96735ms elapsed)
STEP: Saw pod success
Oct 31 09:40:15.302: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-7e6c529d-be1f-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:40:15.339: INFO: Waiting for pod pod-7e6c529d-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:40:15.354: INFO: Pod pod-7e6c529d-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:40:15.354: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s29gk" for this suite.
Oct 31 09:40:22.113: INFO: namespace: e2e-tests-emptydir-s29gk, resource: bindings, ignored listing per whitelist
Oct 31 09:40:22.713: INFO: namespace e2e-tests-emptydir-s29gk deletion completed in 7.330140693s

• [SLOW TEST:9.584 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:118
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:40:22.713: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
STEP: Creating configMap with name projected-configmap-test-volume-8428f68a-be1f-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:40:22.904: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-842c03a0-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:40:22.918: INFO: Waiting for pod pod-projected-configmaps-842c03a0-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-c5krk' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.397452ms elapsed)
STEP: Saw pod success
Oct 31 09:40:24.949: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-projected-configmaps-842c03a0-be1f-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:40:24.988: INFO: Waiting for pod pod-projected-configmaps-842c03a0-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:40:25.003: INFO: Pod pod-projected-configmaps-842c03a0-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:40:25.003: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c5krk" for this suite.
Oct 31 09:40:32.261: INFO: namespace: e2e-tests-projected-c5krk, resource: bindings, ignored listing per whitelist
Oct 31 09:40:32.351: INFO: namespace e2e-tests-projected-c5krk deletion completed in 7.319069612s

• [SLOW TEST:9.638 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Projected 
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:382
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:40:32.351: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:382
Oct 31 09:40:32.472: INFO: Couldn't get ttl annotation from: v1.Node{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ci-prtest-cc63063-94-ig-m-vc0v", GenerateName:"", Namespace:"", SelfLink:"/api/v1/nodes/ci-prtest-cc63063-94-ig-m-vc0v", UID:"06ac70b9-be1b-11e7-99d7-42010a8e0005", ResourceVersion:"8653", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"beta.kubernetes.io/os":"linux", "failure-domain.beta.kubernetes.io/region":"us-east1", "failure-domain.beta.kubernetes.io/zone":"us-east1-c", "kubernetes.io/hostname":"ci-prtest-cc63063-94-ig-m-vc0v", "role":"infra", "beta.kubernetes.io/arch":"amd64", "beta.kubernetes.io/instance-type":"n1-standard-2"}, Annotations:map[string]string{"volumes.kubernetes.io/controller-managed-attach-detach":"true"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.NodeSpec{PodCIDR:"", ExternalID:"4410777290361111587", ProviderID:"gce://openshift-gce-devel-ci/us-east1-c/ci-prtest-cc63063-94-ig-m-vc0v", Unschedulable:true, Taints:[]v1.Taint(nil)}, Status:v1.NodeStatus{Capacity:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:7674347520, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}, "pods":resource.Quantity{i:resource.int64Amount{value:20, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"20", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}}, Allocatable:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:7569489920, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}, "pods":resource.Quantity{i:resource.int64Amount{value:20, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"20", Format:"DecimalSI"}}, Phase:"", Conditions:[]v1.NodeCondition{v1.NodeCondition{Type:"NetworkUnavailable", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"RouteCreated", Message:"openshift-sdn cleared kubelet-set NoRouteCreated"}, v1.NodeCondition{Type:"OutOfDisk", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645039623, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasSufficientDisk", Message:"kubelet has sufficient disk space available"}, v1.NodeCondition{Type:"MemoryPressure", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645039623, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasSufficientMemory", Message:"kubelet has sufficient memory available"}, v1.NodeCondition{Type:"DiskPressure", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645039623, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasNoDiskPressure", Message:"kubelet has no disk pressure"}, v1.NodeCondition{Type:"Ready", Status:"True", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645039623, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037741, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletReady", Message:"kubelet is posting ready status"}}, Addresses:[]v1.NodeAddress{v1.NodeAddress{Type:"InternalIP", Address:"10.142.0.5"}, v1.NodeAddress{Type:"ExternalIP", Address:"35.185.59.209"}, v1.NodeAddress{Type:"Hostname", Address:"ci-prtest-cc63063-94-ig-m-vc0v"}}, DaemonEndpoints:v1.NodeDaemonEndpoints{KubeletEndpoint:v1.DaemonEndpoint{Port:10250}}, NodeInfo:v1.NodeSystemInfo{MachineID:"2a7732260ac3433670dca5264dac2c09", SystemUUID:"8FB459F4-8877-60F0-DD81-8F83C83F05BE", BootID:"a3687b11-ff57-477d-a3f8-772040e7942b", KernelVersion:"3.10.0-693.2.2.el7.x86_64", OSImage:"CentOS Linux 7 (Core)", ContainerRuntimeVersion:"docker://1.12.6", KubeletVersion:"v1.7.6+a08f5eeb62", KubeProxyVersion:"v1.7.6+a08f5eeb62", OperatingSystem:"linux", Architecture:"amd64"}, Images:[]v1.ContainerImage{v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-haproxy-router@sha256:c73bb0af60175b3254b34a0ad37f0233b3671629e0277ee877deda5683d6316a", "docker.io/openshift/origin-haproxy-router:v3.7.0-rc.0"}, SizeBytes:1120763263}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-deployer@sha256:ba3bd8f55417936ac77edb05d2b15654aca1a7365b9af445797aa1f46ca0f116", "docker.io/openshift/origin-deployer:v3.7.0-rc.0"}, SizeBytes:1098621753}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-docker-registry@sha256:0283c6fa2a32ffdae4f3acf3519352929498d21643ee38c1824c8f9fef5c7bf5", "docker.io/openshift/origin-docker-registry:v3.7.0-rc.0"}, SizeBytes:508629603}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-pod@sha256:616b850e04084409d23d61a33fd367af7fae189b4ce45c037d3628b13197a524", "docker.io/openshift/origin-pod:v3.7.0-rc.0"}, SizeBytes:218419873}}, VolumesInUse:[]v1.UniqueVolumeName(nil), VolumesAttached:[]v1.AttachedVolume(nil)}}
STEP: Creating secret with name s-test-opt-del-89e33e27-be1f-11e7-9459-0e11a30959be
STEP: Creating secret with name s-test-opt-upd-89e33e69-be1f-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-89e33e27-be1f-11e7-9459-0e11a30959be
STEP: Updating secret s-test-opt-upd-89e33e69-be1f-11e7-9459-0e11a30959be
STEP: Creating secret with name s-test-opt-create-89e33e85-be1f-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:40:36.755: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-npxvx" for this suite.
Oct 31 09:40:59.659: INFO: namespace: e2e-tests-projected-npxvx, resource: bindings, ignored listing per whitelist
Oct 31 09:41:00.114: INFO: namespace e2e-tests-projected-npxvx deletion completed in 23.329908015s

• [SLOW TEST:27.763 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:382
------------------------------
SSSS
------------------------------
[k8s.io] Networking [k8s.io] Granular Checks: Pods 
  should function for node-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:59
[BeforeEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:41:00.114: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:59
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-bjzl9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 31 09:41:00.229: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Oct 31 09:41:22.566: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.4.22 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bjzl9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:41:22.566: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:41:23.856: INFO: Found all expected endpoints: [netserver-0]
Oct 31 09:41:23.871: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.2.23 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bjzl9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:41:23.871: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:41:25.059: INFO: Found all expected endpoints: [netserver-1]
Oct 31 09:41:25.074: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.6.27 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bjzl9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:41:25.074: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:41:26.259: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:41:26.259: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-bjzl9" for this suite.
Oct 31 09:41:49.082: INFO: namespace: e2e-tests-pod-network-test-bjzl9, resource: bindings, ignored listing per whitelist
Oct 31 09:41:49.624: INFO: namespace e2e-tests-pod-network-test-bjzl9 deletion completed in 23.335879378s

• [SLOW TEST:49.510 seconds]
[k8s.io] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should function for node-pod communication: udp [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:59
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:40
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:41:49.624: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:40
STEP: Creating projection with secret that has name projected-secret-test-b7eaf9b7-be1f-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:41:49.730: INFO: Waiting up to 5m0s for pod pod-projected-secrets-b7ed4fd4-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:41:49.745: INFO: Waiting for pod pod-projected-secrets-b7ed4fd4-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-n9wnv' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.488474ms elapsed)
STEP: Saw pod success
Oct 31 09:41:51.776: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-secrets-b7ed4fd4-be1f-11e7-9459-0e11a30959be container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:41:51.814: INFO: Waiting for pod pod-projected-secrets-b7ed4fd4-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:41:51.830: INFO: Pod pod-projected-secrets-b7ed4fd4-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:41:51.830: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n9wnv" for this suite.
Oct 31 09:41:58.655: INFO: namespace: e2e-tests-projected-n9wnv, resource: bindings, ignored listing per whitelist
Oct 31 09:41:59.182: INFO: namespace e2e-tests-projected-n9wnv deletion completed in 7.323753506s

• [SLOW TEST:9.558 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:40
------------------------------
SS
------------------------------
[k8s.io] Projected 
  updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:509
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:41:59.182: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:509
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bda56d47-be1f-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-bda56d47-be1f-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:43:16.164: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ngg82" for this suite.
Oct 31 09:43:39.390: INFO: namespace: e2e-tests-projected-ngg82, resource: bindings, ignored listing per whitelist
Oct 31 09:43:39.511: INFO: namespace e2e-tests-projected-ngg82 deletion completed in 23.318156296s

• [SLOW TEST:100.329 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:509
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Projected 
  should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:944
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:43:39.511: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:944
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:43:39.605: INFO: Waiting up to 5m0s for pod downwardapi-volume-f96a8eb0-be1f-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:43:39.620: INFO: Waiting for pod downwardapi-volume-f96a8eb0-be1f-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-hp987' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.573042ms elapsed)
STEP: Saw pod success
Oct 31 09:43:41.650: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-f96a8eb0-be1f-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:43:41.702: INFO: Waiting for pod downwardapi-volume-f96a8eb0-be1f-11e7-9459-0e11a30959be to disappear
Oct 31 09:43:41.717: INFO: Pod downwardapi-volume-f96a8eb0-be1f-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:43:41.717: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hp987" for this suite.
Oct 31 09:43:48.872: INFO: namespace: e2e-tests-projected-hp987, resource: bindings, ignored listing per whitelist
Oct 31 09:43:49.065: INFO: namespace e2e-tests-projected-hp987 deletion completed in 7.319172919s

• [SLOW TEST:9.554 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:944
------------------------------
SS
------------------------------
[k8s.io] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0 [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1449
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:43:49.065: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should support proxy with --port 0 [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1449
STEP: starting the proxy server
Oct 31 09:43:49.191: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0'
STEP: curling proxy /api/ output
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:43:49.387: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ztp5g" for this suite.
Oct 31 09:43:56.434: INFO: namespace: e2e-tests-kubectl-ztp5g, resource: bindings, ignored listing per whitelist
Oct 31 09:43:56.729: INFO: namespace e2e-tests-kubectl-ztp5g deletion completed in 7.313071127s

• [SLOW TEST:7.664 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should support proxy with --port 0 [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1449
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:418
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:43:56.729: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:418
STEP: Creating configMap with name projected-configmap-test-volume-map-03b16d5d-be20-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 09:43:56.862: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-03b3e9b3-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:43:56.879: INFO: Waiting for pod pod-projected-configmaps-03b3e9b3-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-xxzfr' status to be 'success or failure'(found phase: "Pending", readiness: false) (17.766946ms elapsed)
STEP: Saw pod success
Oct 31 09:43:58.910: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-configmaps-03b3e9b3-be20-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 09:43:58.948: INFO: Waiting for pod pod-projected-configmaps-03b3e9b3-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:43:58.963: INFO: Pod pod-projected-configmaps-03b3e9b3-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:43:58.963: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xxzfr" for this suite.
Oct 31 09:44:06.131: INFO: namespace: e2e-tests-projected-xxzfr, resource: bindings, ignored listing per whitelist
Oct 31 09:44:06.309: INFO: namespace e2e-tests-projected-xxzfr deletion completed in 7.317621874s

• [SLOW TEST:9.580 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:418
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:96
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:44:06.309: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:96
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:45:06.430: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-g4sdp" for this suite.
Oct 31 09:45:29.068: INFO: namespace: e2e-tests-container-probe-g4sdp, resource: bindings, ignored listing per whitelist
Oct 31 09:45:29.778: INFO: namespace e2e-tests-container-probe-g4sdp deletion completed in 23.319933495s

• [SLOW TEST:83.469 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:96
------------------------------
[k8s.io] Downward API volume 
  should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:181
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:45:29.778: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:181
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:45:29.901: INFO: Waiting up to 5m0s for pod downwardapi-volume-3b28b379-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:45:29.916: INFO: Waiting for pod downwardapi-volume-3b28b379-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-5qnwn' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.980541ms elapsed)
STEP: Saw pod success
Oct 31 09:45:31.947: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-3b28b379-be20-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:45:31.988: INFO: Waiting for pod downwardapi-volume-3b28b379-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:45:32.003: INFO: Pod downwardapi-volume-3b28b379-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:45:32.003: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5qnwn" for this suite.
Oct 31 09:45:39.272: INFO: namespace: e2e-tests-downward-api-5qnwn, resource: bindings, ignored listing per whitelist
Oct 31 09:45:39.362: INFO: namespace e2e-tests-downward-api-5qnwn deletion completed in 7.330012875s

• [SLOW TEST:9.583 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's cpu request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:181
------------------------------
SSS
------------------------------
[k8s.io] Proxy version v1 
  should proxy logs on node with explicit kubelet port [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:62
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:45:39.362: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:62
Oct 31 09:45:39.487: INFO: (0) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 20.088911ms)
Oct 31 09:45:39.503: INFO: (1) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.75636ms)
Oct 31 09:45:39.519: INFO: (2) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.944038ms)
Oct 31 09:45:39.535: INFO: (3) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.989371ms)
Oct 31 09:45:39.551: INFO: (4) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.906164ms)
Oct 31 09:45:39.567: INFO: (5) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.15547ms)
Oct 31 09:45:39.583: INFO: (6) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.088121ms)
Oct 31 09:45:39.599: INFO: (7) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.260709ms)
Oct 31 09:45:39.615: INFO: (8) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.873256ms)
Oct 31 09:45:39.632: INFO: (9) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.82451ms)
Oct 31 09:45:39.648: INFO: (10) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.895351ms)
Oct 31 09:45:39.664: INFO: (11) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.909ms)
Oct 31 09:45:39.681: INFO: (12) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.159367ms)
Oct 31 09:45:39.697: INFO: (13) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.411834ms)
Oct 31 09:45:39.713: INFO: (14) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.032344ms)
Oct 31 09:45:39.730: INFO: (15) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.135108ms)
Oct 31 09:45:39.746: INFO: (16) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.177053ms)
Oct 31 09:45:39.762: INFO: (17) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.033698ms)
Oct 31 09:45:39.779: INFO: (18) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.793792ms)
Oct 31 09:45:39.795: INFO: (19) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.167281ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:45:39.795: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-9729t" for this suite.
Oct 31 09:45:46.553: INFO: namespace: e2e-tests-proxy-9729t, resource: bindings, ignored listing per whitelist
Oct 31 09:45:47.129: INFO: namespace e2e-tests-proxy-9729t deletion completed in 7.31816845s

• [SLOW TEST:7.767 seconds]
[k8s.io] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:275
    should proxy logs on node with explicit kubelet port [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:62
------------------------------
SSSS
------------------------------
[k8s.io] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:66
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:45:47.129: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:66
Oct 31 09:45:47.262: INFO: (0) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.99172ms)
Oct 31 09:45:47.278: INFO: (1) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.243415ms)
Oct 31 09:45:47.294: INFO: (2) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.956132ms)
Oct 31 09:45:47.310: INFO: (3) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.905545ms)
Oct 31 09:45:47.326: INFO: (4) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.252173ms)
Oct 31 09:45:47.342: INFO: (5) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.03305ms)
Oct 31 09:45:47.359: INFO: (6) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.018651ms)
Oct 31 09:45:47.374: INFO: (7) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.827996ms)
Oct 31 09:45:47.391: INFO: (8) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.265093ms)
Oct 31 09:45:47.407: INFO: (9) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.159389ms)
Oct 31 09:45:47.423: INFO: (10) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.747657ms)
Oct 31 09:45:47.439: INFO: (11) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.053696ms)
Oct 31 09:45:47.455: INFO: (12) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.891144ms)
Oct 31 09:45:47.471: INFO: (13) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.942981ms)
Oct 31 09:45:47.487: INFO: (14) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.973215ms)
Oct 31 09:45:47.503: INFO: (15) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.020536ms)
Oct 31 09:45:47.519: INFO: (16) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.905666ms)
Oct 31 09:45:47.535: INFO: (17) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.17516ms)
Oct 31 09:45:47.551: INFO: (18) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.045888ms)
Oct 31 09:45:47.575: INFO: (19) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 23.991426ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:45:47.575: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-rl997" for this suite.
Oct 31 09:45:54.659: INFO: namespace: e2e-tests-proxy-rl997, resource: bindings, ignored listing per whitelist
Oct 31 09:45:54.924: INFO: namespace e2e-tests-proxy-rl997 deletion completed in 7.333171229s

• [SLOW TEST:7.794 seconds]
[k8s.io] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:275
    should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:66
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:879
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:45:54.924: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should check if kubectl describe prints relevant information for rc and pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:879
Oct 31 09:45:55.071: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version --client'
Oct 31 09:45:55.179: INFO: stderr: ""
Oct 31 09:45:55.179: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"7+\", GitVersion:\"v1.7.10-beta.0.33+3f573516fbbf05\", GitCommit:\"3f573516fbbf05bf28a463773192773debf060a7\", GitTreeState:\"clean\", BuildDate:\"2017-10-31T09:14:09Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Oct 31 09:45:55.193: INFO: Not supported for server versions before "1.7.10-beta.0.33+3f573516fbbf05"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:45:55.194: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ft6gh" for this suite.
Oct 31 09:46:02.239: INFO: namespace: e2e-tests-kubectl-ft6gh, resource: bindings, ignored listing per whitelist
Oct 31 09:46:02.547: INFO: namespace e2e-tests-kubectl-ft6gh deletion completed in 7.324175141s

S [SKIPPING] [7.623 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl describe
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should check if kubectl describe prints relevant information for rc and pods [Conformance] [It]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:879

    Oct 31 09:45:55.193: Not supported for server versions before "1.7.10-beta.0.33+3f573516fbbf05"

    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:303
------------------------------
SSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:295
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:46:02.547: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:274
[It] should scale a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:295
STEP: creating a replication controller
Oct 31 09:46:02.646: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:03.363: INFO: stderr: ""
Oct 31 09:46:03.363: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 09:46:03.363: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:03.557: INFO: stderr: ""
Oct 31 09:46:03.557: INFO: stdout: "update-demo-nautilus-hg7jp update-demo-nautilus-sll9v "
Oct 31 09:46:03.557: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-hg7jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:03.747: INFO: stderr: ""
Oct 31 09:46:03.747: INFO: stdout: ""
Oct 31 09:46:03.747: INFO: update-demo-nautilus-hg7jp is created but not running
Oct 31 09:46:08.747: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:08.939: INFO: stderr: ""
Oct 31 09:46:08.939: INFO: stdout: "update-demo-nautilus-hg7jp update-demo-nautilus-sll9v "
Oct 31 09:46:08.939: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-hg7jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:09.128: INFO: stderr: ""
Oct 31 09:46:09.128: INFO: stdout: "true"
Oct 31 09:46:09.129: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-hg7jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:09.322: INFO: stderr: ""
Oct 31 09:46:09.322: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:09.322: INFO: validating pod update-demo-nautilus-hg7jp
Oct 31 09:46:09.355: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:09.355: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:09.355: INFO: update-demo-nautilus-hg7jp is verified up and running
Oct 31 09:46:09.355: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:09.546: INFO: stderr: ""
Oct 31 09:46:09.546: INFO: stdout: "true"
Oct 31 09:46:09.546: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:09.738: INFO: stderr: ""
Oct 31 09:46:09.738: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:09.738: INFO: validating pod update-demo-nautilus-sll9v
Oct 31 09:46:09.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:09.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:09.771: INFO: update-demo-nautilus-sll9v is verified up and running
STEP: scaling down the replication controller
Oct 31 09:46:09.771: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:10.010: INFO: stderr: ""
Oct 31 09:46:10.010: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 09:46:10.010: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:10.202: INFO: stderr: ""
Oct 31 09:46:10.202: INFO: stdout: "update-demo-nautilus-hg7jp update-demo-nautilus-sll9v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 31 09:46:15.202: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:15.394: INFO: stderr: ""
Oct 31 09:46:15.394: INFO: stdout: "update-demo-nautilus-sll9v "
Oct 31 09:46:15.394: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:15.585: INFO: stderr: ""
Oct 31 09:46:15.585: INFO: stdout: "true"
Oct 31 09:46:15.585: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:15.774: INFO: stderr: ""
Oct 31 09:46:15.774: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:15.774: INFO: validating pod update-demo-nautilus-sll9v
Oct 31 09:46:15.806: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:15.806: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:15.806: INFO: update-demo-nautilus-sll9v is verified up and running
STEP: scaling up the replication controller
Oct 31 09:46:15.806: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:16.064: INFO: stderr: ""
Oct 31 09:46:16.064: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 09:46:16.064: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:16.256: INFO: stderr: ""
Oct 31 09:46:16.256: INFO: stdout: "update-demo-nautilus-sll9v update-demo-nautilus-xxtjr "
Oct 31 09:46:16.256: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:16.446: INFO: stderr: ""
Oct 31 09:46:16.446: INFO: stdout: "true"
Oct 31 09:46:16.446: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:16.639: INFO: stderr: ""
Oct 31 09:46:16.639: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:16.639: INFO: validating pod update-demo-nautilus-sll9v
Oct 31 09:46:16.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:16.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:16.669: INFO: update-demo-nautilus-sll9v is verified up and running
Oct 31 09:46:16.669: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-xxtjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:16.859: INFO: stderr: ""
Oct 31 09:46:16.859: INFO: stdout: ""
Oct 31 09:46:16.859: INFO: update-demo-nautilus-xxtjr is created but not running
Oct 31 09:46:21.859: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:22.052: INFO: stderr: ""
Oct 31 09:46:22.052: INFO: stdout: "update-demo-nautilus-sll9v update-demo-nautilus-xxtjr "
Oct 31 09:46:22.052: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:22.241: INFO: stderr: ""
Oct 31 09:46:22.241: INFO: stdout: "true"
Oct 31 09:46:22.241: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-sll9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:22.431: INFO: stderr: ""
Oct 31 09:46:22.431: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:22.431: INFO: validating pod update-demo-nautilus-sll9v
Oct 31 09:46:22.463: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:22.463: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:22.463: INFO: update-demo-nautilus-sll9v is verified up and running
Oct 31 09:46:22.463: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-xxtjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:22.653: INFO: stderr: ""
Oct 31 09:46:22.653: INFO: stdout: "true"
Oct 31 09:46:22.653: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-xxtjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:22.843: INFO: stderr: ""
Oct 31 09:46:22.843: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:46:22.843: INFO: validating pod update-demo-nautilus-xxtjr
Oct 31 09:46:22.876: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:46:22.876: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:46:22.876: INFO: update-demo-nautilus-xxtjr is verified up and running
STEP: using delete to clean up resources
Oct 31 09:46:22.876: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:23.167: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 09:46:23.167: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Oct 31 09:46:23.167: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-s2pkq'
Oct 31 09:46:23.374: INFO: stderr: "No resources found.\n"
Oct 31 09:46:23.374: INFO: stdout: ""
Oct 31 09:46:23.374: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-s2pkq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 31 09:46:23.566: INFO: stderr: ""
Oct 31 09:46:23.566: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:46:23.566: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s2pkq" for this suite.
Oct 31 09:46:46.845: INFO: namespace: e2e-tests-kubectl-s2pkq, resource: bindings, ignored listing per whitelist
Oct 31 09:46:46.934: INFO: namespace e2e-tests-kubectl-s2pkq deletion completed in 23.339067987s

• [SLOW TEST:44.387 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should scale a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:295
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:132
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:46:46.935: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:132
STEP: Creating a pod to test substitution in container's args
Oct 31 09:46:47.058: INFO: Waiting up to 5m0s for pod var-expansion-6925d5be-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:46:47.073: INFO: Waiting for pod var-expansion-6925d5be-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-var-expansion-wv6j7' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.325989ms elapsed)
STEP: Saw pod success
Oct 31 09:46:49.105: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod var-expansion-6925d5be-be20-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:46:49.144: INFO: Waiting for pod var-expansion-6925d5be-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:46:49.159: INFO: Pod var-expansion-6925d5be-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:46:49.159: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-wv6j7" for this suite.
Oct 31 09:46:56.043: INFO: namespace: e2e-tests-var-expansion-wv6j7, resource: bindings, ignored listing per whitelist
Oct 31 09:46:56.516: INFO: namespace e2e-tests-var-expansion-wv6j7 deletion completed in 7.328886912s

• [SLOW TEST:9.582 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:132
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:110
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:46:56.516: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:110
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 31 09:46:56.628: INFO: Waiting up to 5m0s for pod pod-6eda2168-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:46:56.643: INFO: Waiting for pod pod-6eda2168-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-hr772' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.954129ms elapsed)
STEP: Saw pod success
Oct 31 09:46:58.676: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-6eda2168-be20-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:46:58.717: INFO: Waiting for pod pod-6eda2168-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:46:58.732: INFO: Pod pod-6eda2168-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:46:58.732: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hr772" for this suite.
Oct 31 09:47:06.039: INFO: namespace: e2e-tests-emptydir-hr772, resource: bindings, ignored listing per whitelist
Oct 31 09:47:06.085: INFO: namespace e2e-tests-emptydir-hr772 deletion completed in 7.324566005s

• [SLOW TEST:9.569 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:110
------------------------------
[k8s.io] Networking [k8s.io] Granular Checks: Pods 
  should function for node-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:52
[BeforeEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:47:06.085: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:52
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ndjwg
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 31 09:47:06.207: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Oct 31 09:47:28.517: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.2.29:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-ndjwg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:47:28.517: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:47:28.816: INFO: Found all expected endpoints: [netserver-0]
Oct 31 09:47:28.831: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.4.27:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-ndjwg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:47:28.831: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:47:29.016: INFO: Found all expected endpoints: [netserver-1]
Oct 31 09:47:29.031: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.6.31:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-ndjwg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:47:29.031: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:47:29.218: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:47:29.218: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ndjwg" for this suite.
Oct 31 09:47:51.978: INFO: namespace: e2e-tests-pod-network-test-ndjwg, resource: bindings, ignored listing per whitelist
Oct 31 09:47:52.561: INFO: namespace e2e-tests-pod-network-test-ndjwg deletion completed in 23.31399314s

• [SLOW TEST:46.476 seconds]
[k8s.io] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should function for node-pod communication: http [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:52
------------------------------
SS
------------------------------
[k8s.io] PreStop 
  should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:179
[BeforeEach] [k8s.io] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:47:52.561: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:179
STEP: Creating server pod server in namespace e2e-tests-prestop-vtk9m
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-vtk9m
STEP: Deleting pre-stop pod
Oct 31 09:48:03.819: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"Unable to read the endpoints for default/nettest: the server does not allow access to the requested resource (get endpoints nettest); will try again.",
		"Unable to read the endpoints for default/nettest: the server does not allow access to the requested resource (get endpoints nettest); will try again."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:48:03.837: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-vtk9m" for this suite.
Oct 31 09:48:42.850: INFO: namespace: e2e-tests-prestop-vtk9m, resource: bindings, ignored listing per whitelist
Oct 31 09:48:43.176: INFO: namespace e2e-tests-prestop-vtk9m deletion completed in 39.310956257s

• [SLOW TEST:50.615 seconds]
[k8s.io] PreStop
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:179
------------------------------
SS
------------------------------
[k8s.io] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:281
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:48:43.176: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:274
[It] should create and stop a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:281
STEP: creating a replication controller
Oct 31 09:48:43.298: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:43.596: INFO: stderr: ""
Oct 31 09:48:43.596: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 09:48:43.596: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:43.788: INFO: stderr: ""
Oct 31 09:48:43.788: INFO: stdout: "update-demo-nautilus-2l9c9 update-demo-nautilus-zqsts "
Oct 31 09:48:43.788: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-2l9c9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:43.979: INFO: stderr: ""
Oct 31 09:48:43.979: INFO: stdout: ""
Oct 31 09:48:43.979: INFO: update-demo-nautilus-2l9c9 is created but not running
Oct 31 09:48:48.979: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:49.171: INFO: stderr: ""
Oct 31 09:48:49.172: INFO: stdout: "update-demo-nautilus-2l9c9 update-demo-nautilus-zqsts "
Oct 31 09:48:49.172: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-2l9c9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:49.364: INFO: stderr: ""
Oct 31 09:48:49.364: INFO: stdout: "true"
Oct 31 09:48:49.364: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-2l9c9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:49.554: INFO: stderr: ""
Oct 31 09:48:49.554: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:48:49.554: INFO: validating pod update-demo-nautilus-2l9c9
Oct 31 09:48:49.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:48:49.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:48:49.588: INFO: update-demo-nautilus-2l9c9 is verified up and running
Oct 31 09:48:49.588: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-zqsts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:49.783: INFO: stderr: ""
Oct 31 09:48:49.784: INFO: stdout: "true"
Oct 31 09:48:49.784: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-zqsts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:49.975: INFO: stderr: ""
Oct 31 09:48:49.975: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 09:48:49.975: INFO: validating pod update-demo-nautilus-zqsts
Oct 31 09:48:50.007: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 09:48:50.007: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 09:48:50.007: INFO: update-demo-nautilus-zqsts is verified up and running
STEP: using delete to clean up resources
Oct 31 09:48:50.007: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:50.287: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 09:48:50.287: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Oct 31 09:48:50.287: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-zdpw5'
Oct 31 09:48:50.497: INFO: stderr: "No resources found.\n"
Oct 31 09:48:50.497: INFO: stdout: ""
Oct 31 09:48:50.497: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-zdpw5 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 31 09:48:50.691: INFO: stderr: ""
Oct 31 09:48:50.691: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:48:50.691: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zdpw5" for this suite.
Oct 31 09:48:57.647: INFO: namespace: e2e-tests-kubectl-zdpw5, resource: bindings, ignored listing per whitelist
Oct 31 09:48:58.043: INFO: namespace e2e-tests-kubectl-zdpw5 deletion completed in 7.322862955s

• [SLOW TEST:14.867 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create and stop a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:281
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Downward API volume 
  should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:172
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:48:58.043: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:172
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:48:58.167: INFO: Waiting up to 5m0s for pod downwardapi-volume-b74b96f6-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:48:58.182: INFO: Waiting for pod downwardapi-volume-b74b96f6-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-w4x45' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.517988ms elapsed)
STEP: Saw pod success
Oct 31 09:49:00.213: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-b74b96f6-be20-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:49:00.251: INFO: Waiting for pod downwardapi-volume-b74b96f6-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:49:00.265: INFO: Pod downwardapi-volume-b74b96f6-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:49:00.266: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w4x45" for this suite.
Oct 31 09:49:07.122: INFO: namespace: e2e-tests-downward-api-w4x45, resource: bindings, ignored listing per whitelist
Oct 31 09:49:07.611: INFO: namespace e2e-tests-downward-api-w4x45 deletion completed in 7.317156279s

• [SLOW TEST:9.568 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's memory limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:172
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:55
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:49:07.611: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:55
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 31 09:49:13.870: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-ssfb7 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:49:13.870: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:49:14.064: INFO: Exec stderr: ""
Oct 31 09:49:14.064: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-ssfb7 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:49:14.064: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:49:14.250: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 31 09:49:14.250: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-ssfb7 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:49:14.250: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:49:14.442: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 31 09:49:14.442: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-ssfb7 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:49:14.442: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:49:14.640: INFO: Exec stderr: ""
Oct 31 09:49:14.640: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-ssfb7 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 09:49:14.640: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 09:49:14.826: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:49:14.826: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-ssfb7" for this suite.
Oct 31 09:50:06.091: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-ssfb7, resource: bindings, ignored listing per whitelist
Oct 31 09:50:06.165: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-ssfb7 deletion completed in 51.31088237s

• [SLOW TEST:58.554 seconds]
[k8s.io] KubeletManagedEtcHosts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:55
------------------------------
SSS
------------------------------
[k8s.io] Secrets 
  should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:154
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:06.166: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:154
STEP: Creating secret with name secret-test-dfe4f745-be20-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:50:06.298: INFO: Waiting up to 5m0s for pod pod-secrets-dfe77561-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:50:06.313: INFO: Waiting for pod pod-secrets-dfe77561-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-qzjcq' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.043685ms elapsed)
STEP: Saw pod success
Oct 31 09:50:08.345: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-secrets-dfe77561-be20-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:50:08.383: INFO: Waiting for pod pod-secrets-dfe77561-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:50:08.398: INFO: Pod pod-secrets-dfe77561-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:08.398: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qzjcq" for this suite.
Oct 31 09:50:15.723: INFO: namespace: e2e-tests-secrets-qzjcq, resource: bindings, ignored listing per whitelist
Oct 31 09:50:15.751: INFO: namespace e2e-tests-secrets-qzjcq deletion completed in 7.324398278s

• [SLOW TEST:9.586 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable in multiple volumes in a pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:154
------------------------------
S
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:86
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:15.751: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:86
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 31 09:50:15.880: INFO: Waiting up to 5m0s for pod pod-e59d8b41-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:50:15.896: INFO: Waiting for pod pod-e59d8b41-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-wtvr5' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.053224ms elapsed)
STEP: Saw pod success
Oct 31 09:50:17.926: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-e59d8b41-be20-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:50:17.964: INFO: Waiting for pod pod-e59d8b41-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:50:17.979: INFO: Pod pod-e59d8b41-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:17.979: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wtvr5" for this suite.
Oct 31 09:50:25.272: INFO: namespace: e2e-tests-emptydir-wtvr5, resource: bindings, ignored listing per whitelist
Oct 31 09:50:25.330: INFO: namespace e2e-tests-emptydir-wtvr5 deletion completed in 7.322114348s

• [SLOW TEST:9.579 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:86
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:39
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:25.330: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:39
STEP: Creating secret with name secret-test-eb5860d9-be20-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 09:50:25.509: INFO: Waiting up to 5m0s for pod pod-secrets-eb5aca4a-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:50:25.524: INFO: Waiting for pod pod-secrets-eb5aca4a-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-mm9ds' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.915116ms elapsed)
STEP: Saw pod success
Oct 31 09:50:27.555: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-secrets-eb5aca4a-be20-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 09:50:27.593: INFO: Waiting for pod pod-secrets-eb5aca4a-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:50:27.608: INFO: Pod pod-secrets-eb5aca4a-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:27.608: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mm9ds" for this suite.
Oct 31 09:50:34.505: INFO: namespace: e2e-tests-secrets-mm9ds, resource: bindings, ignored listing per whitelist
Oct 31 09:50:34.961: INFO: namespace e2e-tests-secrets-mm9ds deletion completed in 7.323237292s

• [SLOW TEST:9.630 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:39
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:69
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:34.961: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:69
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:50:35.082: INFO: Waiting up to 5m0s for pod downwardapi-volume-f10f5449-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:50:35.097: INFO: Waiting for pod downwardapi-volume-f10f5449-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-4kvhb' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.267414ms elapsed)
STEP: Saw pod success
Oct 31 09:50:37.128: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-f10f5449-be20-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:50:37.167: INFO: Waiting for pod downwardapi-volume-f10f5449-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:50:37.183: INFO: Pod downwardapi-volume-f10f5449-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:37.183: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4kvhb" for this suite.
Oct 31 09:50:44.040: INFO: namespace: e2e-tests-downward-api-4kvhb, resource: bindings, ignored listing per whitelist
Oct 31 09:50:44.542: INFO: namespace e2e-tests-downward-api-4kvhb deletion completed in 7.329093774s

• [SLOW TEST:9.581 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should set mode on item file [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:69
------------------------------
S
------------------------------
[k8s.io] Projected 
  should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:953
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:44.542: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:953
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:50:44.648: INFO: Waiting up to 5m0s for pod downwardapi-volume-f6c35029-be20-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:50:44.665: INFO: Waiting for pod downwardapi-volume-f6c35029-be20-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-vbsk4' status to be 'success or failure'(found phase: "Pending", readiness: false) (16.475821ms elapsed)
STEP: Saw pod success
Oct 31 09:50:46.696: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-f6c35029-be20-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:50:46.735: INFO: Waiting for pod downwardapi-volume-f6c35029-be20-11e7-9459-0e11a30959be to disappear
Oct 31 09:50:46.749: INFO: Pod downwardapi-volume-f6c35029-be20-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:46.749: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vbsk4" for this suite.
Oct 31 09:50:53.881: INFO: namespace: e2e-tests-projected-vbsk4, resource: bindings, ignored listing per whitelist
Oct 31 09:50:54.150: INFO: namespace e2e-tests-projected-vbsk4 deletion completed in 7.372624803s

• [SLOW TEST:9.608 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:953
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] ServiceAccounts 
  should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:384
[BeforeEach] [k8s.io] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:50:54.150: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:384
STEP: getting the auto-created API token
Oct 31 09:50:54.896: INFO: created pod pod-service-account-defaultsa
Oct 31 09:50:54.896: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 31 09:50:54.913: INFO: created pod pod-service-account-mountsa
Oct 31 09:50:54.913: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 31 09:50:54.930: INFO: created pod pod-service-account-nomountsa
Oct 31 09:50:54.930: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 31 09:50:54.946: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 31 09:50:54.946: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 31 09:50:54.962: INFO: created pod pod-service-account-mountsa-mountspec
Oct 31 09:50:54.962: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 31 09:50:54.978: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 31 09:50:54.978: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 31 09:50:54.994: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 31 09:50:54.994: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 31 09:50:55.012: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 31 09:50:55.012: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 31 09:50:55.028: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 31 09:50:55.028: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [k8s.io] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:50:55.029: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-6n2nk" for this suite.
Oct 31 09:51:02.200: INFO: namespace: e2e-tests-svcaccounts-6n2nk, resource: bindings, ignored listing per whitelist
Oct 31 09:51:02.362: INFO: namespace e2e-tests-svcaccounts-6n2nk deletion completed in 7.304585542s

• [SLOW TEST:8.212 seconds]
[k8s.io] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:384
------------------------------
[k8s.io] EmptyDir volumes 
  volume on default medium should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:98
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:51:02.362: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:98
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 31 09:51:02.490: INFO: Waiting up to 5m0s for pod pod-0165baab-be21-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:51:02.504: INFO: Waiting for pod pod-0165baab-be21-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-jhwsk' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.248837ms elapsed)
STEP: Saw pod success
Oct 31 09:51:04.535: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-0165baab-be21-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:51:04.573: INFO: Waiting for pod pod-0165baab-be21-11e7-9459-0e11a30959be to disappear
Oct 31 09:51:04.587: INFO: Pod pod-0165baab-be21-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:51:04.587: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jhwsk" for this suite.
Oct 31 09:51:11.363: INFO: namespace: e2e-tests-emptydir-jhwsk, resource: bindings, ignored listing per whitelist
Oct 31 09:51:11.924: INFO: namespace e2e-tests-emptydir-jhwsk deletion completed in 7.308838437s

• [SLOW TEST:9.562 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  volume on default medium should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:98
------------------------------
[k8s.io] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1477
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:51:11.925: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should support --unix-socket=/path [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1477
STEP: Starting the proxy
Oct 31 09:51:12.043: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix353171525/test'
STEP: retrieving proxy /api/ output
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:51:12.155: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7xw26" for this suite.
Oct 31 09:51:18.830: INFO: namespace: e2e-tests-kubectl-7xw26, resource: bindings, ignored listing per whitelist
Oct 31 09:51:19.518: INFO: namespace e2e-tests-kubectl-7xw26 deletion completed in 7.33422059s

• [SLOW TEST:7.594 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should support --unix-socket=/path [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1477
------------------------------
[k8s.io] ServiceAccounts 
  should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:243
[BeforeEach] [k8s.io] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:51:19.519: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:243
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Oct 31 09:51:20.216: INFO: Waiting up to 5m0s for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-g4xdp status to be success or failure
Oct 31 09:51:20.231: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-g4xdp in namespace 'e2e-tests-svcaccounts-k4lj4' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.840663ms elapsed)
STEP: Saw pod success
Oct 31 09:51:22.263: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-g4xdp container token-test: <nil>
STEP: delete the pod
Oct 31 09:51:22.307: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-g4xdp to disappear
Oct 31 09:51:22.322: INFO: Pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-g4xdp no longer exists
STEP: Creating a pod to test consume service account root CA
Oct 31 09:51:22.339: INFO: Waiting up to 5m0s for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-rvjg4 status to be success or failure
Oct 31 09:51:22.355: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-rvjg4 in namespace 'e2e-tests-svcaccounts-k4lj4' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.360427ms elapsed)
STEP: Saw pod success
Oct 31 09:51:24.386: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-rvjg4 container root-ca-test: <nil>
STEP: delete the pod
Oct 31 09:51:24.424: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-rvjg4 to disappear
Oct 31 09:51:24.439: INFO: Pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-rvjg4 no longer exists
STEP: Creating a pod to test consume service account namespace
Oct 31 09:51:24.455: INFO: Waiting up to 5m0s for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-689c5 status to be success or failure
Oct 31 09:51:24.470: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-689c5 in namespace 'e2e-tests-svcaccounts-k4lj4' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.082604ms elapsed)
STEP: Saw pod success
Oct 31 09:51:26.501: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-689c5 container namespace-test: <nil>
STEP: delete the pod
Oct 31 09:51:26.540: INFO: Waiting for pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-689c5 to disappear
Oct 31 09:51:26.555: INFO: Pod pod-service-account-0bf43dc4-be21-11e7-9459-0e11a30959be-689c5 no longer exists
[AfterEach] [k8s.io] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:51:26.555: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-k4lj4" for this suite.
Oct 31 09:51:33.838: INFO: namespace: e2e-tests-svcaccounts-k4lj4, resource: bindings, ignored listing per whitelist
Oct 31 09:51:33.912: INFO: namespace e2e-tests-svcaccounts-k4lj4 deletion completed in 7.327226437s

• [SLOW TEST:14.393 seconds]
[k8s.io] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:243
------------------------------
SS
------------------------------
[k8s.io] EmptyDir volumes 
  volume on tmpfs should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:70
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:51:33.912: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:70
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 31 09:51:34.027: INFO: Waiting up to 5m0s for pod pod-14318e17-be21-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:51:34.041: INFO: Waiting for pod pod-14318e17-be21-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-8br8q' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.793277ms elapsed)
STEP: Saw pod success
Oct 31 09:51:36.072: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-14318e17-be21-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:51:36.109: INFO: Waiting for pod pod-14318e17-be21-11e7-9459-0e11a30959be to disappear
Oct 31 09:51:36.124: INFO: Pod pod-14318e17-be21-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:51:36.125: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8br8q" for this suite.
Oct 31 09:51:42.920: INFO: namespace: e2e-tests-emptydir-8br8q, resource: bindings, ignored listing per whitelist
Oct 31 09:51:43.458: INFO: namespace e2e-tests-emptydir-8br8q deletion completed in 7.304559062s

• [SLOW TEST:9.546 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  volume on tmpfs should have the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:70
------------------------------
[k8s.io] Secrets 
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:341
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:51:43.458: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:341
Oct 31 09:51:43.575: INFO: Couldn't get ttl annotation from: v1.Node{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ci-prtest-cc63063-94-ig-m-vc0v", GenerateName:"", Namespace:"", SelfLink:"/api/v1/nodes/ci-prtest-cc63063-94-ig-m-vc0v", UID:"06ac70b9-be1b-11e7-99d7-42010a8e0005", ResourceVersion:"12097", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"role":"infra", "beta.kubernetes.io/arch":"amd64", "beta.kubernetes.io/instance-type":"n1-standard-2", "beta.kubernetes.io/os":"linux", "failure-domain.beta.kubernetes.io/region":"us-east1", "failure-domain.beta.kubernetes.io/zone":"us-east1-c", "kubernetes.io/hostname":"ci-prtest-cc63063-94-ig-m-vc0v"}, Annotations:map[string]string{"volumes.kubernetes.io/controller-managed-attach-detach":"true"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.NodeSpec{PodCIDR:"", ExternalID:"4410777290361111587", ProviderID:"gce://openshift-gce-devel-ci/us-east1-c/ci-prtest-cc63063-94-ig-m-vc0v", Unschedulable:true, Taints:[]v1.Taint(nil)}, Status:v1.NodeStatus{Capacity:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:7674347520, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}, "pods":resource.Quantity{i:resource.int64Amount{value:20, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"20", Format:"DecimalSI"}}, Allocatable:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:7569489920, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}, "pods":resource.Quantity{i:resource.int64Amount{value:20, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"20", Format:"DecimalSI"}}, Phase:"", Conditions:[]v1.NodeCondition{v1.NodeCondition{Type:"NetworkUnavailable", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"RouteCreated", Message:"openshift-sdn cleared kubelet-set NoRouteCreated"}, v1.NodeCondition{Type:"OutOfDisk", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645040294, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasSufficientDisk", Message:"kubelet has sufficient disk space available"}, v1.NodeCondition{Type:"MemoryPressure", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645040294, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasSufficientMemory", Message:"kubelet has sufficient memory available"}, v1.NodeCondition{Type:"DiskPressure", Status:"False", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645040294, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037694, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletHasNoDiskPressure", Message:"kubelet has no disk pressure"}, v1.NodeCondition{Type:"Ready", Status:"True", LastHeartbeatTime:v1.Time{Time:time.Time{sec:63645040294, nsec:0, loc:(*time.Location)(0x5786480)}}, LastTransitionTime:v1.Time{Time:time.Time{sec:63645037741, nsec:0, loc:(*time.Location)(0x5786480)}}, Reason:"KubeletReady", Message:"kubelet is posting ready status"}}, Addresses:[]v1.NodeAddress{v1.NodeAddress{Type:"InternalIP", Address:"10.142.0.5"}, v1.NodeAddress{Type:"ExternalIP", Address:"35.185.59.209"}, v1.NodeAddress{Type:"Hostname", Address:"ci-prtest-cc63063-94-ig-m-vc0v"}}, DaemonEndpoints:v1.NodeDaemonEndpoints{KubeletEndpoint:v1.DaemonEndpoint{Port:10250}}, NodeInfo:v1.NodeSystemInfo{MachineID:"2a7732260ac3433670dca5264dac2c09", SystemUUID:"8FB459F4-8877-60F0-DD81-8F83C83F05BE", BootID:"a3687b11-ff57-477d-a3f8-772040e7942b", KernelVersion:"3.10.0-693.2.2.el7.x86_64", OSImage:"CentOS Linux 7 (Core)", ContainerRuntimeVersion:"docker://1.12.6", KubeletVersion:"v1.7.6+a08f5eeb62", KubeProxyVersion:"v1.7.6+a08f5eeb62", OperatingSystem:"linux", Architecture:"amd64"}, Images:[]v1.ContainerImage{v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-haproxy-router@sha256:c73bb0af60175b3254b34a0ad37f0233b3671629e0277ee877deda5683d6316a", "docker.io/openshift/origin-haproxy-router:v3.7.0-rc.0"}, SizeBytes:1120763263}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-deployer@sha256:ba3bd8f55417936ac77edb05d2b15654aca1a7365b9af445797aa1f46ca0f116", "docker.io/openshift/origin-deployer:v3.7.0-rc.0"}, SizeBytes:1098621753}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-docker-registry@sha256:0283c6fa2a32ffdae4f3acf3519352929498d21643ee38c1824c8f9fef5c7bf5", "docker.io/openshift/origin-docker-registry:v3.7.0-rc.0"}, SizeBytes:508629603}, v1.ContainerImage{Names:[]string{"docker.io/openshift/origin-pod@sha256:616b850e04084409d23d61a33fd367af7fae189b4ce45c037d3628b13197a524", "docker.io/openshift/origin-pod:v3.7.0-rc.0"}, SizeBytes:218419873}}, VolumesInUse:[]v1.UniqueVolumeName(nil), VolumesAttached:[]v1.AttachedVolume(nil)}}
STEP: Creating secret with name s-test-opt-del-19e575fb-be21-11e7-9459-0e11a30959be
STEP: Creating secret with name s-test-opt-upd-19e57652-be21-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-19e575fb-be21-11e7-9459-0e11a30959be
STEP: Updating secret s-test-opt-upd-19e57652-be21-11e7-9459-0e11a30959be
STEP: Creating secret with name s-test-opt-create-19e5767d-be21-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:52:52.469: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vqvlx" for this suite.
Oct 31 09:53:15.033: INFO: namespace: e2e-tests-secrets-vqvlx, resource: bindings, ignored listing per whitelist
Oct 31 09:53:15.814: INFO: namespace e2e-tests-secrets-vqvlx deletion completed in 23.316621587s

• [SLOW TEST:92.357 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:341
------------------------------
SSS
------------------------------
[k8s.io] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:330
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:53:15.815: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:108
Oct 31 09:53:15.913: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 31 09:54:16.043: INFO: Waiting for terminating namespaces to be deleted...
Oct 31 09:54:16.074: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 31 09:54:16.120: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 31 09:54:16.120: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 31 09:54:16.135: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Oct 31 09:54:16.135: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-2mxx before test
Oct 31 09:54:16.168: INFO: registry-console-1-rhw5g from default started at 2017-10-31 09:12:05 +0000 UTC (1 container statuses recorded)
Oct 31 09:54:16.168: INFO: 	Container registry-console ready: true, restart count 0
Oct 31 09:54:16.168: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-cfzz before test
Oct 31 09:54:16.199: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-94-ig-n-sttk before test
[It] validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:330
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-76233734-be21-11e7-9459-0e11a30959be 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-76233734-be21-11e7-9459-0e11a30959be off the node ci-prtest-cc63063-94-ig-n-sttk
STEP: verifying the node doesn't have the label kubernetes.io/e2e-76233734-be21-11e7-9459-0e11a30959be
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:54:20.488: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hnxjl" for this suite.
Oct 31 09:54:43.612: INFO: namespace: e2e-tests-sched-pred-hnxjl, resource: bindings, ignored listing per whitelist
Oct 31 09:54:43.853: INFO: namespace e2e-tests-sched-pred-hnxjl deletion completed in 23.336396404s
[AfterEach] [k8s.io] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:88.039 seconds]
[k8s.io] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:330
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:207
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:54:43.854: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:207
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-8vh2k
Oct 31 09:54:46.007: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-8vh2k
STEP: checking the pod's current state and verifying that restartCount is present
Oct 31 09:54:46.022: INFO: Initial restart count of pod liveness-http is 0
Oct 31 09:55:06.193: INFO: Restart count of pod e2e-tests-container-probe-8vh2k/liveness-http is now 1 (20.170708663s elapsed)
Oct 31 09:55:26.349: INFO: Restart count of pod e2e-tests-container-probe-8vh2k/liveness-http is now 2 (40.327189337s elapsed)
Oct 31 09:55:46.511: INFO: Restart count of pod e2e-tests-container-probe-8vh2k/liveness-http is now 3 (1m0.488914846s elapsed)
Oct 31 09:56:06.668: INFO: Restart count of pod e2e-tests-container-probe-8vh2k/liveness-http is now 4 (1m20.64598508s elapsed)
Oct 31 09:57:21.250: INFO: Restart count of pod e2e-tests-container-probe-8vh2k/liveness-http is now 5 (2m35.228380336s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:57:21.268: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8vh2k" for this suite.
Oct 31 09:57:28.437: INFO: namespace: e2e-tests-container-probe-8vh2k, resource: bindings, ignored listing per whitelist
Oct 31 09:57:28.637: INFO: namespace e2e-tests-container-probe-8vh2k deletion completed in 7.340078039s

• [SLOW TEST:164.784 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:207
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:101
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:57:28.637: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:101
STEP: Creating a pod to test substitution in container's command
Oct 31 09:57:28.776: INFO: Waiting up to 5m0s for pod var-expansion-e7a43894-be21-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:57:28.791: INFO: Waiting for pod var-expansion-e7a43894-be21-11e7-9459-0e11a30959be in namespace 'e2e-tests-var-expansion-t9s9q' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.106895ms elapsed)
STEP: Saw pod success
Oct 31 09:57:30.823: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod var-expansion-e7a43894-be21-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:57:30.863: INFO: Waiting for pod var-expansion-e7a43894-be21-11e7-9459-0e11a30959be to disappear
Oct 31 09:57:30.878: INFO: Pod var-expansion-e7a43894-be21-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:57:30.878: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-t9s9q" for this suite.
Oct 31 09:57:38.193: INFO: namespace: e2e-tests-var-expansion-t9s9q, resource: bindings, ignored listing per whitelist
Oct 31 09:57:38.238: INFO: namespace e2e-tests-var-expansion-t9s9q deletion completed in 7.331675545s

• [SLOW TEST:9.601 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:101
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Services 
  should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:71
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:57:38.239: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:52
[It] should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:71
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:57:38.322: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-ktht8" for this suite.
Oct 31 09:57:45.410: INFO: namespace: e2e-tests-services-ktht8, resource: bindings, ignored listing per whitelist
Oct 31 09:57:45.694: INFO: namespace e2e-tests-services-ktht8 deletion completed in 7.343543471s
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:64

• [SLOW TEST:7.455 seconds]
[k8s.io] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:71
------------------------------
CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/custom_resource_definition.go:58
[BeforeEach] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:57:45.694: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/custom_resource_definition.go:58
Oct 31 09:57:45.812: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
[AfterEach] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:57:46.020: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-62d7l" for this suite.
Oct 31 09:57:52.700: INFO: namespace: e2e-tests-custom-resource-definition-62d7l, resource: bindings, ignored listing per whitelist
Oct 31 09:57:53.407: INFO: namespace e2e-tests-custom-resource-definition-62d7l deletion completed in 7.358528206s

• [SLOW TEST:7.714 seconds]
CustomResourceDefinition resources
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/custom_resource_definition.go:60
  Simple CustomResourceDefinition
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/custom_resource_definition.go:59
    creating/deleting custom resource definition objects works [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/custom_resource_definition.go:58
------------------------------
SSSSS
------------------------------
[k8s.io] Projected 
  should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:888
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:57:53.408: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:888
STEP: Creating the pod
Oct 31 09:57:56.115: INFO: Successfully updated pod "labelsupdatef662746e-be21-11e7-9459-0e11a30959be"
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:58:00.173: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2hqsj" for this suite.
Oct 31 09:58:23.254: INFO: namespace: e2e-tests-projected-2hqsj, resource: bindings, ignored listing per whitelist
Oct 31 09:58:23.539: INFO: namespace e2e-tests-projected-2hqsj deletion completed in 23.337128487s

• [SLOW TEST:30.131 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:888
------------------------------
SSSSS
------------------------------
[k8s.io] Downward API 
  should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:63
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:58:23.539: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:63
STEP: Creating a pod to test downward api env vars
Oct 31 09:58:23.635: INFO: Waiting up to 5m0s for pod downward-api-08562d55-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:58:23.651: INFO: Waiting for pod downward-api-08562d55-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-z9qr5' status to be 'success or failure'(found phase: "Pending", readiness: false) (16.499352ms elapsed)
STEP: Saw pod success
Oct 31 09:58:25.683: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downward-api-08562d55-be22-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 09:58:25.723: INFO: Waiting for pod downward-api-08562d55-be22-11e7-9459-0e11a30959be to disappear
Oct 31 09:58:25.738: INFO: Pod downward-api-08562d55-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:58:25.738: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z9qr5" for this suite.
Oct 31 09:58:32.604: INFO: namespace: e2e-tests-downward-api-z9qr5, resource: bindings, ignored listing per whitelist
Oct 31 09:58:33.099: INFO: namespace e2e-tests-downward-api-z9qr5 deletion completed in 7.332124796s

• [SLOW TEST:9.560 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:63
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:65
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:58:33.099: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:65
STEP: Creating a pod to test override all
Oct 31 09:58:33.217: INFO: Waiting up to 5m0s for pod client-containers-0e0d31d6-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:58:33.232: INFO: Waiting for pod client-containers-0e0d31d6-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-containers-nfgwg' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.024348ms elapsed)
STEP: Saw pod success
Oct 31 09:58:35.263: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod client-containers-0e0d31d6-be22-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:58:35.301: INFO: Waiting for pod client-containers-0e0d31d6-be22-11e7-9459-0e11a30959be to disappear
Oct 31 09:58:35.316: INFO: Pod client-containers-0e0d31d6-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:58:35.316: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-nfgwg" for this suite.
Oct 31 09:58:42.154: INFO: namespace: e2e-tests-containers-nfgwg, resource: bindings, ignored listing per whitelist
Oct 31 09:58:42.689: INFO: namespace e2e-tests-containers-nfgwg deletion completed in 7.344754015s

• [SLOW TEST:9.590 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:65
------------------------------
S
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1143
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:58:42.689: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1123
[It] should create an rc or deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1143
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 09:58:42.804: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=gcr.io/google_containers/nginx-slim:0.7 --namespace=e2e-tests-kubectl-hdsrt'
Oct 31 09:58:43.944: INFO: stderr: ""
Oct 31 09:58:43.944: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1127
Oct 31 09:58:43.959: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-hdsrt'
Oct 31 09:58:47.330: INFO: stderr: ""
Oct 31 09:58:47.330: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:58:47.330: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hdsrt" for this suite.
Oct 31 09:58:53.988: INFO: namespace: e2e-tests-kubectl-hdsrt, resource: bindings, ignored listing per whitelist
Oct 31 09:58:54.688: INFO: namespace e2e-tests-kubectl-hdsrt deletion completed in 7.328940244s

• [SLOW TEST:11.999 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create an rc or deployment from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1143
------------------------------
[k8s.io] Projected 
  should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:926
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:58:54.688: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:926
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:58:54.830: INFO: Waiting up to 5m0s for pod downwardapi-volume-1aeef3f4-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:58:54.845: INFO: Waiting for pod downwardapi-volume-1aeef3f4-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-k44rv' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.926404ms elapsed)
STEP: Saw pod success
Oct 31 09:58:56.876: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-1aeef3f4-be22-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:58:56.915: INFO: Waiting for pod downwardapi-volume-1aeef3f4-be22-11e7-9459-0e11a30959be to disappear
Oct 31 09:58:56.930: INFO: Pod downwardapi-volume-1aeef3f4-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:58:56.930: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k44rv" for this suite.
Oct 31 09:59:03.723: INFO: namespace: e2e-tests-projected-k44rv, resource: bindings, ignored listing per whitelist
Oct 31 09:59:04.285: INFO: namespace e2e-tests-projected-k44rv deletion completed in 7.326291321s

• [SLOW TEST:9.597 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's cpu limit [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:926
------------------------------
[k8s.io] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:197
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:59:04.285: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:197
STEP: Creating a pod to test downward API volume plugin
Oct 31 09:59:04.395: INFO: Waiting up to 5m0s for pod downwardapi-volume-20a28b3d-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:59:04.410: INFO: Waiting for pod downwardapi-volume-20a28b3d-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-kgv4q' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.165461ms elapsed)
STEP: Saw pod success
Oct 31 09:59:06.441: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downwardapi-volume-20a28b3d-be22-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 09:59:06.479: INFO: Waiting for pod downwardapi-volume-20a28b3d-be22-11e7-9459-0e11a30959be to disappear
Oct 31 09:59:06.494: INFO: Pod downwardapi-volume-20a28b3d-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:59:06.494: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kgv4q" for this suite.
Oct 31 09:59:13.849: INFO: namespace: e2e-tests-downward-api-kgv4q, resource: bindings, ignored listing per whitelist
Oct 31 09:59:13.849: INFO: namespace e2e-tests-downward-api-kgv4q deletion completed in 7.326112655s

• [SLOW TEST:9.564 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:197
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:76
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:59:13.849: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:76
Oct 31 09:59:34.002: INFO: Container started at 2017-10-31 09:59:15 +0000 UTC, pod became ready at 2017-10-31 09:59:33 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:59:34.002: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5h9s8" for this suite.
Oct 31 09:59:56.899: INFO: namespace: e2e-tests-container-probe-5h9s8, resource: bindings, ignored listing per whitelist
Oct 31 09:59:57.355: INFO: namespace e2e-tests-container-probe-5h9s8 deletion completed in 23.324055375s

• [SLOW TEST:43.506 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:76
------------------------------
SSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:78
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 09:59:57.355: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:78
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 31 09:59:57.468: INFO: Waiting up to 5m0s for pod pod-4044d43d-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 09:59:57.482: INFO: Waiting for pod pod-4044d43d-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-ks9tw' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.839411ms elapsed)
STEP: Saw pod success
Oct 31 09:59:59.513: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-4044d43d-be22-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 09:59:59.552: INFO: Waiting for pod pod-4044d43d-be22-11e7-9459-0e11a30959be to disappear
Oct 31 09:59:59.567: INFO: Pod pod-4044d43d-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 09:59:59.567: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ks9tw" for this suite.
Oct 31 10:00:06.148: INFO: namespace: e2e-tests-emptydir-ks9tw, resource: bindings, ignored listing per whitelist
Oct 31 10:00:06.915: INFO: namespace e2e-tests-emptydir-ks9tw deletion completed in 7.318615872s

• [SLOW TEST:9.560 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0666,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:78
------------------------------
[k8s.io] Downward API volume 
  should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:190
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:00:06.915: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:190
STEP: Creating a pod to test downward API volume plugin
Oct 31 10:00:07.053: INFO: Waiting up to 5m0s for pod downwardapi-volume-45fb556a-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:00:07.069: INFO: Waiting for pod downwardapi-volume-45fb556a-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-4tsz4' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.059136ms elapsed)
STEP: Saw pod success
Oct 31 10:00:09.100: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod downwardapi-volume-45fb556a-be22-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 10:00:09.141: INFO: Waiting for pod downwardapi-volume-45fb556a-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:00:09.157: INFO: Pod downwardapi-volume-45fb556a-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:00:09.157: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4tsz4" for this suite.
Oct 31 10:00:15.877: INFO: namespace: e2e-tests-downward-api-4tsz4, resource: bindings, ignored listing per whitelist
Oct 31 10:00:16.513: INFO: namespace e2e-tests-downward-api-4tsz4 deletion completed in 7.327711613s

• [SLOW TEST:9.598 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide container's memory request [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:190
------------------------------
SS
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:38
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:00:16.513: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:38
STEP: Creating configMap with name configmap-test-volume-4bb8681b-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:00:16.704: INFO: Waiting up to 5m0s for pod pod-configmaps-4bbb8f32-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:00:16.720: INFO: Waiting for pod pod-configmaps-4bbb8f32-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-vghvf' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.183067ms elapsed)
STEP: Saw pod success
Oct 31 10:00:18.750: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-configmaps-4bbb8f32-be22-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:00:18.789: INFO: Waiting for pod pod-configmaps-4bbb8f32-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:00:18.804: INFO: Pod pod-configmaps-4bbb8f32-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:00:18.804: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vghvf" for this suite.
Oct 31 10:00:26.067: INFO: namespace: e2e-tests-configmap-vghvf, resource: bindings, ignored listing per whitelist
Oct 31 10:00:26.155: INFO: namespace e2e-tests-configmap-vghvf deletion completed in 7.32248958s

• [SLOW TEST:9.642 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:38
------------------------------
SSSS
------------------------------
[k8s.io] Networking [k8s.io] Granular Checks: Pods 
  should function for intra-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:38
[BeforeEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:00:26.155: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:38
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vpprq
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 31 10:00:26.259: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Oct 31 10:00:48.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.6.48:8080/dial?request=hostName&protocol=http&host=172.16.4.45&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-vpprq PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:00:48.597: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:00:48.885: INFO: Waiting for endpoints: map[]
Oct 31 10:00:48.900: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.6.48:8080/dial?request=hostName&protocol=http&host=172.16.2.46&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-vpprq PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:00:48.900: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:00:49.085: INFO: Waiting for endpoints: map[]
Oct 31 10:00:49.100: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.6.48:8080/dial?request=hostName&protocol=http&host=172.16.6.47&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-vpprq PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:00:49.100: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:00:49.287: INFO: Waiting for endpoints: map[]
[AfterEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:00:49.287: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vpprq" for this suite.
Oct 31 10:01:12.093: INFO: namespace: e2e-tests-pod-network-test-vpprq, resource: bindings, ignored listing per whitelist
Oct 31 10:01:12.642: INFO: namespace e2e-tests-pod-network-test-vpprq deletion completed in 23.325819683s

• [SLOW TEST:46.486 seconds]
[k8s.io] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should function for intra-pod communication: http [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:38
------------------------------
S
------------------------------
[k8s.io] Secrets 
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:425
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:01:12.642: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:425
STEP: creating secret e2e-tests-secrets-t68wl/secret-test-6d2511ac-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 10:01:12.773: INFO: Waiting up to 5m0s for pod pod-configmaps-6d276bba-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:01:12.787: INFO: Waiting for pod pod-configmaps-6d276bba-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-t68wl' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.604828ms elapsed)
STEP: Saw pod success
Oct 31 10:01:14.818: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-configmaps-6d276bba-be22-11e7-9459-0e11a30959be container env-test: <nil>
STEP: delete the pod
Oct 31 10:01:14.857: INFO: Waiting for pod pod-configmaps-6d276bba-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:01:14.871: INFO: Pod pod-configmaps-6d276bba-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:01:14.871: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t68wl" for this suite.
Oct 31 10:01:21.746: INFO: namespace: e2e-tests-secrets-t68wl, resource: bindings, ignored listing per whitelist
Oct 31 10:01:22.217: INFO: namespace e2e-tests-secrets-t68wl deletion completed in 7.316992067s

• [SLOW TEST:9.575 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:425
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:122
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:01:22.217: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:122
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 31 10:01:22.318: INFO: Waiting up to 5m0s for pod pod-72d7e98e-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:01:22.333: INFO: Waiting for pod pod-72d7e98e-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-7fxgr' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.175668ms elapsed)
STEP: Saw pod success
Oct 31 10:01:24.363: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-72d7e98e-be22-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:01:24.402: INFO: Waiting for pod pod-72d7e98e-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:01:24.417: INFO: Pod pod-72d7e98e-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:01:24.417: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7fxgr" for this suite.
Oct 31 10:01:31.002: INFO: namespace: e2e-tests-emptydir-7fxgr, resource: bindings, ignored listing per whitelist
Oct 31 10:01:31.780: INFO: namespace e2e-tests-emptydir-7fxgr deletion completed in 7.334609498s

• [SLOW TEST:9.564 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0777,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:122
------------------------------
SS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:55
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:01:31.781: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:55
STEP: Creating secret with name secret-test-map-788c95a8-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 10:01:31.906: INFO: Waiting up to 5m0s for pod pod-secrets-788f0284-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:01:31.922: INFO: Waiting for pod pod-secrets-788f0284-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-rqqfk' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.618832ms elapsed)
STEP: Saw pod success
Oct 31 10:01:33.954: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-secrets-788f0284-be22-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 10:01:33.992: INFO: Waiting for pod pod-secrets-788f0284-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:01:34.008: INFO: Pod pod-secrets-788f0284-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:01:34.008: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rqqfk" for this suite.
Oct 31 10:01:40.786: INFO: namespace: e2e-tests-secrets-rqqfk, resource: bindings, ignored listing per whitelist
Oct 31 10:01:41.368: INFO: namespace e2e-tests-secrets-rqqfk deletion completed in 7.3321285s

• [SLOW TEST:9.588 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:55
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Projected 
  should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:795
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:01:41.369: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:795
STEP: Creating configMap with name projected-configmap-test-volume-7e40cb88-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:01:41.475: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-7e43267e-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:01:41.489: INFO: Waiting for pod pod-projected-configmaps-7e43267e-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-qfgmb' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.767973ms elapsed)
STEP: Saw pod success
Oct 31 10:01:43.521: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-projected-configmaps-7e43267e-be22-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:01:43.559: INFO: Waiting for pod pod-projected-configmaps-7e43267e-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:01:43.575: INFO: Pod pod-projected-configmaps-7e43267e-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:01:43.575: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qfgmb" for this suite.
Oct 31 10:01:50.268: INFO: namespace: e2e-tests-projected-qfgmb, resource: bindings, ignored listing per whitelist
Oct 31 10:01:50.943: INFO: namespace e2e-tests-projected-qfgmb deletion completed in 7.339120668s

• [SLOW TEST:9.574 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:795
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:178
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:01:50.943: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:178
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-844k4
Oct 31 10:01:53.187: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-844k4
STEP: checking the pod's current state and verifying that restartCount is present
Oct 31 10:01:53.202: INFO: Initial restart count of pod liveness-http is 0
Oct 31 10:02:13.374: INFO: Restart count of pod e2e-tests-container-probe-844k4/liveness-http is now 1 (20.171939175s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:02:13.393: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-844k4" for this suite.
Oct 31 10:02:20.199: INFO: namespace: e2e-tests-container-probe-844k4, resource: bindings, ignored listing per whitelist
Oct 31 10:02:20.753: INFO: namespace e2e-tests-container-probe-844k4 deletion completed in 7.33041473s

• [SLOW TEST:29.810 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:178
------------------------------
[k8s.io] HostPath 
  should give a volume the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:56
[BeforeEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:02:20.753: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:41
[It] should give a volume the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:56
STEP: Creating a pod to test hostPath mode
Oct 31 10:02:20.880: INFO: Waiting up to 5m0s for pod pod-host-path-test status to be success or failure
Oct 31 10:02:20.894: INFO: Waiting for pod pod-host-path-test in namespace 'e2e-tests-hostpath-spbbn' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.610103ms elapsed)
Oct 31 10:02:22.910: INFO: Waiting for pod pod-host-path-test in namespace 'e2e-tests-hostpath-spbbn' status to be 'success or failure'(found phase: "Pending", readiness: false) (2.030198087s elapsed)
STEP: Saw pod success
Oct 31 10:02:24.941: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Oct 31 10:02:24.983: INFO: Waiting for pod pod-host-path-test to disappear
Oct 31 10:02:24.998: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:02:24.998: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-spbbn" for this suite.
Oct 31 10:02:31.629: INFO: namespace: e2e-tests-hostpath-spbbn, resource: bindings, ignored listing per whitelist
Oct 31 10:02:32.330: INFO: namespace e2e-tests-hostpath-spbbn deletion completed in 7.304100467s

• [SLOW TEST:11.577 seconds]
[k8s.io] HostPath
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should give a volume the correct mode [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:56
------------------------------
SSSSS
------------------------------
[k8s.io] Proxy version v1 
  should proxy logs on node using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:67
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:02:32.331: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:67
Oct 31 10:02:32.485: INFO: (0) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 19.415303ms)
Oct 31 10:02:32.501: INFO: (1) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.332349ms)
Oct 31 10:02:32.517: INFO: (2) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.079674ms)
Oct 31 10:02:32.533: INFO: (3) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.142171ms)
Oct 31 10:02:32.549: INFO: (4) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.762905ms)
Oct 31 10:02:32.566: INFO: (5) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.288197ms)
Oct 31 10:02:32.582: INFO: (6) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.211924ms)
Oct 31 10:02:32.597: INFO: (7) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.724229ms)
Oct 31 10:02:32.613: INFO: (8) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.898142ms)
Oct 31 10:02:32.629: INFO: (9) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.036977ms)
Oct 31 10:02:32.645: INFO: (10) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.638614ms)
Oct 31 10:02:32.661: INFO: (11) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.800003ms)
Oct 31 10:02:32.677: INFO: (12) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.760165ms)
Oct 31 10:02:32.692: INFO: (13) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.682165ms)
Oct 31 10:02:32.708: INFO: (14) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.014336ms)
Oct 31 10:02:32.724: INFO: (15) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.000894ms)
Oct 31 10:02:32.740: INFO: (16) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.976771ms)
Oct 31 10:02:32.756: INFO: (17) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.972444ms)
Oct 31 10:02:32.772: INFO: (18) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.003905ms)
Oct 31 10:02:32.789: INFO: (19) /api/v1/nodes/ci-prtest-cc63063-94-ig-n-2mxx/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.054412ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:02:32.789: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zp7h4" for this suite.
Oct 31 10:02:39.333: INFO: namespace: e2e-tests-proxy-zp7h4, resource: bindings, ignored listing per whitelist
Oct 31 10:02:40.113: INFO: namespace e2e-tests-proxy-zp7h4 deletion completed in 7.308517037s

• [SLOW TEST:7.782 seconds]
[k8s.io] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:275
    should proxy logs on node using proxy subresource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:67
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:69
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:02:40.113: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:69
STEP: Creating configMap with name configmap-test-volume-map-a150b7c8-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:02:40.301: INFO: Waiting up to 5m0s for pod pod-configmaps-a153070f-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:02:40.316: INFO: Waiting for pod pod-configmaps-a153070f-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-sxs8c' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.915377ms elapsed)
STEP: Saw pod success
Oct 31 10:02:42.347: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-configmaps-a153070f-be22-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:02:42.387: INFO: Waiting for pod pod-configmaps-a153070f-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:02:42.402: INFO: Pod pod-configmaps-a153070f-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:02:42.402: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sxs8c" for this suite.
Oct 31 10:02:49.474: INFO: namespace: e2e-tests-configmap-sxs8c, resource: bindings, ignored listing per whitelist
Oct 31 10:02:49.737: INFO: namespace e2e-tests-configmap-sxs8c deletion completed in 7.305648702s

• [SLOW TEST:9.623 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings as non-root [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:69
------------------------------
[k8s.io] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:967
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:02:49.737: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:967
STEP: Creating a pod to test downward API volume plugin
Oct 31 10:02:49.849: INFO: Waiting up to 5m0s for pod downwardapi-volume-a7041bc4-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:02:49.864: INFO: Waiting for pod downwardapi-volume-a7041bc4-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-5qnsq' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.680592ms elapsed)
STEP: Saw pod success
Oct 31 10:02:51.894: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-a7041bc4-be22-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 10:02:51.932: INFO: Waiting for pod downwardapi-volume-a7041bc4-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:02:51.947: INFO: Pod downwardapi-volume-a7041bc4-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:02:51.947: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5qnsq" for this suite.
Oct 31 10:02:59.238: INFO: namespace: e2e-tests-projected-5qnsq, resource: bindings, ignored listing per whitelist
Oct 31 10:02:59.297: INFO: namespace e2e-tests-projected-5qnsq deletion completed in 7.3227684s

• [SLOW TEST:9.561 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:967
------------------------------
SSSS
------------------------------
[k8s.io] Downward API volume 
  should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:125
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:02:59.298: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:125
STEP: Creating the pod
Oct 31 10:03:02.030: INFO: Successfully updated pod "labelsupdateacba0def-be22-11e7-9459-0e11a30959be"
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:03:06.087: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pmz6h" for this suite.
Oct 31 10:03:29.268: INFO: namespace: e2e-tests-downward-api-pmz6h, resource: bindings, ignored listing per whitelist
Oct 31 10:03:29.430: INFO: namespace e2e-tests-downward-api-pmz6h deletion completed in 23.315145958s

• [SLOW TEST:30.133 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should update labels on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:125
------------------------------
SSSSS
------------------------------
[k8s.io] Service endpoints latency 
  should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_latency.go:117
[BeforeEach] [k8s.io] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:03:29.430: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_latency.go:117
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-qz7br
I1031 10:03:29.547098    7114 runners.go:176] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-qz7br, replica count: 1
I1031 10:03:30.547390    7114 runners.go:176] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1031 10:03:31.547588    7114 runners.go:176] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 31 10:03:31.666: INFO: Created: latency-svc-zn4rq
Oct 31 10:03:31.683: INFO: Got endpoints: latency-svc-zn4rq [35.740023ms]
Oct 31 10:03:31.705: INFO: Created: latency-svc-4tkb9
Oct 31 10:03:31.718: INFO: Got endpoints: latency-svc-4tkb9 [34.802346ms]
Oct 31 10:03:31.729: INFO: Created: latency-svc-pw4z2
Oct 31 10:03:31.748: INFO: Created: latency-svc-68p64
Oct 31 10:03:31.787: INFO: Created: latency-svc-mkvfb
Oct 31 10:03:31.788: INFO: Got endpoints: latency-svc-pw4z2 [104.463362ms]
Oct 31 10:03:31.795: INFO: Got endpoints: latency-svc-mkvfb [112.070367ms]
Oct 31 10:03:31.796: INFO: Got endpoints: latency-svc-68p64 [112.30699ms]
Oct 31 10:03:31.798: INFO: Created: latency-svc-whpvp
Oct 31 10:03:31.799: INFO: Got endpoints: latency-svc-whpvp [115.380825ms]
Oct 31 10:03:31.853: INFO: Created: latency-svc-bvwwp
Oct 31 10:03:31.871: INFO: Got endpoints: latency-svc-bvwwp [187.915263ms]
Oct 31 10:03:31.890: INFO: Created: latency-svc-2gh9c
Oct 31 10:03:31.917: INFO: Got endpoints: latency-svc-2gh9c [233.929695ms]
Oct 31 10:03:31.918: INFO: Created: latency-svc-jzf6f
Oct 31 10:03:31.939: INFO: Created: latency-svc-tptwh
Oct 31 10:03:31.939: INFO: Got endpoints: latency-svc-jzf6f [255.710046ms]
Oct 31 10:03:31.961: INFO: Got endpoints: latency-svc-tptwh [278.064433ms]
Oct 31 10:03:31.967: INFO: Created: latency-svc-pqcbq
Oct 31 10:03:31.989: INFO: Created: latency-svc-77vbh
Oct 31 10:03:31.994: INFO: Got endpoints: latency-svc-pqcbq [310.981515ms]
Oct 31 10:03:32.002: INFO: Created: latency-svc-wc42w
Oct 31 10:03:32.032: INFO: Created: latency-svc-9jq2s
Oct 31 10:03:32.037: INFO: Got endpoints: latency-svc-wc42w [249.075152ms]
Oct 31 10:03:32.037: INFO: Got endpoints: latency-svc-77vbh [354.018643ms]
Oct 31 10:03:32.054: INFO: Created: latency-svc-d6dj7
Oct 31 10:03:32.075: INFO: Got endpoints: latency-svc-9jq2s [391.390132ms]
Oct 31 10:03:32.079: INFO: Created: latency-svc-p6gss
Oct 31 10:03:32.082: INFO: Got endpoints: latency-svc-d6dj7 [87.821482ms]
Oct 31 10:03:32.096: INFO: Created: latency-svc-xt8c5
Oct 31 10:03:32.102: INFO: Got endpoints: latency-svc-p6gss [302.703363ms]
Oct 31 10:03:32.123: INFO: Got endpoints: latency-svc-xt8c5 [327.823857ms]
Oct 31 10:03:32.126: INFO: Created: latency-svc-jkr6d
Oct 31 10:03:32.133: INFO: Got endpoints: latency-svc-jkr6d [449.84325ms]
Oct 31 10:03:32.136: INFO: Created: latency-svc-kcdjg
Oct 31 10:03:32.141: INFO: Got endpoints: latency-svc-kcdjg [458.023741ms]
Oct 31 10:03:32.146: INFO: Created: latency-svc-xsh8k
Oct 31 10:03:32.159: INFO: Created: latency-svc-7tsvn
Oct 31 10:03:32.168: INFO: Got endpoints: latency-svc-xsh8k [484.422977ms]
Oct 31 10:03:32.168: INFO: Got endpoints: latency-svc-7tsvn [296.716054ms]
Oct 31 10:03:32.178: INFO: Created: latency-svc-btzcd
Oct 31 10:03:32.182: INFO: Created: latency-svc-v5hc6
Oct 31 10:03:32.186: INFO: Got endpoints: latency-svc-btzcd [467.653012ms]
Oct 31 10:03:32.188: INFO: Got endpoints: latency-svc-v5hc6 [270.116086ms]
Oct 31 10:03:32.188: INFO: Created: latency-svc-lhkc6
Oct 31 10:03:32.197: INFO: Got endpoints: latency-svc-lhkc6 [257.847205ms]
Oct 31 10:03:32.198: INFO: Created: latency-svc-kvl2g
Oct 31 10:03:32.201: INFO: Created: latency-svc-b4shh
Oct 31 10:03:32.204: INFO: Got endpoints: latency-svc-kvl2g [242.444635ms]
Oct 31 10:03:32.210: INFO: Created: latency-svc-lnzcz
Oct 31 10:03:32.215: INFO: Got endpoints: latency-svc-b4shh [419.311792ms]
Oct 31 10:03:32.216: INFO: Got endpoints: latency-svc-lnzcz [179.105407ms]
Oct 31 10:03:32.218: INFO: Created: latency-svc-87vfs
Oct 31 10:03:32.224: INFO: Got endpoints: latency-svc-87vfs [186.580105ms]
Oct 31 10:03:32.224: INFO: Created: latency-svc-f4cmq
Oct 31 10:03:32.233: INFO: Created: latency-svc-kppf9
Oct 31 10:03:32.233: INFO: Got endpoints: latency-svc-f4cmq [157.95439ms]
Oct 31 10:03:32.241: INFO: Got endpoints: latency-svc-kppf9 [159.14138ms]
Oct 31 10:03:32.247: INFO: Created: latency-svc-hmrks
Oct 31 10:03:32.253: INFO: Got endpoints: latency-svc-hmrks [129.935308ms]
Oct 31 10:03:32.253: INFO: Created: latency-svc-vdqns
Oct 31 10:03:32.253: INFO: Got endpoints: latency-svc-vdqns [151.669489ms]
Oct 31 10:03:32.254: INFO: Created: latency-svc-l7r8q
Oct 31 10:03:32.258: INFO: Created: latency-svc-nblgt
Oct 31 10:03:32.260: INFO: Got endpoints: latency-svc-l7r8q [126.536435ms]
Oct 31 10:03:32.266: INFO: Got endpoints: latency-svc-nblgt [124.918419ms]
Oct 31 10:03:32.268: INFO: Created: latency-svc-z5qtx
Oct 31 10:03:32.270: INFO: Got endpoints: latency-svc-z5qtx [101.700255ms]
Oct 31 10:03:32.272: INFO: Created: latency-svc-g6s6z
Oct 31 10:03:32.276: INFO: Got endpoints: latency-svc-g6s6z [107.603006ms]
Oct 31 10:03:32.277: INFO: Created: latency-svc-6dgsd
Oct 31 10:03:32.282: INFO: Created: latency-svc-94w9t
Oct 31 10:03:32.285: INFO: Got endpoints: latency-svc-6dgsd [99.804252ms]
Oct 31 10:03:32.289: INFO: Got endpoints: latency-svc-94w9t [100.994934ms]
Oct 31 10:03:32.290: INFO: Created: latency-svc-vv5cc
Oct 31 10:03:32.292: INFO: Created: latency-svc-nb7f4
Oct 31 10:03:32.296: INFO: Got endpoints: latency-svc-nb7f4 [92.078566ms]
Oct 31 10:03:32.298: INFO: Got endpoints: latency-svc-vv5cc [100.786204ms]
Oct 31 10:03:32.299: INFO: Created: latency-svc-sgjqr
Oct 31 10:03:32.302: INFO: Created: latency-svc-lfhrb
Oct 31 10:03:32.318: INFO: Got endpoints: latency-svc-lfhrb [101.405584ms]
Oct 31 10:03:32.318: INFO: Got endpoints: latency-svc-sgjqr [102.774426ms]
Oct 31 10:03:32.320: INFO: Created: latency-svc-69d85
Oct 31 10:03:32.324: INFO: Created: latency-svc-58dkl
Oct 31 10:03:32.324: INFO: Got endpoints: latency-svc-58dkl [99.819229ms]
Oct 31 10:03:32.328: INFO: Created: latency-svc-ppnkb
Oct 31 10:03:32.329: INFO: Got endpoints: latency-svc-69d85 [95.891318ms]
Oct 31 10:03:32.339: INFO: Created: latency-svc-9fxsr
Oct 31 10:03:32.347: INFO: Created: latency-svc-vtk6r
Oct 31 10:03:32.351: INFO: Created: latency-svc-bf58r
Oct 31 10:03:32.357: INFO: Created: latency-svc-8fhl2
Oct 31 10:03:32.362: INFO: Created: latency-svc-d74c9
Oct 31 10:03:32.368: INFO: Created: latency-svc-t7rqg
Oct 31 10:03:32.373: INFO: Got endpoints: latency-svc-ppnkb [131.845067ms]
Oct 31 10:03:32.374: INFO: Created: latency-svc-m5dd9
Oct 31 10:03:32.385: INFO: Created: latency-svc-gqmmp
Oct 31 10:03:32.390: INFO: Created: latency-svc-hwwpj
Oct 31 10:03:32.395: INFO: Created: latency-svc-4j29p
Oct 31 10:03:32.404: INFO: Created: latency-svc-xtcgk
Oct 31 10:03:32.409: INFO: Created: latency-svc-vm9tp
Oct 31 10:03:32.415: INFO: Created: latency-svc-j5psb
Oct 31 10:03:32.424: INFO: Created: latency-svc-gfvqk
Oct 31 10:03:32.427: INFO: Got endpoints: latency-svc-9fxsr [173.635871ms]
Oct 31 10:03:32.431: INFO: Created: latency-svc-lkzk2
Oct 31 10:03:32.448: INFO: Created: latency-svc-6mmt8
Oct 31 10:03:32.469: INFO: Got endpoints: latency-svc-vtk6r [216.222322ms]
Oct 31 10:03:32.492: INFO: Created: latency-svc-sq7rd
Oct 31 10:03:32.521: INFO: Got endpoints: latency-svc-bf58r [260.929277ms]
Oct 31 10:03:32.542: INFO: Created: latency-svc-cwlqx
Oct 31 10:03:32.570: INFO: Got endpoints: latency-svc-8fhl2 [303.272332ms]
Oct 31 10:03:32.588: INFO: Created: latency-svc-xhpk6
Oct 31 10:03:32.620: INFO: Got endpoints: latency-svc-d74c9 [350.033945ms]
Oct 31 10:03:32.640: INFO: Created: latency-svc-rdjpw
Oct 31 10:03:32.671: INFO: Got endpoints: latency-svc-t7rqg [395.061505ms]
Oct 31 10:03:32.690: INFO: Created: latency-svc-stx7m
Oct 31 10:03:32.720: INFO: Got endpoints: latency-svc-m5dd9 [434.236848ms]
Oct 31 10:03:32.738: INFO: Created: latency-svc-qdfl8
Oct 31 10:03:32.771: INFO: Got endpoints: latency-svc-gqmmp [481.934113ms]
Oct 31 10:03:32.808: INFO: Created: latency-svc-lj764
Oct 31 10:03:32.822: INFO: Got endpoints: latency-svc-hwwpj [523.977676ms]
Oct 31 10:03:32.840: INFO: Created: latency-svc-v5bwb
Oct 31 10:03:32.870: INFO: Got endpoints: latency-svc-4j29p [574.509823ms]
Oct 31 10:03:32.890: INFO: Created: latency-svc-s8s4m
Oct 31 10:03:32.919: INFO: Got endpoints: latency-svc-xtcgk [601.387443ms]
Oct 31 10:03:32.939: INFO: Created: latency-svc-flzkj
Oct 31 10:03:32.970: INFO: Got endpoints: latency-svc-vm9tp [651.954688ms]
Oct 31 10:03:32.991: INFO: Created: latency-svc-p9ttc
Oct 31 10:03:33.021: INFO: Got endpoints: latency-svc-j5psb [697.405111ms]
Oct 31 10:03:33.045: INFO: Created: latency-svc-vfcrb
Oct 31 10:03:33.070: INFO: Got endpoints: latency-svc-gfvqk [741.408798ms]
Oct 31 10:03:33.089: INFO: Created: latency-svc-pxc9s
Oct 31 10:03:33.120: INFO: Got endpoints: latency-svc-lkzk2 [746.592072ms]
Oct 31 10:03:33.138: INFO: Created: latency-svc-flw5h
Oct 31 10:03:33.170: INFO: Got endpoints: latency-svc-6mmt8 [742.998632ms]
Oct 31 10:03:33.189: INFO: Created: latency-svc-vmp94
Oct 31 10:03:33.226: INFO: Got endpoints: latency-svc-sq7rd [756.450878ms]
Oct 31 10:03:33.247: INFO: Created: latency-svc-28q72
Oct 31 10:03:33.270: INFO: Got endpoints: latency-svc-cwlqx [749.457924ms]
Oct 31 10:03:33.289: INFO: Created: latency-svc-vr52b
Oct 31 10:03:33.320: INFO: Got endpoints: latency-svc-xhpk6 [750.210255ms]
Oct 31 10:03:33.340: INFO: Created: latency-svc-rd5q6
Oct 31 10:03:33.370: INFO: Got endpoints: latency-svc-rdjpw [749.696522ms]
Oct 31 10:03:33.388: INFO: Created: latency-svc-ggfhm
Oct 31 10:03:33.421: INFO: Got endpoints: latency-svc-stx7m [749.894005ms]
Oct 31 10:03:33.440: INFO: Created: latency-svc-7n8m4
Oct 31 10:03:33.470: INFO: Got endpoints: latency-svc-qdfl8 [750.231404ms]
Oct 31 10:03:33.490: INFO: Created: latency-svc-rvc27
Oct 31 10:03:33.520: INFO: Got endpoints: latency-svc-lj764 [749.59169ms]
Oct 31 10:03:33.543: INFO: Created: latency-svc-mn9hx
Oct 31 10:03:33.570: INFO: Got endpoints: latency-svc-v5bwb [748.520351ms]
Oct 31 10:03:33.593: INFO: Created: latency-svc-jwvdv
Oct 31 10:03:33.620: INFO: Got endpoints: latency-svc-s8s4m [749.922197ms]
Oct 31 10:03:33.642: INFO: Created: latency-svc-bvfdf
Oct 31 10:03:33.670: INFO: Got endpoints: latency-svc-flzkj [750.95387ms]
Oct 31 10:03:33.691: INFO: Created: latency-svc-dwdqw
Oct 31 10:03:33.720: INFO: Got endpoints: latency-svc-p9ttc [750.951938ms]
Oct 31 10:03:33.742: INFO: Created: latency-svc-s44n4
Oct 31 10:03:33.770: INFO: Got endpoints: latency-svc-vfcrb [748.58011ms]
Oct 31 10:03:33.793: INFO: Created: latency-svc-zszqm
Oct 31 10:03:33.820: INFO: Got endpoints: latency-svc-pxc9s [749.440514ms]
Oct 31 10:03:33.840: INFO: Created: latency-svc-5zttf
Oct 31 10:03:33.871: INFO: Got endpoints: latency-svc-flw5h [750.898082ms]
Oct 31 10:03:33.894: INFO: Created: latency-svc-cv6sf
Oct 31 10:03:33.921: INFO: Got endpoints: latency-svc-vmp94 [751.134589ms]
Oct 31 10:03:33.944: INFO: Created: latency-svc-jbsnv
Oct 31 10:03:33.971: INFO: Got endpoints: latency-svc-28q72 [744.810179ms]
Oct 31 10:03:33.993: INFO: Created: latency-svc-n2f92
Oct 31 10:03:34.022: INFO: Got endpoints: latency-svc-vr52b [751.548458ms]
Oct 31 10:03:34.043: INFO: Created: latency-svc-vstgz
Oct 31 10:03:34.071: INFO: Got endpoints: latency-svc-rd5q6 [751.14803ms]
Oct 31 10:03:34.091: INFO: Created: latency-svc-ddbj8
Oct 31 10:03:34.121: INFO: Got endpoints: latency-svc-ggfhm [751.014925ms]
Oct 31 10:03:34.140: INFO: Created: latency-svc-crxkl
Oct 31 10:03:34.170: INFO: Got endpoints: latency-svc-7n8m4 [749.877908ms]
Oct 31 10:03:34.190: INFO: Created: latency-svc-ll4x6
Oct 31 10:03:34.221: INFO: Got endpoints: latency-svc-rvc27 [750.919001ms]
Oct 31 10:03:34.242: INFO: Created: latency-svc-hk9dd
Oct 31 10:03:34.271: INFO: Got endpoints: latency-svc-mn9hx [751.300099ms]
Oct 31 10:03:34.292: INFO: Created: latency-svc-ggphv
Oct 31 10:03:34.321: INFO: Got endpoints: latency-svc-jwvdv [750.716339ms]
Oct 31 10:03:34.344: INFO: Created: latency-svc-9bvd6
Oct 31 10:03:34.369: INFO: Got endpoints: latency-svc-bvfdf [749.068445ms]
Oct 31 10:03:34.393: INFO: Created: latency-svc-xl9l5
Oct 31 10:03:34.421: INFO: Got endpoints: latency-svc-dwdqw [751.284631ms]
Oct 31 10:03:34.441: INFO: Created: latency-svc-dp9jl
Oct 31 10:03:34.470: INFO: Got endpoints: latency-svc-s44n4 [749.051853ms]
Oct 31 10:03:34.491: INFO: Created: latency-svc-8bd26
Oct 31 10:03:34.521: INFO: Got endpoints: latency-svc-zszqm [751.571177ms]
Oct 31 10:03:34.544: INFO: Created: latency-svc-jxd78
Oct 31 10:03:34.571: INFO: Got endpoints: latency-svc-5zttf [751.138422ms]
Oct 31 10:03:34.592: INFO: Created: latency-svc-ddp5w
Oct 31 10:03:34.621: INFO: Got endpoints: latency-svc-cv6sf [750.635791ms]
Oct 31 10:03:34.645: INFO: Created: latency-svc-8fs25
Oct 31 10:03:34.672: INFO: Got endpoints: latency-svc-jbsnv [750.748046ms]
Oct 31 10:03:34.693: INFO: Created: latency-svc-9t4gr
Oct 31 10:03:34.719: INFO: Got endpoints: latency-svc-n2f92 [748.726639ms]
Oct 31 10:03:34.740: INFO: Created: latency-svc-hggcq
Oct 31 10:03:34.771: INFO: Got endpoints: latency-svc-vstgz [748.630019ms]
Oct 31 10:03:34.791: INFO: Created: latency-svc-7zf7q
Oct 31 10:03:34.821: INFO: Got endpoints: latency-svc-ddbj8 [749.513289ms]
Oct 31 10:03:34.842: INFO: Created: latency-svc-dpqrj
Oct 31 10:03:34.871: INFO: Got endpoints: latency-svc-crxkl [749.976501ms]
Oct 31 10:03:34.890: INFO: Created: latency-svc-smtr8
Oct 31 10:03:34.923: INFO: Got endpoints: latency-svc-ll4x6 [752.907749ms]
Oct 31 10:03:34.943: INFO: Created: latency-svc-lxbrs
Oct 31 10:03:34.971: INFO: Got endpoints: latency-svc-hk9dd [750.108587ms]
Oct 31 10:03:34.992: INFO: Created: latency-svc-jgthh
Oct 31 10:03:35.020: INFO: Got endpoints: latency-svc-ggphv [748.5207ms]
Oct 31 10:03:35.040: INFO: Created: latency-svc-l9mqk
Oct 31 10:03:35.070: INFO: Got endpoints: latency-svc-9bvd6 [749.159298ms]
Oct 31 10:03:35.092: INFO: Created: latency-svc-2jjvz
Oct 31 10:03:35.121: INFO: Got endpoints: latency-svc-xl9l5 [751.023355ms]
Oct 31 10:03:35.142: INFO: Created: latency-svc-gzhqb
Oct 31 10:03:35.172: INFO: Got endpoints: latency-svc-dp9jl [750.309037ms]
Oct 31 10:03:35.194: INFO: Created: latency-svc-tkflv
Oct 31 10:03:35.221: INFO: Got endpoints: latency-svc-8bd26 [751.643524ms]
Oct 31 10:03:35.241: INFO: Created: latency-svc-wdvfw
Oct 31 10:03:35.270: INFO: Got endpoints: latency-svc-jxd78 [748.332593ms]
Oct 31 10:03:35.290: INFO: Created: latency-svc-fhj9c
Oct 31 10:03:35.320: INFO: Got endpoints: latency-svc-ddp5w [748.729291ms]
Oct 31 10:03:35.341: INFO: Created: latency-svc-qmgwd
Oct 31 10:03:35.372: INFO: Got endpoints: latency-svc-8fs25 [750.196231ms]
Oct 31 10:03:35.393: INFO: Created: latency-svc-ff7kg
Oct 31 10:03:35.422: INFO: Got endpoints: latency-svc-9t4gr [750.126907ms]
Oct 31 10:03:35.443: INFO: Created: latency-svc-4vvh5
Oct 31 10:03:35.472: INFO: Got endpoints: latency-svc-hggcq [752.000869ms]
Oct 31 10:03:35.494: INFO: Created: latency-svc-jkqb5
Oct 31 10:03:35.520: INFO: Got endpoints: latency-svc-7zf7q [749.18153ms]
Oct 31 10:03:35.545: INFO: Created: latency-svc-zc7lj
Oct 31 10:03:35.571: INFO: Got endpoints: latency-svc-dpqrj [750.712912ms]
Oct 31 10:03:35.593: INFO: Created: latency-svc-chr5g
Oct 31 10:03:35.621: INFO: Got endpoints: latency-svc-smtr8 [750.770555ms]
Oct 31 10:03:35.644: INFO: Created: latency-svc-hvknw
Oct 31 10:03:35.671: INFO: Got endpoints: latency-svc-lxbrs [747.679604ms]
Oct 31 10:03:35.693: INFO: Created: latency-svc-9nvtn
Oct 31 10:03:35.721: INFO: Got endpoints: latency-svc-jgthh [750.058361ms]
Oct 31 10:03:35.747: INFO: Created: latency-svc-fxpth
Oct 31 10:03:35.771: INFO: Got endpoints: latency-svc-l9mqk [750.938495ms]
Oct 31 10:03:35.791: INFO: Created: latency-svc-xfvpl
Oct 31 10:03:35.821: INFO: Got endpoints: latency-svc-2jjvz [751.111919ms]
Oct 31 10:03:35.845: INFO: Created: latency-svc-j22z7
Oct 31 10:03:35.871: INFO: Got endpoints: latency-svc-gzhqb [750.494308ms]
Oct 31 10:03:35.893: INFO: Created: latency-svc-hxwl4
Oct 31 10:03:35.921: INFO: Got endpoints: latency-svc-tkflv [749.340552ms]
Oct 31 10:03:35.942: INFO: Created: latency-svc-wsxrf
Oct 31 10:03:35.972: INFO: Got endpoints: latency-svc-wdvfw [750.418503ms]
Oct 31 10:03:35.993: INFO: Created: latency-svc-zpjcl
Oct 31 10:03:36.022: INFO: Got endpoints: latency-svc-fhj9c [751.92035ms]
Oct 31 10:03:36.041: INFO: Created: latency-svc-tvv4b
Oct 31 10:03:36.073: INFO: Got endpoints: latency-svc-qmgwd [753.092156ms]
Oct 31 10:03:36.094: INFO: Created: latency-svc-tq24x
Oct 31 10:03:36.121: INFO: Got endpoints: latency-svc-ff7kg [748.921639ms]
Oct 31 10:03:36.142: INFO: Created: latency-svc-n6r6n
Oct 31 10:03:36.172: INFO: Got endpoints: latency-svc-4vvh5 [749.738726ms]
Oct 31 10:03:36.194: INFO: Created: latency-svc-5s6vx
Oct 31 10:03:36.222: INFO: Got endpoints: latency-svc-jkqb5 [750.062406ms]
Oct 31 10:03:36.243: INFO: Created: latency-svc-hrbzd
Oct 31 10:03:36.271: INFO: Got endpoints: latency-svc-zc7lj [750.813839ms]
Oct 31 10:03:36.292: INFO: Created: latency-svc-vg6ft
Oct 31 10:03:36.320: INFO: Got endpoints: latency-svc-chr5g [748.356574ms]
Oct 31 10:03:36.340: INFO: Created: latency-svc-rj2mv
Oct 31 10:03:36.370: INFO: Got endpoints: latency-svc-hvknw [748.793535ms]
Oct 31 10:03:36.392: INFO: Created: latency-svc-685zr
Oct 31 10:03:36.421: INFO: Got endpoints: latency-svc-9nvtn [749.489073ms]
Oct 31 10:03:36.442: INFO: Created: latency-svc-db9f5
Oct 31 10:03:36.471: INFO: Got endpoints: latency-svc-fxpth [750.080788ms]
Oct 31 10:03:36.492: INFO: Created: latency-svc-tbgb7
Oct 31 10:03:36.522: INFO: Got endpoints: latency-svc-xfvpl [750.926445ms]
Oct 31 10:03:36.545: INFO: Created: latency-svc-wd5zn
Oct 31 10:03:36.572: INFO: Got endpoints: latency-svc-j22z7 [750.23102ms]
Oct 31 10:03:36.593: INFO: Created: latency-svc-nd4cx
Oct 31 10:03:36.620: INFO: Got endpoints: latency-svc-hxwl4 [748.726429ms]
Oct 31 10:03:36.642: INFO: Created: latency-svc-kp9lc
Oct 31 10:03:36.671: INFO: Got endpoints: latency-svc-wsxrf [749.922203ms]
Oct 31 10:03:36.692: INFO: Created: latency-svc-8npkk
Oct 31 10:03:36.720: INFO: Got endpoints: latency-svc-zpjcl [748.279004ms]
Oct 31 10:03:36.739: INFO: Created: latency-svc-gl7pd
Oct 31 10:03:36.774: INFO: Got endpoints: latency-svc-tvv4b [751.99366ms]
Oct 31 10:03:36.800: INFO: Created: latency-svc-6c45t
Oct 31 10:03:36.820: INFO: Got endpoints: latency-svc-tq24x [747.433496ms]
Oct 31 10:03:36.840: INFO: Created: latency-svc-7dhcz
Oct 31 10:03:36.871: INFO: Got endpoints: latency-svc-n6r6n [750.313521ms]
Oct 31 10:03:36.892: INFO: Created: latency-svc-mz7sn
Oct 31 10:03:36.921: INFO: Got endpoints: latency-svc-5s6vx [749.517287ms]
Oct 31 10:03:36.943: INFO: Created: latency-svc-d6qx9
Oct 31 10:03:36.970: INFO: Got endpoints: latency-svc-hrbzd [748.859967ms]
Oct 31 10:03:36.992: INFO: Created: latency-svc-727tt
Oct 31 10:03:37.021: INFO: Got endpoints: latency-svc-vg6ft [750.680337ms]
Oct 31 10:03:37.042: INFO: Created: latency-svc-q2hv9
Oct 31 10:03:37.070: INFO: Got endpoints: latency-svc-rj2mv [750.547602ms]
Oct 31 10:03:37.089: INFO: Created: latency-svc-bj25q
Oct 31 10:03:37.120: INFO: Got endpoints: latency-svc-685zr [749.792193ms]
Oct 31 10:03:37.142: INFO: Created: latency-svc-rfllw
Oct 31 10:03:37.170: INFO: Got endpoints: latency-svc-db9f5 [749.75161ms]
Oct 31 10:03:37.190: INFO: Created: latency-svc-6qh5b
Oct 31 10:03:37.222: INFO: Got endpoints: latency-svc-tbgb7 [750.694326ms]
Oct 31 10:03:37.244: INFO: Created: latency-svc-wx8g2
Oct 31 10:03:37.270: INFO: Got endpoints: latency-svc-wd5zn [748.24372ms]
Oct 31 10:03:37.291: INFO: Created: latency-svc-tvj56
Oct 31 10:03:37.320: INFO: Got endpoints: latency-svc-nd4cx [748.412174ms]
Oct 31 10:03:37.343: INFO: Created: latency-svc-xnnwp
Oct 31 10:03:37.372: INFO: Got endpoints: latency-svc-kp9lc [751.886873ms]
Oct 31 10:03:37.393: INFO: Created: latency-svc-vzlj4
Oct 31 10:03:37.421: INFO: Got endpoints: latency-svc-8npkk [749.943077ms]
Oct 31 10:03:37.441: INFO: Created: latency-svc-hz252
Oct 31 10:03:37.470: INFO: Got endpoints: latency-svc-gl7pd [750.291276ms]
Oct 31 10:03:37.492: INFO: Created: latency-svc-2pkd7
Oct 31 10:03:37.520: INFO: Got endpoints: latency-svc-6c45t [746.763934ms]
Oct 31 10:03:37.539: INFO: Created: latency-svc-2fxcg
Oct 31 10:03:37.571: INFO: Got endpoints: latency-svc-7dhcz [750.615913ms]
Oct 31 10:03:37.591: INFO: Created: latency-svc-q2nnp
Oct 31 10:03:37.621: INFO: Got endpoints: latency-svc-mz7sn [750.346473ms]
Oct 31 10:03:37.643: INFO: Created: latency-svc-x4fnx
Oct 31 10:03:37.671: INFO: Got endpoints: latency-svc-d6qx9 [750.231524ms]
Oct 31 10:03:37.693: INFO: Created: latency-svc-rk6d7
Oct 31 10:03:37.721: INFO: Got endpoints: latency-svc-727tt [750.046414ms]
Oct 31 10:03:37.742: INFO: Created: latency-svc-zpqks
Oct 31 10:03:37.771: INFO: Got endpoints: latency-svc-q2hv9 [749.271809ms]
Oct 31 10:03:37.791: INFO: Created: latency-svc-hf7fn
Oct 31 10:03:37.820: INFO: Got endpoints: latency-svc-bj25q [749.99045ms]
Oct 31 10:03:37.840: INFO: Created: latency-svc-jmlkl
Oct 31 10:03:37.871: INFO: Got endpoints: latency-svc-rfllw [750.665801ms]
Oct 31 10:03:37.893: INFO: Created: latency-svc-4cl6s
Oct 31 10:03:37.922: INFO: Got endpoints: latency-svc-6qh5b [752.008415ms]
Oct 31 10:03:37.942: INFO: Created: latency-svc-qnt47
Oct 31 10:03:37.971: INFO: Got endpoints: latency-svc-wx8g2 [748.951691ms]
Oct 31 10:03:37.991: INFO: Created: latency-svc-ll97v
Oct 31 10:03:38.020: INFO: Got endpoints: latency-svc-tvj56 [749.944465ms]
Oct 31 10:03:38.046: INFO: Created: latency-svc-m4ssp
Oct 31 10:03:38.070: INFO: Got endpoints: latency-svc-xnnwp [749.882074ms]
Oct 31 10:03:38.093: INFO: Created: latency-svc-qbk9v
Oct 31 10:03:38.122: INFO: Got endpoints: latency-svc-vzlj4 [749.806105ms]
Oct 31 10:03:38.143: INFO: Created: latency-svc-jwbxh
Oct 31 10:03:38.171: INFO: Got endpoints: latency-svc-hz252 [750.20749ms]
Oct 31 10:03:38.191: INFO: Created: latency-svc-vxzp9
Oct 31 10:03:38.220: INFO: Got endpoints: latency-svc-2pkd7 [749.476639ms]
Oct 31 10:03:38.241: INFO: Created: latency-svc-mcg2n
Oct 31 10:03:38.272: INFO: Got endpoints: latency-svc-2fxcg [751.384366ms]
Oct 31 10:03:38.292: INFO: Created: latency-svc-b9qrj
Oct 31 10:03:38.321: INFO: Got endpoints: latency-svc-q2nnp [750.701889ms]
Oct 31 10:03:38.343: INFO: Created: latency-svc-wml9j
Oct 31 10:03:38.369: INFO: Got endpoints: latency-svc-x4fnx [748.14233ms]
Oct 31 10:03:38.391: INFO: Created: latency-svc-m282s
Oct 31 10:03:38.422: INFO: Got endpoints: latency-svc-rk6d7 [750.275291ms]
Oct 31 10:03:38.442: INFO: Created: latency-svc-whvl6
Oct 31 10:03:38.471: INFO: Got endpoints: latency-svc-zpqks [750.442097ms]
Oct 31 10:03:38.493: INFO: Created: latency-svc-s5h4r
Oct 31 10:03:38.522: INFO: Got endpoints: latency-svc-hf7fn [750.856218ms]
Oct 31 10:03:38.541: INFO: Created: latency-svc-cjhzp
Oct 31 10:03:38.570: INFO: Got endpoints: latency-svc-jmlkl [750.113586ms]
Oct 31 10:03:38.592: INFO: Created: latency-svc-bp6lk
Oct 31 10:03:38.621: INFO: Got endpoints: latency-svc-4cl6s [750.345869ms]
Oct 31 10:03:38.644: INFO: Created: latency-svc-x6pt7
Oct 31 10:03:38.671: INFO: Got endpoints: latency-svc-qnt47 [748.203257ms]
Oct 31 10:03:38.692: INFO: Created: latency-svc-qh6x2
Oct 31 10:03:38.720: INFO: Got endpoints: latency-svc-ll97v [748.838927ms]
Oct 31 10:03:38.739: INFO: Created: latency-svc-vpxfn
Oct 31 10:03:38.772: INFO: Got endpoints: latency-svc-m4ssp [752.013639ms]
Oct 31 10:03:38.797: INFO: Created: latency-svc-swvjp
Oct 31 10:03:38.821: INFO: Got endpoints: latency-svc-qbk9v [750.569265ms]
Oct 31 10:03:38.841: INFO: Created: latency-svc-lqrcf
Oct 31 10:03:38.871: INFO: Got endpoints: latency-svc-jwbxh [749.827019ms]
Oct 31 10:03:38.899: INFO: Created: latency-svc-xhkk2
Oct 31 10:03:38.922: INFO: Got endpoints: latency-svc-vxzp9 [750.322156ms]
Oct 31 10:03:38.944: INFO: Created: latency-svc-tlk7h
Oct 31 10:03:38.972: INFO: Got endpoints: latency-svc-mcg2n [751.818548ms]
Oct 31 10:03:38.990: INFO: Created: latency-svc-ptnqx
Oct 31 10:03:39.020: INFO: Got endpoints: latency-svc-b9qrj [748.300112ms]
Oct 31 10:03:39.044: INFO: Created: latency-svc-zr2gz
Oct 31 10:03:39.070: INFO: Got endpoints: latency-svc-wml9j [748.776927ms]
Oct 31 10:03:39.091: INFO: Created: latency-svc-582wr
Oct 31 10:03:39.120: INFO: Got endpoints: latency-svc-m282s [750.975232ms]
Oct 31 10:03:39.141: INFO: Created: latency-svc-x8f2j
Oct 31 10:03:39.170: INFO: Got endpoints: latency-svc-whvl6 [747.891188ms]
Oct 31 10:03:39.191: INFO: Created: latency-svc-fc4xd
Oct 31 10:03:39.221: INFO: Got endpoints: latency-svc-s5h4r [749.884906ms]
Oct 31 10:03:39.240: INFO: Created: latency-svc-l446x
Oct 31 10:03:39.271: INFO: Got endpoints: latency-svc-cjhzp [749.058626ms]
Oct 31 10:03:39.293: INFO: Created: latency-svc-6kzls
Oct 31 10:03:39.321: INFO: Got endpoints: latency-svc-bp6lk [751.083579ms]
Oct 31 10:03:39.344: INFO: Created: latency-svc-ksgss
Oct 31 10:03:39.371: INFO: Got endpoints: latency-svc-x6pt7 [749.736982ms]
Oct 31 10:03:39.393: INFO: Created: latency-svc-9h8v4
Oct 31 10:03:39.420: INFO: Got endpoints: latency-svc-qh6x2 [749.309503ms]
Oct 31 10:03:39.442: INFO: Created: latency-svc-6bmn6
Oct 31 10:03:39.470: INFO: Got endpoints: latency-svc-vpxfn [749.962357ms]
Oct 31 10:03:39.521: INFO: Got endpoints: latency-svc-swvjp [748.481729ms]
Oct 31 10:03:39.570: INFO: Got endpoints: latency-svc-lqrcf [749.585573ms]
Oct 31 10:03:39.620: INFO: Got endpoints: latency-svc-xhkk2 [749.074339ms]
Oct 31 10:03:39.670: INFO: Got endpoints: latency-svc-tlk7h [748.5771ms]
Oct 31 10:03:39.720: INFO: Got endpoints: latency-svc-ptnqx [747.909045ms]
Oct 31 10:03:39.770: INFO: Got endpoints: latency-svc-zr2gz [749.724245ms]
Oct 31 10:03:39.820: INFO: Got endpoints: latency-svc-582wr [750.139804ms]
Oct 31 10:03:39.870: INFO: Got endpoints: latency-svc-x8f2j [749.814184ms]
Oct 31 10:03:39.923: INFO: Got endpoints: latency-svc-fc4xd [753.310543ms]
Oct 31 10:03:39.973: INFO: Got endpoints: latency-svc-l446x [752.292145ms]
Oct 31 10:03:40.021: INFO: Got endpoints: latency-svc-6kzls [750.017746ms]
Oct 31 10:03:40.072: INFO: Got endpoints: latency-svc-ksgss [750.110764ms]
Oct 31 10:03:40.120: INFO: Got endpoints: latency-svc-9h8v4 [749.152541ms]
Oct 31 10:03:40.172: INFO: Got endpoints: latency-svc-6bmn6 [751.615386ms]
Oct 31 10:03:40.172: INFO: Latencies: [34.802346ms 87.821482ms 92.078566ms 95.891318ms 99.804252ms 99.819229ms 100.786204ms 100.994934ms 101.405584ms 101.700255ms 102.774426ms 104.463362ms 107.603006ms 112.070367ms 112.30699ms 115.380825ms 124.918419ms 126.536435ms 129.935308ms 131.845067ms 151.669489ms 157.95439ms 159.14138ms 173.635871ms 179.105407ms 186.580105ms 187.915263ms 216.222322ms 233.929695ms 242.444635ms 249.075152ms 255.710046ms 257.847205ms 260.929277ms 270.116086ms 278.064433ms 296.716054ms 302.703363ms 303.272332ms 310.981515ms 327.823857ms 350.033945ms 354.018643ms 391.390132ms 395.061505ms 419.311792ms 434.236848ms 449.84325ms 458.023741ms 467.653012ms 481.934113ms 484.422977ms 523.977676ms 574.509823ms 601.387443ms 651.954688ms 697.405111ms 741.408798ms 742.998632ms 744.810179ms 746.592072ms 746.763934ms 747.433496ms 747.679604ms 747.891188ms 747.909045ms 748.14233ms 748.203257ms 748.24372ms 748.279004ms 748.300112ms 748.332593ms 748.356574ms 748.412174ms 748.481729ms 748.520351ms 748.5207ms 748.5771ms 748.58011ms 748.630019ms 748.726429ms 748.726639ms 748.729291ms 748.776927ms 748.793535ms 748.838927ms 748.859967ms 748.921639ms 748.951691ms 749.051853ms 749.058626ms 749.068445ms 749.074339ms 749.152541ms 749.159298ms 749.18153ms 749.271809ms 749.309503ms 749.340552ms 749.440514ms 749.457924ms 749.476639ms 749.489073ms 749.513289ms 749.517287ms 749.585573ms 749.59169ms 749.696522ms 749.724245ms 749.736982ms 749.738726ms 749.75161ms 749.792193ms 749.806105ms 749.814184ms 749.827019ms 749.877908ms 749.882074ms 749.884906ms 749.894005ms 749.922197ms 749.922203ms 749.943077ms 749.944465ms 749.962357ms 749.976501ms 749.99045ms 750.017746ms 750.046414ms 750.058361ms 750.062406ms 750.080788ms 750.108587ms 750.110764ms 750.113586ms 750.126907ms 750.139804ms 750.196231ms 750.20749ms 750.210255ms 750.23102ms 750.231404ms 750.231524ms 750.275291ms 750.291276ms 750.309037ms 750.313521ms 750.322156ms 750.345869ms 750.346473ms 750.418503ms 750.442097ms 750.494308ms 750.547602ms 750.569265ms 750.615913ms 750.635791ms 750.665801ms 750.680337ms 750.694326ms 750.701889ms 750.712912ms 750.716339ms 750.748046ms 750.770555ms 750.813839ms 750.856218ms 750.898082ms 750.919001ms 750.926445ms 750.938495ms 750.951938ms 750.95387ms 750.975232ms 751.014925ms 751.023355ms 751.083579ms 751.111919ms 751.134589ms 751.138422ms 751.14803ms 751.284631ms 751.300099ms 751.384366ms 751.548458ms 751.571177ms 751.615386ms 751.643524ms 751.818548ms 751.886873ms 751.92035ms 751.99366ms 752.000869ms 752.008415ms 752.013639ms 752.292145ms 752.907749ms 753.092156ms 753.310543ms 756.450878ms]
Oct 31 10:03:40.172: INFO: 50 %ile: 749.457924ms
Oct 31 10:03:40.172: INFO: 90 %ile: 751.14803ms
Oct 31 10:03:40.172: INFO: 99 %ile: 753.310543ms
Oct 31 10:03:40.172: INFO: Total sample count: 200
[AfterEach] [k8s.io] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:03:40.172: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-qz7br" for this suite.
Oct 31 10:03:53.517: INFO: namespace: e2e-tests-svc-latency-qz7br, resource: bindings, ignored listing per whitelist
Oct 31 10:03:53.532: INFO: namespace e2e-tests-svc-latency-qz7br deletion completed in 13.331068983s

• [SLOW TEST:24.102 seconds]
[k8s.io] Service endpoints latency
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service_latency.go:117
------------------------------
SSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:114
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:03:53.532: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:114
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 31 10:03:53.672: INFO: Waiting up to 5m0s for pod pod-cd0ea724-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:03:53.687: INFO: Waiting for pod pod-cd0ea724-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-xcsrg' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.452103ms elapsed)
STEP: Saw pod success
Oct 31 10:03:55.717: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-cd0ea724-be22-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:03:55.759: INFO: Waiting for pod pod-cd0ea724-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:03:55.773: INFO: Pod pod-cd0ea724-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:03:55.773: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xcsrg" for this suite.
Oct 31 10:04:02.972: INFO: namespace: e2e-tests-emptydir-xcsrg, resource: bindings, ignored listing per whitelist
Oct 31 10:04:03.103: INFO: namespace e2e-tests-emptydir-xcsrg deletion completed in 7.300240394s

• [SLOW TEST:9.571 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:114
------------------------------
SSS
------------------------------
[k8s.io] Services 
  should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:131
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:04:03.104: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:52
[It] should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:131
STEP: creating service endpoint-test2 in namespace e2e-tests-services-spwmz
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-spwmz to expose endpoints map[]
Oct 31 10:04:03.287: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-spwmz exposes endpoints map[] (18.016544ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-spwmz
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-spwmz to expose endpoints map[pod1:[80]]
Oct 31 10:04:05.412: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-spwmz exposes endpoints map[pod1:[80]] (2.0913736s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-spwmz
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-spwmz to expose endpoints map[pod1:[80] pod2:[80]]
Oct 31 10:04:07.578: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-spwmz exposes endpoints map[pod2:[80] pod1:[80]] (2.135533792s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-spwmz
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-spwmz to expose endpoints map[pod2:[80]]
Oct 31 10:04:08.653: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-spwmz exposes endpoints map[pod2:[80]] (1.059448205s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-spwmz
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-spwmz to expose endpoints map[]
Oct 31 10:04:10.716: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-spwmz exposes endpoints map[] (2.045230651s elapsed)
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:04:10.746: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-spwmz" for this suite.
Oct 31 10:04:33.840: INFO: namespace: e2e-tests-services-spwmz, resource: bindings, ignored listing per whitelist
Oct 31 10:04:34.074: INFO: namespace e2e-tests-services-spwmz deletion completed in 23.299431847s
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:64

• [SLOW TEST:30.970 seconds]
[k8s.io] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:131
------------------------------
SS
------------------------------
[k8s.io] Proxy version v1 
  should proxy logs on node [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:63
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:04:34.074: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:63
Oct 31 10:04:34.250: INFO: (0) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.885115ms)
Oct 31 10:04:34.267: INFO: (1) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.421385ms)
Oct 31 10:04:34.283: INFO: (2) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.830437ms)
Oct 31 10:04:34.299: INFO: (3) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.028262ms)
Oct 31 10:04:34.315: INFO: (4) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.449184ms)
Oct 31 10:04:34.331: INFO: (5) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.907979ms)
Oct 31 10:04:34.347: INFO: (6) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.785234ms)
Oct 31 10:04:34.363: INFO: (7) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.936995ms)
Oct 31 10:04:34.379: INFO: (8) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.843931ms)
Oct 31 10:04:34.395: INFO: (9) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.065093ms)
Oct 31 10:04:34.412: INFO: (10) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.719632ms)
Oct 31 10:04:34.429: INFO: (11) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.995385ms)
Oct 31 10:04:34.445: INFO: (12) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.760202ms)
Oct 31 10:04:34.462: INFO: (13) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.235247ms)
Oct 31 10:04:34.478: INFO: (14) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.039196ms)
Oct 31 10:04:34.494: INFO: (15) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.798628ms)
Oct 31 10:04:34.510: INFO: (16) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.992053ms)
Oct 31 10:04:34.526: INFO: (17) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.864544ms)
Oct 31 10:04:34.542: INFO: (18) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.068503ms)
Oct 31 10:04:34.558: INFO: (19) /api/v1/proxy/nodes/ci-prtest-cc63063-94-ig-n-2mxx/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.8364ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:04:34.558: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-j59x9" for this suite.
Oct 31 10:04:41.611: INFO: namespace: e2e-tests-proxy-j59x9, resource: bindings, ignored listing per whitelist
Oct 31 10:04:41.895: INFO: namespace e2e-tests-proxy-j59x9 deletion completed in 7.307919737s

• [SLOW TEST:7.821 seconds]
[k8s.io] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:275
    should proxy logs on node [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:63
------------------------------
SS
------------------------------
[k8s.io] ConfigMap 
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:421
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:04:41.895: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:421
STEP: Creating configMap e2e-tests-configmap-phw9s/configmap-test-e9e3254d-be22-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:04:42.058: INFO: Waiting up to 5m0s for pod pod-configmaps-e9e596c6-be22-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:04:42.072: INFO: Waiting for pod pod-configmaps-e9e596c6-be22-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-phw9s' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.733308ms elapsed)
STEP: Saw pod success
Oct 31 10:04:44.104: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-configmaps-e9e596c6-be22-11e7-9459-0e11a30959be container env-test: <nil>
STEP: delete the pod
Oct 31 10:04:44.145: INFO: Waiting for pod pod-configmaps-e9e596c6-be22-11e7-9459-0e11a30959be to disappear
Oct 31 10:04:44.160: INFO: Pod pod-configmaps-e9e596c6-be22-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:04:44.160: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-phw9s" for this suite.
Oct 31 10:04:51.335: INFO: namespace: e2e-tests-configmap-phw9s, resource: bindings, ignored listing per whitelist
Oct 31 10:04:51.533: INFO: namespace e2e-tests-configmap-phw9s deletion completed in 7.342695825s

• [SLOW TEST:9.638 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:421
------------------------------
SS
------------------------------
[k8s.io] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:337
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:04:51.533: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should create and stop a working application [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:337
STEP: creating all guestbook components
Oct 31 10:04:51.677: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v4
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Oct 31 10:04:51.678: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:52.027: INFO: stderr: ""
Oct 31 10:04:52.027: INFO: stdout: "deployment \"frontend\" created\n"
Oct 31 10:04:52.027: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 31 10:04:52.027: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:52.279: INFO: stderr: ""
Oct 31 10:04:52.279: INFO: stdout: "service \"frontend\" created\n"
Oct 31 10:04:52.279: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/google_containers/redis:e2e  # or just image: redis
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 31 10:04:52.279: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:52.513: INFO: stderr: ""
Oct 31 10:04:52.513: INFO: stdout: "deployment \"redis-master\" created\n"
Oct 31 10:04:52.513: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Oct 31 10:04:52.514: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:52.770: INFO: stderr: ""
Oct 31 10:04:52.770: INFO: stdout: "service \"redis-master\" created\n"
Oct 31 10:04:52.771: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google_samples/gb-redisslave:v1
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Oct 31 10:04:52.771: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:53.005: INFO: stderr: ""
Oct 31 10:04:53.005: INFO: stdout: "deployment \"redis-slave\" created\n"
Oct 31 10:04:53.005: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Oct 31 10:04:53.005: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:04:53.260: INFO: stderr: ""
Oct 31 10:04:53.260: INFO: stdout: "service \"redis-slave\" created\n"
STEP: validating guestbook app
Oct 31 10:04:53.260: INFO: Waiting for all frontend pods to be Running.
Oct 31 10:05:13.261: INFO: Waiting for frontend to serve content.
Oct 31 10:05:16.317: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:168
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(97): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(58): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#2 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(94): Predis\Connection\StreamConnection-&gt;createResource()
#3 /usr/local/lib/php/Predis/Connection/StreamConnection.php(158): Predis\Connection\AbstractConnection-&gt;connect()
#4 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(193): Predis\Connection\StreamConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/StreamConnection.php(184): Predis\Connection\AbstractConnection-&gt;getResource()
#6 /usr/local/lib/php/Predis/Connection/StreamCo in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>168</b><br />

Oct 31 10:05:21.357: INFO: Trying to add a new entry to the guestbook.
Oct 31 10:05:23.037: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-master:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:168
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(97): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(58): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#2 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(94): Predis\Connection\StreamConnection-&gt;createResource()
#3 /usr/local/lib/php/Predis/Connection/StreamConnection.php(158): Predis\Connection\AbstractConnection-&gt;connect()
#4 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(193): Predis\Connection\StreamConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/StreamConnection.php(184): Predis\Connection\AbstractConnection-&gt;getResource()
#6 /usr/local/lib/php/Predis/Connection/StreamC in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>168</b><br />

Oct 31 10:05:28.077: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct 31 10:05:28.115: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:31.482: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:31.482: INFO: stdout: "deployment \"frontend\" deleted\n"
STEP: using delete to clean up resources
Oct 31 10:05:31.482: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:31.709: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:31.709: INFO: stdout: "service \"frontend\" deleted\n"
STEP: using delete to clean up resources
Oct 31 10:05:31.710: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:35.076: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:35.076: INFO: stdout: "deployment \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Oct 31 10:05:35.076: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:35.300: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:35.300: INFO: stdout: "service \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Oct 31 10:05:35.300: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:38.668: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:38.668: INFO: stdout: "deployment \"redis-slave\" deleted\n"
STEP: using delete to clean up resources
Oct 31 10:05:38.668: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4jctp'
Oct 31 10:05:38.891: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 31 10:05:38.891: INFO: stdout: "service \"redis-slave\" deleted\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:05:38.891: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4jctp" for this suite.
Oct 31 10:06:18.164: INFO: namespace: e2e-tests-kubectl-4jctp, resource: bindings, ignored listing per whitelist
Oct 31 10:06:18.270: INFO: namespace e2e-tests-kubectl-4jctp deletion completed in 39.348491639s

• [SLOW TEST:86.736 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Guestbook application
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create and stop a working application [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:337
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Projected 
  should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:917
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:06:18.270: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:917
STEP: Creating the pod
Oct 31 10:06:21.046: INFO: Successfully updated pod "annotationupdate23570548-be23-11e7-9459-0e11a30959be"
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:07:25.723: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8shtj" for this suite.
Oct 31 10:07:48.934: INFO: namespace: e2e-tests-projected-8shtj, resource: bindings, ignored listing per whitelist
Oct 31 10:07:49.084: INFO: namespace e2e-tests-projected-8shtj deletion completed in 23.344490832s

• [SLOW TEST:90.814 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:917
------------------------------
SSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:102
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:07:49.084: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:102
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 31 10:07:49.197: INFO: Waiting up to 5m0s for pod pod-5970fb19-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:07:49.212: INFO: Waiting for pod pod-5970fb19-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-xmtln' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.537037ms elapsed)
STEP: Saw pod success
Oct 31 10:07:51.244: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-5970fb19-be23-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:07:51.297: INFO: Waiting for pod pod-5970fb19-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:07:51.313: INFO: Pod pod-5970fb19-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:07:51.313: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xmtln" for this suite.
Oct 31 10:07:58.583: INFO: namespace: e2e-tests-emptydir-xmtln, resource: bindings, ignored listing per whitelist
Oct 31 10:07:58.687: INFO: namespace e2e-tests-emptydir-xmtln deletion completed in 7.344382316s

• [SLOW TEST:9.603 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0644,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:102
------------------------------
[k8s.io] Proxy version v1 
  should proxy through a service and a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:274
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:07:58.687: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:274
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ljw2f in namespace e2e-tests-proxy-9d75r
I1031 10:07:58.825228    7114 runners.go:176] Created replication controller with name: proxy-service-ljw2f, namespace: e2e-tests-proxy-9d75r, replica count: 1
I1031 10:07:59.825563    7114 runners.go:176] proxy-service-ljw2f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1031 10:08:00.825800    7114 runners.go:176] proxy-service-ljw2f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1031 10:08:01.826031    7114 runners.go:176] proxy-service-ljw2f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1031 10:08:02.826278    7114 runners.go:176] proxy-service-ljw2f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 31 10:08:02.841: INFO: setup took 4.065601372s, starting test cases
STEP: running 34 cases, 20 attempts per case, 680 total attempts
Oct 31 10:08:02.863: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 21.44686ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 25.502317ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 25.516392ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 25.852411ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 25.903297ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 25.645939ms)
Oct 31 10:08:02.867: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 26.080553ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 34.53095ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 34.264109ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 34.460651ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 34.64927ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 34.630952ms)
Oct 31 10:08:02.876: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.51159ms)
Oct 31 10:08:02.878: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 36.667219ms)
Oct 31 10:08:02.880: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 38.323644ms)
Oct 31 10:08:02.880: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 38.856139ms)
Oct 31 10:08:02.881: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 38.920724ms)
Oct 31 10:08:02.882: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 40.177441ms)
Oct 31 10:08:02.882: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 40.143727ms)
Oct 31 10:08:02.882: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 39.970824ms)
Oct 31 10:08:02.882: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 40.147247ms)
Oct 31 10:08:02.882: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 40.688826ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 49.294081ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 49.377397ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 49.62396ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 49.696737ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 49.581374ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 49.296867ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 49.576208ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 49.410796ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 49.3712ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 49.328758ms)
Oct 31 10:08:02.891: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 49.906143ms)
Oct 31 10:08:02.893: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 51.080858ms)
Oct 31 10:08:02.917: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 24.048901ms)
Oct 31 10:08:02.918: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 24.686713ms)
Oct 31 10:08:02.918: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 25.106651ms)
Oct 31 10:08:02.918: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 24.855517ms)
Oct 31 10:08:02.919: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 25.868504ms)
Oct 31 10:08:02.919: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 25.986402ms)
Oct 31 10:08:02.919: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 25.777399ms)
Oct 31 10:08:02.919: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 25.82494ms)
Oct 31 10:08:02.920: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 26.830216ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 27.377704ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 27.237885ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 27.421415ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 27.470668ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 27.864176ms)
Oct 31 10:08:02.921: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 27.918831ms)
Oct 31 10:08:02.926: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 32.636936ms)
Oct 31 10:08:02.926: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 32.986783ms)
Oct 31 10:08:02.926: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 33.199522ms)
Oct 31 10:08:02.926: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.141445ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 37.246588ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 37.483783ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 37.379259ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 37.751989ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 37.375547ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 37.720378ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 37.470579ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 37.423878ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 37.810766ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 37.458616ms)
Oct 31 10:08:02.931: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 38.293183ms)
Oct 31 10:08:02.932: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 38.56215ms)
Oct 31 10:08:02.932: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 39.188879ms)
Oct 31 10:08:02.932: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 39.242906ms)
Oct 31 10:08:02.933: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 39.256824ms)
Oct 31 10:08:02.962: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 29.09978ms)
Oct 31 10:08:02.963: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 29.528994ms)
Oct 31 10:08:02.963: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 29.971231ms)
Oct 31 10:08:02.963: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 30.028916ms)
Oct 31 10:08:02.963: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 29.885488ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 31.915844ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 32.445836ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 32.081638ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 32.420756ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 32.732025ms)
Oct 31 10:08:02.965: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 32.636417ms)
Oct 31 10:08:02.966: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 33.622775ms)
Oct 31 10:08:02.967: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 33.983434ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 34.619547ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 34.707712ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.948229ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 35.26642ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 34.838292ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 35.261462ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 35.33512ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 35.173223ms)
Oct 31 10:08:02.968: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 35.372997ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 35.550305ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 35.799763ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 35.809562ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.353024ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 35.9266ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 35.84982ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 36.469193ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 35.910477ms)
Oct 31 10:08:02.969: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 36.486838ms)
Oct 31 10:08:02.970: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 36.758397ms)
Oct 31 10:08:02.970: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 36.447212ms)
Oct 31 10:08:02.970: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 36.626212ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 21.912703ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 22.312269ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 22.303479ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 22.128686ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 22.208542ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 22.392128ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 22.290115ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 22.057704ms)
Oct 31 10:08:02.992: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 22.195031ms)
Oct 31 10:08:03.003: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 32.7483ms)
Oct 31 10:08:03.003: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 32.779672ms)
Oct 31 10:08:03.003: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 32.825676ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 33.386997ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 33.606246ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 33.661485ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 33.635031ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 33.770851ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 34.308749ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 34.381496ms)
Oct 31 10:08:03.004: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 34.216106ms)
Oct 31 10:08:03.005: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 34.749097ms)
Oct 31 10:08:03.005: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 34.812935ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 38.839421ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 38.970489ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 39.329476ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 39.168185ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 39.416914ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 39.55936ms)
Oct 31 10:08:03.009: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 39.234418ms)
Oct 31 10:08:03.010: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 39.948306ms)
Oct 31 10:08:03.012: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 41.813795ms)
Oct 31 10:08:03.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 41.534435ms)
Oct 31 10:08:03.012: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 41.899103ms)
Oct 31 10:08:03.012: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 41.629016ms)
Oct 31 10:08:03.035: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 23.283489ms)
Oct 31 10:08:03.038: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 25.754885ms)
Oct 31 10:08:03.039: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 26.754424ms)
Oct 31 10:08:03.039: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 26.746679ms)
Oct 31 10:08:03.039: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 26.954902ms)
Oct 31 10:08:03.039: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 26.7591ms)
Oct 31 10:08:03.039: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 26.675759ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 29.56558ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 29.448774ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 30.019377ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 29.999793ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 30.097647ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 29.917208ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 30.203871ms)
Oct 31 10:08:03.042: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 29.997714ms)
Oct 31 10:08:03.043: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 30.68375ms)
Oct 31 10:08:03.044: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 31.401898ms)
Oct 31 10:08:03.044: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 31.727822ms)
Oct 31 10:08:03.044: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 32.076767ms)
Oct 31 10:08:03.044: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 32.094628ms)
Oct 31 10:08:03.045: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 32.965379ms)
Oct 31 10:08:03.045: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 33.149343ms)
Oct 31 10:08:03.045: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 33.223265ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.427954ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.162952ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 33.517237ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 33.401824ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 33.67037ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 33.94959ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 34.416658ms)
Oct 31 10:08:03.046: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 34.034001ms)
Oct 31 10:08:03.047: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 34.302482ms)
Oct 31 10:08:03.047: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 34.166557ms)
Oct 31 10:08:03.047: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.496946ms)
Oct 31 10:08:03.072: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 25.666282ms)
Oct 31 10:08:03.073: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 25.807391ms)
Oct 31 10:08:03.073: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 25.739454ms)
Oct 31 10:08:03.074: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 26.976147ms)
Oct 31 10:08:03.074: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 27.481569ms)
Oct 31 10:08:03.074: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 27.195599ms)
Oct 31 10:08:03.075: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 28.25752ms)
Oct 31 10:08:03.075: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 28.583387ms)
Oct 31 10:08:03.075: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 28.897608ms)
Oct 31 10:08:03.076: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 28.811094ms)
Oct 31 10:08:03.076: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 28.982843ms)
Oct 31 10:08:03.076: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 29.003084ms)
Oct 31 10:08:03.076: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 29.052369ms)
Oct 31 10:08:03.077: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 29.536369ms)
Oct 31 10:08:03.077: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 29.812335ms)
Oct 31 10:08:03.078: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 31.218001ms)
Oct 31 10:08:03.079: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 31.612563ms)
Oct 31 10:08:03.079: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 31.879461ms)
Oct 31 10:08:03.079: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 31.970439ms)
Oct 31 10:08:03.079: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 32.407637ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 34.520958ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 35.211951ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 34.968309ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 35.004209ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 34.889676ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 35.12522ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 35.156394ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 35.276848ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 35.178168ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 34.989911ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 35.302393ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 35.345238ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 35.359841ms)
Oct 31 10:08:03.082: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.098709ms)
Oct 31 10:08:03.105: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 22.536119ms)
Oct 31 10:08:03.105: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 23.165636ms)
Oct 31 10:08:03.105: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 23.012487ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 24.290135ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 24.782302ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 24.578212ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 24.710275ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 24.800141ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 24.976708ms)
Oct 31 10:08:03.107: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 25.244472ms)
Oct 31 10:08:03.108: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 25.218608ms)
Oct 31 10:08:03.108: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 25.476871ms)
Oct 31 10:08:03.108: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 25.177128ms)
Oct 31 10:08:03.108: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 25.713263ms)
Oct 31 10:08:03.108: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 25.717146ms)
Oct 31 10:08:03.112: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 29.370263ms)
Oct 31 10:08:03.113: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 30.469199ms)
Oct 31 10:08:03.114: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 31.317958ms)
Oct 31 10:08:03.114: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 31.555783ms)
Oct 31 10:08:03.114: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 31.701462ms)
Oct 31 10:08:03.114: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 32.009512ms)
Oct 31 10:08:03.114: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 31.933292ms)
Oct 31 10:08:03.117: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 34.686951ms)
Oct 31 10:08:03.117: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 35.167671ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 35.370851ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 35.185691ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 35.085808ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 35.400462ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.39434ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 35.330776ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 35.628006ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 35.944421ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 35.779076ms)
Oct 31 10:08:03.118: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 36.141091ms)
Oct 31 10:08:03.136: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 17.32192ms)
Oct 31 10:08:03.136: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 17.315643ms)
Oct 31 10:08:03.136: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 17.74811ms)
Oct 31 10:08:03.137: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 18.253997ms)
Oct 31 10:08:03.143: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 24.220954ms)
Oct 31 10:08:03.144: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 24.955337ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 25.705429ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 24.905133ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 25.456456ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 25.431383ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 25.345366ms)
Oct 31 10:08:03.145: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 25.555348ms)
Oct 31 10:08:03.147: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 28.061681ms)
Oct 31 10:08:03.147: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 27.334554ms)
Oct 31 10:08:03.147: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 28.398097ms)
Oct 31 10:08:03.147: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 28.449131ms)
Oct 31 10:08:03.147: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 28.951837ms)
Oct 31 10:08:03.148: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 28.040317ms)
Oct 31 10:08:03.148: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 29.304908ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 28.884391ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 28.70265ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 29.918546ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 29.444692ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 30.057901ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 30.57889ms)
Oct 31 10:08:03.149: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 29.496808ms)
Oct 31 10:08:03.151: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 32.035375ms)
Oct 31 10:08:03.152: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 32.467166ms)
Oct 31 10:08:03.153: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 33.516393ms)
Oct 31 10:08:03.153: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 33.94492ms)
Oct 31 10:08:03.154: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.006396ms)
Oct 31 10:08:03.154: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 34.920964ms)
Oct 31 10:08:03.155: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 35.173118ms)
Oct 31 10:08:03.155: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 35.712557ms)
Oct 31 10:08:03.175: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 20.006576ms)
Oct 31 10:08:03.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 20.243725ms)
Oct 31 10:08:03.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 20.28201ms)
Oct 31 10:08:03.176: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 20.651592ms)
Oct 31 10:08:03.176: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 21.152018ms)
Oct 31 10:08:03.177: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 21.515986ms)
Oct 31 10:08:03.177: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 21.437664ms)
Oct 31 10:08:03.178: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 22.272831ms)
Oct 31 10:08:03.178: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 22.570277ms)
Oct 31 10:08:03.178: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 22.634708ms)
Oct 31 10:08:03.178: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 22.62459ms)
Oct 31 10:08:03.186: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 30.542551ms)
Oct 31 10:08:03.186: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 30.710693ms)
Oct 31 10:08:03.186: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 30.836797ms)
Oct 31 10:08:03.186: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 31.253748ms)
Oct 31 10:08:03.186: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 31.164771ms)
Oct 31 10:08:03.187: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 32.036343ms)
Oct 31 10:08:03.187: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 31.993632ms)
Oct 31 10:08:03.188: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 32.403239ms)
Oct 31 10:08:03.189: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 33.918896ms)
Oct 31 10:08:03.189: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.090524ms)
Oct 31 10:08:03.189: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 34.104291ms)
Oct 31 10:08:03.190: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 34.225072ms)
Oct 31 10:08:03.190: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 34.538963ms)
Oct 31 10:08:03.191: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 35.776749ms)
Oct 31 10:08:03.191: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 35.883366ms)
Oct 31 10:08:03.191: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 36.252886ms)
Oct 31 10:08:03.192: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 36.395887ms)
Oct 31 10:08:03.192: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 36.721084ms)
Oct 31 10:08:03.192: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 36.892095ms)
Oct 31 10:08:03.192: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 36.822798ms)
Oct 31 10:08:03.193: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 37.516846ms)
Oct 31 10:08:03.193: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 37.489238ms)
Oct 31 10:08:03.194: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 38.230425ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 24.913368ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 24.962353ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 25.230834ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 25.310538ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 25.281668ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 25.540326ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 25.277543ms)
Oct 31 10:08:03.219: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 25.279603ms)
Oct 31 10:08:03.220: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 26.503786ms)
Oct 31 10:08:03.220: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 26.13938ms)
Oct 31 10:08:03.221: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 26.675671ms)
Oct 31 10:08:03.221: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 27.044226ms)
Oct 31 10:08:03.222: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 27.993189ms)
Oct 31 10:08:03.222: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 28.556403ms)
Oct 31 10:08:03.222: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 28.273497ms)
Oct 31 10:08:03.226: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 31.92122ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 32.450523ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 32.940179ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 32.670897ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 32.854011ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 33.142213ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 32.876113ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 33.379591ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 33.021478ms)
Oct 31 10:08:03.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 33.185842ms)
Oct 31 10:08:03.228: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.607068ms)
Oct 31 10:08:03.228: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 34.16158ms)
Oct 31 10:08:03.228: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.088118ms)
Oct 31 10:08:03.228: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.024353ms)
Oct 31 10:08:03.228: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 34.113401ms)
Oct 31 10:08:03.229: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 34.219573ms)
Oct 31 10:08:03.229: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 34.403417ms)
Oct 31 10:08:03.230: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 35.363144ms)
Oct 31 10:08:03.230: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 35.816364ms)
Oct 31 10:08:03.247: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 17.053465ms)
Oct 31 10:08:03.247: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 17.347792ms)
Oct 31 10:08:03.252: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 22.077597ms)
Oct 31 10:08:03.258: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 27.90518ms)
Oct 31 10:08:03.259: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 29.158253ms)
Oct 31 10:08:03.259: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 29.042084ms)
Oct 31 10:08:03.259: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 29.293174ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 31.545228ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 31.608441ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 31.90692ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 31.909823ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 32.403594ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 32.641852ms)
Oct 31 10:08:03.262: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 32.350045ms)
Oct 31 10:08:03.263: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 33.240375ms)
Oct 31 10:08:03.263: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 33.258538ms)
Oct 31 10:08:03.263: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 33.420204ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 33.824203ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 33.890527ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 34.161126ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 34.333304ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 34.220885ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 34.301999ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.224179ms)
Oct 31 10:08:03.264: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 34.567433ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 34.666933ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 34.60569ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 34.840917ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 34.908137ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 34.657366ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.680479ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 34.951122ms)
Oct 31 10:08:03.265: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 35.331389ms)
Oct 31 10:08:03.266: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 35.673362ms)
Oct 31 10:08:03.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 22.132311ms)
Oct 31 10:08:03.292: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 26.328164ms)
Oct 31 10:08:03.293: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 26.797467ms)
Oct 31 10:08:03.293: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 27.653336ms)
Oct 31 10:08:03.295: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 29.655445ms)
Oct 31 10:08:03.295: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 29.703066ms)
Oct 31 10:08:03.295: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 29.272297ms)
Oct 31 10:08:03.295: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 29.700546ms)
Oct 31 10:08:03.296: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 29.457401ms)
Oct 31 10:08:03.295: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 29.618052ms)
Oct 31 10:08:03.296: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 29.748046ms)
Oct 31 10:08:03.296: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 29.947796ms)
Oct 31 10:08:03.296: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 29.944518ms)
Oct 31 10:08:03.297: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 30.761437ms)
Oct 31 10:08:03.297: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 31.139564ms)
Oct 31 10:08:03.297: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 31.277033ms)
Oct 31 10:08:03.298: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 31.887537ms)
Oct 31 10:08:03.298: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 31.836168ms)
Oct 31 10:08:03.298: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 32.194087ms)
Oct 31 10:08:03.298: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 32.303454ms)
Oct 31 10:08:03.298: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 32.762949ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 32.618619ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 32.655739ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 32.855967ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.016454ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 32.778324ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 32.805741ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 33.308545ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 33.410093ms)
Oct 31 10:08:03.299: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 33.212544ms)
Oct 31 10:08:03.300: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 33.667974ms)
Oct 31 10:08:03.300: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.035327ms)
Oct 31 10:08:03.300: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 33.956316ms)
Oct 31 10:08:03.300: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.229571ms)
Oct 31 10:08:03.317: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 17.105244ms)
Oct 31 10:08:03.318: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 17.661216ms)
Oct 31 10:08:03.318: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 18.231939ms)
Oct 31 10:08:03.319: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 18.335961ms)
Oct 31 10:08:03.319: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 18.47987ms)
Oct 31 10:08:03.330: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 29.808106ms)
Oct 31 10:08:03.330: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 29.714505ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 31.595202ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 31.4124ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 31.655001ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 31.603651ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 31.532064ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 31.586199ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 31.588657ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 31.670523ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 31.796707ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 31.779336ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 31.656002ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 31.92321ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 31.806017ms)
Oct 31 10:08:03.332: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 31.704233ms)
Oct 31 10:08:03.339: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 38.871791ms)
Oct 31 10:08:03.339: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 39.334634ms)
Oct 31 10:08:03.340: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 39.412606ms)
Oct 31 10:08:03.340: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 39.258635ms)
Oct 31 10:08:03.341: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 40.283492ms)
Oct 31 10:08:03.341: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 40.716772ms)
Oct 31 10:08:03.341: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 40.483303ms)
Oct 31 10:08:03.341: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 40.971738ms)
Oct 31 10:08:03.341: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 40.737924ms)
Oct 31 10:08:03.343: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 42.47846ms)
Oct 31 10:08:03.343: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 42.281232ms)
Oct 31 10:08:03.343: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 42.747419ms)
Oct 31 10:08:03.343: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 42.762693ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 18.337195ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 18.734389ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 18.854178ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 18.803625ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 19.060723ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 19.145317ms)
Oct 31 10:08:03.362: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 19.275566ms)
Oct 31 10:08:03.363: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 19.890836ms)
Oct 31 10:08:03.373: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 29.845423ms)
Oct 31 10:08:03.373: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 30.061313ms)
Oct 31 10:08:03.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 30.215606ms)
Oct 31 10:08:03.374: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 30.596986ms)
Oct 31 10:08:03.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 30.71483ms)
Oct 31 10:08:03.375: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 31.856823ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 32.696016ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 32.914699ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 32.923299ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 33.294324ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 33.106609ms)
Oct 31 10:08:03.376: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 33.073696ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 33.387187ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 33.531933ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 33.485907ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 33.745091ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 33.638884ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 33.682157ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 34.006604ms)
Oct 31 10:08:03.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 34.268564ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 34.89067ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 34.935894ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 34.98606ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 35.124215ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.990512ms)
Oct 31 10:08:03.378: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 35.297978ms)
Oct 31 10:08:03.402: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 22.819299ms)
Oct 31 10:08:03.402: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 23.579123ms)
Oct 31 10:08:03.403: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 23.454059ms)
Oct 31 10:08:03.403: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 24.694204ms)
Oct 31 10:08:03.404: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 24.904826ms)
Oct 31 10:08:03.404: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 25.095548ms)
Oct 31 10:08:03.404: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 24.827506ms)
Oct 31 10:08:03.404: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 25.388245ms)
Oct 31 10:08:03.404: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 25.418811ms)
Oct 31 10:08:03.405: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 25.682075ms)
Oct 31 10:08:03.405: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 25.608694ms)
Oct 31 10:08:03.405: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 26.17413ms)
Oct 31 10:08:03.405: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 25.795902ms)
Oct 31 10:08:03.405: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 26.022702ms)
Oct 31 10:08:03.406: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 27.187994ms)
Oct 31 10:08:03.406: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 27.395784ms)
Oct 31 10:08:03.406: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 27.47257ms)
Oct 31 10:08:03.406: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 27.153053ms)
Oct 31 10:08:03.407: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 28.384508ms)
Oct 31 10:08:03.407: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 28.29365ms)
Oct 31 10:08:03.407: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 28.342544ms)
Oct 31 10:08:03.408: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 28.706971ms)
Oct 31 10:08:03.408: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 28.82357ms)
Oct 31 10:08:03.410: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 30.947259ms)
Oct 31 10:08:03.410: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 31.26338ms)
Oct 31 10:08:03.410: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 31.207799ms)
Oct 31 10:08:03.410: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 31.668909ms)
Oct 31 10:08:03.411: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 31.524062ms)
Oct 31 10:08:03.411: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 31.607895ms)
Oct 31 10:08:03.411: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 31.789867ms)
Oct 31 10:08:03.413: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 33.529292ms)
Oct 31 10:08:03.413: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 33.852779ms)
Oct 31 10:08:03.413: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 34.34912ms)
Oct 31 10:08:03.413: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 34.055266ms)
Oct 31 10:08:03.444: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 30.688407ms)
Oct 31 10:08:03.444: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 31.164911ms)
Oct 31 10:08:03.444: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 30.967539ms)
Oct 31 10:08:03.444: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 31.371201ms)
Oct 31 10:08:03.444: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 31.272808ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 31.115518ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 31.839627ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 31.89625ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 32.079278ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 32.240739ms)
Oct 31 10:08:03.445: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 32.232817ms)
Oct 31 10:08:03.446: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 32.266244ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 33.48064ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 33.288021ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 33.577745ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 33.64444ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 33.891693ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 34.015889ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 33.840785ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 33.755525ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 33.648835ms)
Oct 31 10:08:03.447: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 33.97617ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 34.658525ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 34.66932ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.131869ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 34.568579ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 34.644228ms)
Oct 31 10:08:03.448: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 35.016244ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 35.250234ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 35.226236ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 35.339908ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 36.084796ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 36.088269ms)
Oct 31 10:08:03.449: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 36.211473ms)
Oct 31 10:08:03.475: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 24.964541ms)
Oct 31 10:08:03.475: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 25.136908ms)
Oct 31 10:08:03.475: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 25.436057ms)
Oct 31 10:08:03.475: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 25.507047ms)
Oct 31 10:08:03.476: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 26.661669ms)
Oct 31 10:08:03.477: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 26.963118ms)
Oct 31 10:08:03.477: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 26.976181ms)
Oct 31 10:08:03.477: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 27.098312ms)
Oct 31 10:08:03.477: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 26.954207ms)
Oct 31 10:08:03.477: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 27.714274ms)
Oct 31 10:08:03.478: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 27.851573ms)
Oct 31 10:08:03.478: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 27.871031ms)
Oct 31 10:08:03.478: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 28.019416ms)
Oct 31 10:08:03.478: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 27.974132ms)
Oct 31 10:08:03.478: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 27.848035ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 30.085719ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 30.105989ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 30.228603ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 30.206357ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 31.023577ms)
Oct 31 10:08:03.480: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 30.669317ms)
Oct 31 10:08:03.483: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 32.955579ms)
Oct 31 10:08:03.483: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 33.365398ms)
Oct 31 10:08:03.483: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 32.944388ms)
Oct 31 10:08:03.484: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 34.135602ms)
Oct 31 10:08:03.484: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 34.512834ms)
Oct 31 10:08:03.485: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 35.376548ms)
Oct 31 10:08:03.485: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 35.419468ms)
Oct 31 10:08:03.485: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 35.34809ms)
Oct 31 10:08:03.485: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 35.732969ms)
Oct 31 10:08:03.485: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 35.612195ms)
Oct 31 10:08:03.486: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 35.635429ms)
Oct 31 10:08:03.487: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 37.496669ms)
Oct 31 10:08:03.488: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 37.905537ms)
Oct 31 10:08:03.516: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 27.830902ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 30.746196ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 31.00279ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 31.036275ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 30.918988ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 31.374699ms)
Oct 31 10:08:03.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 31.461363ms)
Oct 31 10:08:03.520: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 31.846508ms)
Oct 31 10:08:03.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 32.214751ms)
Oct 31 10:08:03.520: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 32.280459ms)
Oct 31 10:08:03.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 32.450255ms)
Oct 31 10:08:03.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 32.901426ms)
Oct 31 10:08:03.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 33.170694ms)
Oct 31 10:08:03.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 33.442161ms)
Oct 31 10:08:03.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 33.357345ms)
Oct 31 10:08:03.521: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 33.234455ms)
Oct 31 10:08:03.522: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 33.990643ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 34.825375ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 35.165043ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 34.859963ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 35.158273ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 34.868199ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 35.237363ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 35.53338ms)
Oct 31 10:08:03.523: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 35.21243ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 35.680483ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 35.797215ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 35.574541ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 35.833977ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 35.880925ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 35.841314ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 35.875726ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 36.197181ms)
Oct 31 10:08:03.524: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 36.137842ms)
Oct 31 10:08:03.550: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 26.189337ms)
Oct 31 10:08:03.551: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 26.801879ms)
Oct 31 10:08:03.551: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 27.01248ms)
Oct 31 10:08:03.553: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 28.345137ms)
Oct 31 10:08:03.555: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 31.025874ms)
Oct 31 10:08:03.555: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 30.9727ms)
Oct 31 10:08:03.555: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 30.805519ms)
Oct 31 10:08:03.556: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 31.335554ms)
Oct 31 10:08:03.557: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 32.937562ms)
Oct 31 10:08:03.557: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 32.899078ms)
Oct 31 10:08:03.558: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 33.591169ms)
Oct 31 10:08:03.558: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 33.865655ms)
Oct 31 10:08:03.558: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 33.932503ms)
Oct 31 10:08:03.558: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 33.98909ms)
Oct 31 10:08:03.558: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 33.96106ms)
Oct 31 10:08:03.559: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 34.773056ms)
Oct 31 10:08:03.559: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 34.809ms)
Oct 31 10:08:03.559: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 34.908158ms)
Oct 31 10:08:03.560: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 35.438975ms)
Oct 31 10:08:03.560: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 35.47902ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 36.189218ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 36.419966ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 36.951463ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 36.809786ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 36.91854ms)
Oct 31 10:08:03.561: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 37.042451ms)
Oct 31 10:08:03.562: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 37.582715ms)
Oct 31 10:08:03.562: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 37.248076ms)
Oct 31 10:08:03.562: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 37.656187ms)
Oct 31 10:08:03.562: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 37.352261ms)
Oct 31 10:08:03.564: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 39.560483ms)
Oct 31 10:08:03.564: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 39.73957ms)
Oct 31 10:08:03.564: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 39.566083ms)
Oct 31 10:08:03.564: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 39.584174ms)
Oct 31 10:08:03.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/proxy/: bar (200; 18.558656ms)
Oct 31 10:08:03.583: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/... (200; 18.992937ms)
Oct 31 10:08:03.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz/proxy/rewriteme"... (200; 18.986079ms)
Oct 31 10:08:03.599: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/: foo (200; 34.514324ms)
Oct 31 10:08:03.599: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/proxy/: bar (200; 34.641163ms)
Oct 31 10:08:03.599: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/: tls baz (200; 34.956988ms)
Oct 31 10:08:03.600: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/: bar (200; 35.158115ms)
Oct 31 10:08:03.600: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname1/proxy/: foo (200; 34.916227ms)
Oct 31 10:08:03.600: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname1/proxy/: tls baz (200; 35.342101ms)
Oct 31 10:08:03.600: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:460/proxy/: tls baz (200; 35.28316ms)
Oct 31 10:08:03.600: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:81/: bar (200; 35.722298ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/proxy/: foo (200; 36.332791ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:portname2/: bar (200; 36.654702ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname1/: foo (200; 36.35735ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/http:proxy-service-ljw2f:80/: foo (200; 36.468647ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:1080/proxy/... (200; 36.756238ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:80/: foo (200; 37.001692ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:portname2/proxy/: bar (200; 36.838995ms)
Oct 31 10:08:03.601: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/: foo (200; 37.084165ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/proxy-service-ljw2f:81/: bar (200; 37.254782ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:443/: tls baz (200; 37.274187ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:160/proxy/: foo (200; 37.699618ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/: bar (200; 37.823495ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:162/: bar (200; 38.043696ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/proxy/: foo (200; 37.887977ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/proxy/rewri... (200; 38.18527ms)
Oct 31 10:08:03.602: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:1080/rewri... (200; 38.009743ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:462/proxy/: tls qux (200; 38.416675ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/pods/proxy-service-ljw2f-llflz:160/: foo (200; 38.217658ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/http:proxy-service-ljw2f-llflz:162/proxy/: bar (200; 38.50254ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9d75r/pods/https:proxy-service-ljw2f-llflz:443/proxy/... (200; 38.327766ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:444/: tls qux (200; 38.77518ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/proxy/: tls qux (200; 38.392354ms)
Oct 31 10:08:03.603: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-9d75r/services/https:proxy-service-ljw2f:tlsportname2/: tls qux (200; 38.877563ms)
STEP: deleting { ReplicationController} proxy-service-ljw2f in namespace e2e-tests-proxy-9d75r
Oct 31 10:08:03.820: INFO: Deleting { ReplicationController} proxy-service-ljw2f took: 101.100981ms
Oct 31 10:08:03.820: INFO: Terminating { ReplicationController} proxy-service-ljw2f pods took: 35.234µs
Oct 31 10:08:05.720: INFO: Garbage collecting { ReplicationController} proxy-service-ljw2f pods took: 2.001296057s
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:08:05.720: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-9d75r" for this suite.
Oct 31 10:08:13.093: INFO: namespace: e2e-tests-proxy-9d75r, resource: bindings, ignored listing per whitelist
Oct 31 10:08:13.108: INFO: namespace e2e-tests-proxy-9d75r deletion completed in 7.358551715s

• [SLOW TEST:14.421 seconds]
[k8s.io] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:275
    should proxy through a service and a pod [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:274
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:123
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:08:13.108: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:123
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-q6khr
Oct 31 10:08:15.287: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-q6khr
STEP: checking the pod's current state and verifying that restartCount is present
Oct 31 10:08:15.303: INFO: Initial restart count of pod liveness-exec is 0
Oct 31 10:09:05.714: INFO: Restart count of pod e2e-tests-container-probe-q6khr/liveness-exec is now 1 (50.411344861s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:09:05.733: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-q6khr" for this suite.
Oct 31 10:09:12.357: INFO: namespace: e2e-tests-container-probe-q6khr, resource: bindings, ignored listing per whitelist
Oct 31 10:09:13.104: INFO: namespace e2e-tests-container-probe-q6khr deletion completed in 7.342295096s

• [SLOW TEST:59.996 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:123
------------------------------
SSSSSS
------------------------------
[k8s.io] ConfigMap 
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:338
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:09:13.104: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:338
STEP: Creating configMap with name cm-test-opt-del-8b8551d3-be23-11e7-9459-0e11a30959be
STEP: Creating configMap with name cm-test-opt-upd-8b8552b7-be23-11e7-9459-0e11a30959be
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8b8551d3-be23-11e7-9459-0e11a30959be
STEP: Updating configmap cm-test-opt-upd-8b8552b7-be23-11e7-9459-0e11a30959be
STEP: Creating configMap with name cm-test-opt-create-8b8552ec-be23-11e7-9459-0e11a30959be
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:10:44.351: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9gz66" for this suite.
Oct 31 10:11:07.073: INFO: namespace: e2e-tests-configmap-9gz66, resource: bindings, ignored listing per whitelist
Oct 31 10:11:07.696: INFO: namespace e2e-tests-configmap-9gz66 deletion completed in 23.32970022s

• [SLOW TEST:114.592 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  optional updates should be reflected in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:338
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:56
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:11:07.696: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:56
STEP: Creating projection with secret that has name projected-secret-test-map-cfd1c5c4-be23-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 10:11:07.818: INFO: Waiting up to 5m0s for pod pod-projected-secrets-cfd40ca8-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:11:07.832: INFO: Waiting for pod pod-projected-secrets-cfd40ca8-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-mtmjl' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.585642ms elapsed)
STEP: Saw pod success
Oct 31 10:11:09.863: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-secrets-cfd40ca8-be23-11e7-9459-0e11a30959be container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 31 10:11:09.904: INFO: Waiting for pod pod-projected-secrets-cfd40ca8-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:11:09.920: INFO: Pod pod-projected-secrets-cfd40ca8-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:11:09.920: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mtmjl" for this suite.
Oct 31 10:11:17.251: INFO: namespace: e2e-tests-projected-mtmjl, resource: bindings, ignored listing per whitelist
Oct 31 10:11:17.280: INFO: namespace e2e-tests-projected-mtmjl deletion completed in 7.33054816s

• [SLOW TEST:9.583 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:56
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:71
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:11:17.280: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:71
STEP: Creating a pod to test env composition
Oct 31 10:11:17.396: INFO: Waiting up to 5m0s for pod var-expansion-d589964d-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:11:17.411: INFO: Waiting for pod var-expansion-d589964d-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-var-expansion-bpzz5' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.515785ms elapsed)
STEP: Saw pod success
Oct 31 10:11:19.442: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod var-expansion-d589964d-be23-11e7-9459-0e11a30959be container dapi-container: <nil>
STEP: delete the pod
Oct 31 10:11:19.484: INFO: Waiting for pod var-expansion-d589964d-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:11:19.499: INFO: Pod var-expansion-d589964d-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:11:19.499: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-bpzz5" for this suite.
Oct 31 10:11:26.464: INFO: namespace: e2e-tests-var-expansion-bpzz5, resource: bindings, ignored listing per whitelist
Oct 31 10:11:26.873: INFO: namespace e2e-tests-var-expansion-bpzz5 deletion completed in 7.345033993s

• [SLOW TEST:9.594 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:71
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1285
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:11:26.873: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1243
[It] should create a deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1285
STEP: running the image gcr.io/google_containers/nginx-slim:0.7
Oct 31 10:11:26.996: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=gcr.io/google_containers/nginx-slim:0.7 --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-79r2s'
Oct 31 10:11:28.228: INFO: stderr: ""
Oct 31 10:11:28.228: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1257
Oct 31 10:11:30.261: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-79r2s'
Oct 31 10:11:33.633: INFO: stderr: ""
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:11:33.633: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-79r2s" for this suite.
Oct 31 10:11:40.644: INFO: namespace: e2e-tests-kubectl-79r2s, resource: bindings, ignored listing per whitelist
Oct 31 10:11:41.004: INFO: namespace e2e-tests-kubectl-79r2s deletion completed in 7.341915271s

• [SLOW TEST:14.131 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should create a deployment from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1285
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:414
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:11:41.005: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:414
STEP: Creating configMap with name projected-configmap-test-volume-map-e3adec72-be23-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:11:41.139: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-e3b05ab8-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:11:41.154: INFO: Waiting for pod pod-projected-configmaps-e3b05ab8-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-hhxpz' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.038842ms elapsed)
STEP: Saw pod success
Oct 31 10:11:43.185: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-projected-configmaps-e3b05ab8-be23-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:11:43.227: INFO: Waiting for pod pod-projected-configmaps-e3b05ab8-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:11:43.241: INFO: Pod pod-projected-configmaps-e3b05ab8-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:11:43.241: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hhxpz" for this suite.
Oct 31 10:11:50.085: INFO: namespace: e2e-tests-projected-hhxpz, resource: bindings, ignored listing per whitelist
Oct 31 10:11:50.606: INFO: namespace e2e-tests-projected-hhxpz deletion completed in 7.336688989s

• [SLOW TEST:9.602 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:414
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:106
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:11:50.607: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 31 10:11:50.737: INFO: Waiting up to 5m0s for pod pod-e968e717-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:11:50.752: INFO: Waiting for pod pod-e968e717-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-vqfgf' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.891022ms elapsed)
STEP: Saw pod success
Oct 31 10:11:52.783: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-e968e717-be23-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:11:52.822: INFO: Waiting for pod pod-e968e717-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:11:52.838: INFO: Pod pod-e968e717-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:11:52.838: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vqfgf" for this suite.
Oct 31 10:11:59.460: INFO: namespace: e2e-tests-emptydir-vqfgf, resource: bindings, ignored listing per whitelist
Oct 31 10:12:00.214: INFO: namespace e2e-tests-emptydir-vqfgf deletion completed in 7.347025664s

• [SLOW TEST:9.607 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0666,default) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:106
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:82
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:00.214: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:82
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 31 10:12:00.354: INFO: Waiting up to 5m0s for pod pod-ef246f16-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:12:00.368: INFO: Waiting for pod pod-ef246f16-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-bwc2l' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.666353ms elapsed)
STEP: Saw pod success
Oct 31 10:12:02.400: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-ef246f16-be23-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:12:02.438: INFO: Waiting for pod pod-ef246f16-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:12:02.453: INFO: Pod pod-ef246f16-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:12:02.453: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bwc2l" for this suite.
Oct 31 10:12:09.305: INFO: namespace: e2e-tests-emptydir-bwc2l, resource: bindings, ignored listing per whitelist
Oct 31 10:12:09.833: INFO: namespace e2e-tests-emptydir-bwc2l deletion completed in 7.350377963s

• [SLOW TEST:9.619 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:82
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a docker exec liveness probe with timeout [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:266
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:09.833: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:49
[It] should be restarted with a docker exec liveness probe with timeout [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:266
Oct 31 10:12:09.948: INFO: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:12:09.949: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8scbm" for this suite.
Oct 31 10:12:16.933: INFO: namespace: e2e-tests-container-probe-8scbm, resource: bindings, ignored listing per whitelist
Oct 31 10:12:17.322: INFO: namespace e2e-tests-container-probe-8scbm deletion completed in 7.343245416s

S [SKIPPING] [7.489 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be restarted with a docker exec liveness probe with timeout [Conformance] [It]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:266

  Oct 31 10:12:09.948: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API

  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:303
------------------------------
SSSSSSS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:60
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:17.322: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:60
STEP: Creating secret with name secret-test-map-f9545759-be23-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume secrets
Oct 31 10:12:17.461: INFO: Waiting up to 5m0s for pod pod-secrets-f956acda-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:12:17.476: INFO: Waiting for pod pod-secrets-f956acda-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-secrets-mld2z' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.259753ms elapsed)
STEP: Saw pod success
Oct 31 10:12:19.507: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod pod-secrets-f956acda-be23-11e7-9459-0e11a30959be container secret-volume-test: <nil>
STEP: delete the pod
Oct 31 10:12:19.546: INFO: Waiting for pod pod-secrets-f956acda-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:12:19.562: INFO: Pod pod-secrets-f956acda-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:12:19.562: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mld2z" for this suite.
Oct 31 10:12:26.685: INFO: namespace: e2e-tests-secrets-mld2z, resource: bindings, ignored listing per whitelist
Oct 31 10:12:26.925: INFO: namespace e2e-tests-secrets-mld2z deletion completed in 7.334187798s

• [SLOW TEST:9.604 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:60
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:387
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:26.925: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:387
STEP: Creating configMap with name projected-configmap-test-volume-ff15526f-be23-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:12:27.114: INFO: Waiting up to 5m0s for pod pod-projected-configmaps-ff17ba81-be23-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:12:27.129: INFO: Waiting for pod pod-projected-configmaps-ff17ba81-be23-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-djsv5' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.300281ms elapsed)
STEP: Saw pod success
Oct 31 10:12:29.162: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-projected-configmaps-ff17ba81-be23-11e7-9459-0e11a30959be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:12:29.201: INFO: Waiting for pod pod-projected-configmaps-ff17ba81-be23-11e7-9459-0e11a30959be to disappear
Oct 31 10:12:29.216: INFO: Pod pod-projected-configmaps-ff17ba81-be23-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:12:29.216: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-djsv5" for this suite.
Oct 31 10:12:35.853: INFO: namespace: e2e-tests-projected-djsv5, resource: bindings, ignored listing per whitelist
Oct 31 10:12:36.588: INFO: namespace e2e-tests-projected-djsv5 deletion completed in 7.342056889s

• [SLOW TEST:9.662 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable from pods in volume [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:387
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:94
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:36.588: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:94
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 31 10:12:36.720: INFO: Waiting up to 5m0s for pod pod-04d0e9fd-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:12:36.734: INFO: Waiting for pod pod-04d0e9fd-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-grrb6' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.74843ms elapsed)
STEP: Saw pod success
Oct 31 10:12:38.766: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-04d0e9fd-be24-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:12:38.811: INFO: Waiting for pod pod-04d0e9fd-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:12:38.826: INFO: Pod pod-04d0e9fd-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:12:38.826: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-grrb6" for this suite.
Oct 31 10:12:46.050: INFO: namespace: e2e-tests-emptydir-grrb6, resource: bindings, ignored listing per whitelist
Oct 31 10:12:46.200: INFO: namespace e2e-tests-emptydir-grrb6 deletion completed in 7.344163984s

• [SLOW TEST:9.612 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (non-root,0777,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:94
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Downward API volume 
  should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:154
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:12:46.200: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:154
STEP: Creating the pod
Oct 31 10:12:48.916: INFO: Successfully updated pod "annotationupdate0a87ab90-be24-11e7-9459-0e11a30959be"
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:14:21.879: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5rm4k" for this suite.
Oct 31 10:14:44.686: INFO: namespace: e2e-tests-downward-api-5rm4k, resource: bindings, ignored listing per whitelist
Oct 31 10:14:45.244: INFO: namespace e2e-tests-downward-api-5rm4k deletion completed in 23.34848521s

• [SLOW TEST:119.044 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should update annotations on modification [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:154
------------------------------
SSSS
------------------------------
[k8s.io] Services 
  should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:215
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:14:45.244: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:52
[It] should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:215
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-dh698
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-dh698 to expose endpoints map[]
Oct 31 10:14:45.379: INFO: Get endpoints failed (16.717341ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Oct 31 10:14:46.395: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dh698 exposes endpoints map[] (1.032436184s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-dh698
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-dh698 to expose endpoints map[pod1:[100]]
Oct 31 10:14:48.521: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dh698 exposes endpoints map[pod1:[100]] (2.093146417s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-dh698
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-dh698 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 31 10:14:50.691: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dh698 exposes endpoints map[pod1:[100] pod2:[101]] (2.139608044s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-dh698
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-dh698 to expose endpoints map[pod2:[101]]
Oct 31 10:14:51.769: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dh698 exposes endpoints map[pod2:[101]] (1.061063642s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-dh698
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-dh698 to expose endpoints map[]
Oct 31 10:14:52.816: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dh698 exposes endpoints map[] (1.030191986s elapsed)
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:14:52.845: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-dh698" for this suite.
Oct 31 10:14:59.641: INFO: namespace: e2e-tests-services-dh698, resource: bindings, ignored listing per whitelist
Oct 31 10:15:00.209: INFO: namespace e2e-tests-services-dh698 deletion completed in 7.334215658s
[AfterEach] [k8s.io] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:64

• [SLOW TEST:14.965 seconds]
[k8s.io] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/service.go:215
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:59
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:15:00.209: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:59
STEP: Creating a pod to test downward API volume plugin
Oct 31 10:15:00.356: INFO: Waiting up to 5m0s for pod downwardapi-volume-5a6e9fbe-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:15:00.371: INFO: Waiting for pod downwardapi-volume-5a6e9fbe-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-kgmgb' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.765122ms elapsed)
STEP: Saw pod success
Oct 31 10:15:02.403: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod downwardapi-volume-5a6e9fbe-be24-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 10:15:02.440: INFO: Waiting for pod downwardapi-volume-5a6e9fbe-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:15:02.455: INFO: Pod downwardapi-volume-5a6e9fbe-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:15:02.455: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kgmgb" for this suite.
Oct 31 10:15:09.546: INFO: namespace: e2e-tests-downward-api-kgmgb, resource: bindings, ignored listing per whitelist
Oct 31 10:15:09.839: INFO: namespace e2e-tests-downward-api-kgmgb deletion completed in 7.355339001s

• [SLOW TEST:9.631 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should set DefaultMode on files [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:59
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:35
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:15:09.840: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:35
STEP: Creating a pod to test use defaults
Oct 31 10:15:09.971: INFO: Waiting up to 5m0s for pod client-containers-6029b8a4-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:15:09.986: INFO: Waiting for pod client-containers-6029b8a4-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-containers-jxzl7' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.107747ms elapsed)
STEP: Saw pod success
Oct 31 10:15:12.018: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod client-containers-6029b8a4-be24-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:15:12.057: INFO: Waiting for pod client-containers-6029b8a4-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:15:12.072: INFO: Pod client-containers-6029b8a4-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:15:12.073: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jxzl7" for this suite.
Oct 31 10:15:18.812: INFO: namespace: e2e-tests-containers-jxzl7, resource: bindings, ignored listing per whitelist
Oct 31 10:15:19.441: INFO: namespace e2e-tests-containers-jxzl7 deletion completed in 7.33900422s

• [SLOW TEST:9.601 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:35
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:204
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:15:19.441: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:204
STEP: Creating a pod to test downward API volume plugin
Oct 31 10:15:19.559: INFO: Waiting up to 5m0s for pod downwardapi-volume-65e0b30d-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:15:19.574: INFO: Waiting for pod downwardapi-volume-65e0b30d-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-downward-api-7r8xk' status to be 'success or failure'(found phase: "Pending", readiness: false) (14.90753ms elapsed)
STEP: Saw pod success
Oct 31 10:15:21.606: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod downwardapi-volume-65e0b30d-be24-11e7-9459-0e11a30959be container client-container: <nil>
STEP: delete the pod
Oct 31 10:15:21.661: INFO: Waiting for pod downwardapi-volume-65e0b30d-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:15:21.676: INFO: Pod downwardapi-volume-65e0b30d-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:15:21.676: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7r8xk" for this suite.
Oct 31 10:15:28.585: INFO: namespace: e2e-tests-downward-api-7r8xk, resource: bindings, ignored listing per whitelist
Oct 31 10:15:29.055: INFO: namespace e2e-tests-downward-api-7r8xk deletion completed in 7.349996684s

• [SLOW TEST:9.614 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:204
------------------------------
SSS
------------------------------
[k8s.io] Networking [k8s.io] Granular Checks: Pods 
  should function for intra-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:45
[BeforeEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:15:29.055: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:45
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-g7rsf
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 31 10:15:29.183: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Oct 31 10:15:51.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.4.60:8080/dial?request=hostName&protocol=udp&host=172.16.4.59&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-g7rsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:15:51.510: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:15:51.797: INFO: Waiting for endpoints: map[]
Oct 31 10:15:51.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.4.60:8080/dial?request=hostName&protocol=udp&host=172.16.6.61&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-g7rsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:15:51.813: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:15:52.001: INFO: Waiting for endpoints: map[]
Oct 31 10:15:52.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.4.60:8080/dial?request=hostName&protocol=udp&host=172.16.2.62&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-g7rsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 31 10:15:52.017: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Oct 31 10:15:52.210: INFO: Waiting for endpoints: map[]
[AfterEach] [k8s.io] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:15:52.210: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-g7rsf" for this suite.
Oct 31 10:16:15.446: INFO: namespace: e2e-tests-pod-network-test-g7rsf, resource: bindings, ignored listing per whitelist
Oct 31 10:16:15.582: INFO: namespace e2e-tests-pod-network-test-g7rsf deletion completed in 23.343190747s

• [SLOW TEST:46.527 seconds]
[k8s.io] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should function for intra-pod communication: udp [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:45
------------------------------
SSSSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:774
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:16:15.583: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should check if Kubernetes master services is included in cluster-info [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:774
STEP: validating cluster-info
Oct 31 10:16:15.678: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig cluster-info'
Oct 31 10:16:15.863: INFO: stderr: ""
Oct 31 10:16:15.863: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://internal-api.prtest-cc63063-94.origin-ci-int-gce.dev.rhcloud.com:8443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:16:15.863: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lwvtd" for this suite.
Oct 31 10:16:22.409: INFO: namespace: e2e-tests-kubectl-lwvtd, resource: bindings, ignored listing per whitelist
Oct 31 10:16:23.268: INFO: namespace e2e-tests-kubectl-lwvtd deletion completed in 7.37561675s

• [SLOW TEST:7.686 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl cluster-info
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should check if Kubernetes master services is included in cluster-info [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:774
------------------------------
SS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:74
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:16:23.268: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:74
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 31 10:16:23.388: INFO: Waiting up to 5m0s for pod pod-8bec4751-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:16:23.403: INFO: Waiting for pod pod-8bec4751-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-emptydir-pdrcx' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.109694ms elapsed)
STEP: Saw pod success
Oct 31 10:16:25.435: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-cfzz pod pod-8bec4751-be24-11e7-9459-0e11a30959be container test-container: <nil>
STEP: delete the pod
Oct 31 10:16:25.475: INFO: Waiting for pod pod-8bec4751-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:16:25.490: INFO: Pod pod-8bec4751-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:16:25.490: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pdrcx" for this suite.
Oct 31 10:16:32.110: INFO: namespace: e2e-tests-emptydir-pdrcx, resource: bindings, ignored listing per whitelist
Oct 31 10:16:32.866: INFO: namespace e2e-tests-emptydir-pdrcx deletion completed in 7.346771508s

• [SLOW TEST:9.598 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should support (root,0644,tmpfs) [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:74
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should project all components that make up the projection API [Conformance] [Volume] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:1023
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:16:32.866: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:803
[It] should project all components that make up the projection API [Conformance] [Volume] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:1023
STEP: Creating configMap with name configmap-projected-all-test-volume-91a6e47d-be24-11e7-9459-0e11a30959be
STEP: Creating secret with name secret-projected-all-test-volume-91a6e45e-be24-11e7-9459-0e11a30959be
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 31 10:16:33.032: INFO: Waiting up to 5m0s for pod projected-volume-91a6e41b-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:16:33.047: INFO: Waiting for pod projected-volume-91a6e41b-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-projected-gnqlk' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.428737ms elapsed)
STEP: Saw pod success
Oct 31 10:16:35.079: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-2mxx pod projected-volume-91a6e41b-be24-11e7-9459-0e11a30959be container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 31 10:16:35.119: INFO: Waiting for pod projected-volume-91a6e41b-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:16:35.134: INFO: Pod projected-volume-91a6e41b-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:16:35.134: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gnqlk" for this suite.
Oct 31 10:16:41.889: INFO: namespace: e2e-tests-projected-gnqlk, resource: bindings, ignored listing per whitelist
Oct 31 10:16:42.504: INFO: namespace e2e-tests-projected-gnqlk deletion completed in 7.34110006s

• [SLOW TEST:9.638 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should project all components that make up the projection API [Conformance] [Volume] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:1023
------------------------------
SSSSSS
------------------------------
[k8s.io] ConfigMap 
  should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:493
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:16:42.504: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:493
STEP: Creating configMap with name configmap-test-volume-97634010-be24-11e7-9459-0e11a30959be
STEP: Creating a pod to test consume configMaps
Oct 31 10:16:42.639: INFO: Waiting up to 5m0s for pod pod-configmaps-9765a58d-be24-11e7-9459-0e11a30959be status to be success or failure
Oct 31 10:16:42.654: INFO: Waiting for pod pod-configmaps-9765a58d-be24-11e7-9459-0e11a30959be in namespace 'e2e-tests-configmap-5brxg' status to be 'success or failure'(found phase: "Pending", readiness: false) (15.108489ms elapsed)
STEP: Saw pod success
Oct 31 10:16:44.685: INFO: Trying to get logs from node ci-prtest-cc63063-94-ig-n-sttk pod pod-configmaps-9765a58d-be24-11e7-9459-0e11a30959be container configmap-volume-test: <nil>
STEP: delete the pod
Oct 31 10:16:44.725: INFO: Waiting for pod pod-configmaps-9765a58d-be24-11e7-9459-0e11a30959be to disappear
Oct 31 10:16:44.741: INFO: Pod pod-configmaps-9765a58d-be24-11e7-9459-0e11a30959be no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:16:44.741: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5brxg" for this suite.
Oct 31 10:16:51.284: INFO: namespace: e2e-tests-configmap-5brxg, resource: bindings, ignored listing per whitelist
Oct 31 10:16:52.105: INFO: namespace e2e-tests-configmap-5brxg deletion completed in 7.334993242s

• [SLOW TEST:9.601 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  should be consumable in multiple volumes in the same pod [Conformance] [Volume]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:493
------------------------------
S
------------------------------
[k8s.io] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1089
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:16:52.105: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[It] should add annotations for pods in rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1089
STEP: creating Redis RC
Oct 31 10:16:52.212: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-sdrv5'
Oct 31 10:16:52.484: INFO: stderr: ""
Oct 31 10:16:52.484: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Oct 31 10:16:53.500: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 10:16:53.500: INFO: Found 0 / 1
Oct 31 10:16:54.500: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 10:16:54.500: INFO: Found 1 / 1
Oct 31 10:16:54.500: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 31 10:16:54.517: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 10:16:54.517: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 31 10:16:54.517: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig patch pod redis-master-h2c24 --namespace=e2e-tests-kubectl-sdrv5 -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 31 10:16:54.731: INFO: stderr: ""
Oct 31 10:16:54.731: INFO: stdout: "pod \"redis-master-h2c24\" patched\n"
STEP: checking annotations
Oct 31 10:16:54.747: INFO: Selector matched 1 pods for map[app:redis]
Oct 31 10:16:54.747: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:16:54.747: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sdrv5" for this suite.
Oct 31 10:17:08.035: INFO: namespace: e2e-tests-kubectl-sdrv5, resource: bindings, ignored listing per whitelist
Oct 31 10:17:08.127: INFO: namespace e2e-tests-kubectl-sdrv5 deletion completed in 13.349855942s

• [SLOW TEST:16.021 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Kubectl patch
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should add annotations for pods in rc [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1089
------------------------------
SSS
------------------------------
[k8s.io] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:305
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:130
STEP: Creating a kubernetes client
Oct 31 10:17:08.127: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:253
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:274
[It] should do a rolling update of a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:305
STEP: creating the initial replication controller
Oct 31 10:17:08.218: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:08.470: INFO: stderr: ""
Oct 31 10:17:08.470: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 10:17:08.470: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:08.666: INFO: stderr: ""
Oct 31 10:17:08.666: INFO: stdout: "update-demo-nautilus-954cd update-demo-nautilus-gqkrx "
Oct 31 10:17:08.666: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-954cd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:08.857: INFO: stderr: ""
Oct 31 10:17:08.857: INFO: stdout: ""
Oct 31 10:17:08.857: INFO: update-demo-nautilus-954cd is created but not running
Oct 31 10:17:13.858: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:14.051: INFO: stderr: ""
Oct 31 10:17:14.051: INFO: stdout: "update-demo-nautilus-954cd update-demo-nautilus-gqkrx "
Oct 31 10:17:14.051: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-954cd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:14.242: INFO: stderr: ""
Oct 31 10:17:14.242: INFO: stdout: "true"
Oct 31 10:17:14.242: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-954cd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:14.432: INFO: stderr: ""
Oct 31 10:17:14.432: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 10:17:14.432: INFO: validating pod update-demo-nautilus-954cd
Oct 31 10:17:14.466: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 10:17:14.466: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 10:17:14.466: INFO: update-demo-nautilus-954cd is verified up and running
Oct 31 10:17:14.466: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-gqkrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:14.657: INFO: stderr: ""
Oct 31 10:17:14.657: INFO: stdout: "true"
Oct 31 10:17:14.657: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-gqkrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:14.849: INFO: stderr: ""
Oct 31 10:17:14.849: INFO: stdout: "gcr.io/google_containers/update-demo:nautilus"
Oct 31 10:17:14.849: INFO: validating pod update-demo-nautilus-gqkrx
Oct 31 10:17:14.882: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 31 10:17:14.882: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 31 10:17:14.882: INFO: update-demo-nautilus-gqkrx is verified up and running
STEP: rolling-update to new replication controller
Oct 31 10:17:14.882: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:22.733: INFO: stderr: ""
Oct 31 10:17:22.733: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting update-demo-nautilus\nreplicationcontroller \"update-demo-nautilus\" rolling updated to \"update-demo-kitten\"\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 31 10:17:22.733: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:22.930: INFO: stderr: ""
Oct 31 10:17:22.930: INFO: stdout: "update-demo-kitten-6b5gz update-demo-kitten-drxlg "
Oct 31 10:17:22.931: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-6b5gz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:23.122: INFO: stderr: ""
Oct 31 10:17:23.122: INFO: stdout: "true"
Oct 31 10:17:23.122: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-6b5gz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:23.313: INFO: stderr: ""
Oct 31 10:17:23.313: INFO: stdout: "gcr.io/google_containers/update-demo:kitten"
Oct 31 10:17:23.313: INFO: validating pod update-demo-kitten-6b5gz
Oct 31 10:17:23.346: INFO: got data: {
  "image": "kitten.jpg"
}

Oct 31 10:17:23.346: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct 31 10:17:23.346: INFO: update-demo-kitten-6b5gz is verified up and running
Oct 31 10:17:23.346: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-drxlg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:23.537: INFO: stderr: ""
Oct 31 10:17:23.537: INFO: stdout: "true"
Oct 31 10:17:23.537: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-drxlg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mshwz'
Oct 31 10:17:23.727: INFO: stderr: ""
Oct 31 10:17:23.727: INFO: stdout: "gcr.io/google_containers/update-demo:kitten"
Oct 31 10:17:23.727: INFO: validating pod update-demo-kitten-drxlg
Oct 31 10:17:23.760: INFO: got data: {
  "image": "kitten.jpg"
}

Oct 31 10:17:23.760: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct 31 10:17:23.760: INFO: update-demo-kitten-drxlg is verified up and running
[AfterEach] [k8s.io] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:131
Oct 31 10:17:23.760: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mshwz" for this suite.
Oct 31 10:17:47.033: INFO: namespace: e2e-tests-kubectl-mshwz, resource: bindings, ignored listing per whitelist
Oct 31 10:17:47.126: INFO: namespace e2e-tests-kubectl-mshwz deletion completed in 23.335994531s

• [SLOW TEST:38.999 seconds]
[k8s.io] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:619
    should do a rolling update of a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:305
------------------------------
SOct 31 10:17:47.126: INFO: Running AfterSuite actions on all node
Oct 31 10:17:47.126: INFO: Running AfterSuite actions on node 1
Oct 31 10:17:47.128: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 146 of 651 Specs in 3550.231 seconds
SUCCESS! -- 146 Passed | 0 Failed | 0 Pending | 505 Skipped PASS

