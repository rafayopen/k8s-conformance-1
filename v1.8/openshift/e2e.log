Jan 15 22:16:00.531: INFO: Overriding default scale value of zero to 1
Jan 15 22:16:00.531: INFO: Overriding default milliseconds value of zero to 5000
I0115 22:16:00.617890   17569 e2e.go:384] Starting e2e run "aada7a3c-fa41-11e7-b6d5-0e5e74815504" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1516054560 - Will randomize all specs
Will run 149 of 699 specs

I0115 22:16:00.637948   17569 e2e.go:63] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
Jan 15 22:16:00.637: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:16:00.640: INFO: Waiting up to 4h0m0s for all (but 1) nodes to be schedulable
Jan 15 22:16:00.709: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 15 22:16:00.801: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 15 22:16:00.801: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 15 22:16:00.816: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jan 15 22:16:00.816: INFO: Dumping network health container logs from all nodes to file /data/src/github.com/openshift/origin/_output/scripts/init/artifacts/nethealth.txt
Jan 15 22:16:00.831: INFO: Client version: v1.8.7-beta.0.34+b30876a5539f09
Jan 15 22:16:00.845: INFO: Server version: v1.8.5+440f8d36da
I0115 22:16:00.845856   17569 e2e.go:63] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
[k8s.io] Downward API volume 
  should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:127
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:16:00.845: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
Jan 15 22:16:00.922: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:127
STEP: Creating the pod
Jan 15 22:16:03.604: INFO: Successfully updated pod "annotationupdateab283dc1-fa41-11e7-b6d5-0e5e74815504"
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:16:07.673: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qfgjc" for this suite.
Jan 15 22:16:30.824: INFO: namespace: e2e-tests-downward-api-qfgjc, resource: bindings, ignored listing per whitelist
Jan 15 22:16:31.094: INFO: namespace e2e-tests-downward-api-qfgjc deletion completed in 23.40606s

• [SLOW TEST:30.249 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:127
------------------------------
S
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:40
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:16:31.094: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:40
STEP: Creating secret with name secret-test-bd2e2143-fa41-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:16:31.216: INFO: Waiting up to 5m0s for pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-rbdbm" to be "success or failure"
Jan 15 22:16:31.233: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.776487ms
Jan 15 22:16:33.248: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032389433s
Jan 15 22:16:35.264: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047821342s
Jan 15 22:16:37.279: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063432763s
Jan 15 22:16:39.296: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079786402s
Jan 15 22:16:41.311: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095292907s
Jan 15 22:16:43.327: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.111126411s
STEP: Saw pod success
Jan 15 22:16:43.327: INFO: Pod "pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:16:43.342: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:16:43.411: INFO: Waiting for pod pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:16:43.426: INFO: Pod pod-secrets-bd309a4a-fa41-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:16:43.427: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rbdbm" for this suite.
Jan 15 22:16:50.417: INFO: namespace: e2e-tests-secrets-rbdbm, resource: bindings, ignored listing per whitelist
Jan 15 22:16:50.844: INFO: namespace e2e-tests-secrets-rbdbm deletion completed in 7.401781499s

• [SLOW TEST:19.750 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:40
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:65
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:16:50.845: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:65
Jan 15 22:16:51.014: INFO: (0) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 29.096013ms)
Jan 15 22:16:51.031: INFO: (1) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.085751ms)
Jan 15 22:16:51.056: INFO: (2) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 24.721811ms)
Jan 15 22:16:51.072: INFO: (3) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.168441ms)
Jan 15 22:16:51.088: INFO: (4) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.016981ms)
Jan 15 22:16:51.109: INFO: (5) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 21.229515ms)
Jan 15 22:16:51.129: INFO: (6) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 20.045204ms)
Jan 15 22:16:51.146: INFO: (7) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.536111ms)
Jan 15 22:16:51.162: INFO: (8) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.149669ms)
Jan 15 22:16:51.178: INFO: (9) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.173108ms)
Jan 15 22:16:51.195: INFO: (10) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.232307ms)
Jan 15 22:16:51.211: INFO: (11) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.372802ms)
Jan 15 22:16:51.227: INFO: (12) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.426557ms)
Jan 15 22:16:51.244: INFO: (13) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.625861ms)
Jan 15 22:16:51.261: INFO: (14) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.953445ms)
Jan 15 22:16:51.277: INFO: (15) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.287981ms)
Jan 15 22:16:51.294: INFO: (16) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.21898ms)
Jan 15 22:16:51.310: INFO: (17) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.11861ms)
Jan 15 22:16:51.326: INFO: (18) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.241712ms)
Jan 15 22:16:51.342: INFO: (19) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.450898ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:16:51.343: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-hsrjx" for this suite.
Jan 15 22:16:58.586: INFO: namespace: e2e-tests-proxy-hsrjx, resource: bindings, ignored listing per whitelist
Jan 15 22:16:58.748: INFO: namespace e2e-tests-proxy-hsrjx deletion completed in 7.389908833s

• [SLOW TEST:7.903 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:65
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:16:58.748: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:303
[It] should do a rolling update of a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
STEP: creating the initial replication controller
Jan 15 22:16:58.849: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:16:59.661: INFO: stderr: ""
Jan 15 22:16:59.661: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:16:59.661: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:16:59.896: INFO: stderr: ""
Jan 15 22:16:59.896: INFO: stdout: "update-demo-nautilus-fz499 update-demo-nautilus-x9b5n "
Jan 15 22:16:59.896: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fz499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:00.102: INFO: stderr: ""
Jan 15 22:17:00.102: INFO: stdout: ""
Jan 15 22:17:00.102: INFO: update-demo-nautilus-fz499 is created but not running
Jan 15 22:17:05.102: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:05.323: INFO: stderr: ""
Jan 15 22:17:05.323: INFO: stdout: "update-demo-nautilus-fz499 update-demo-nautilus-x9b5n "
Jan 15 22:17:05.323: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fz499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:05.530: INFO: stderr: ""
Jan 15 22:17:05.530: INFO: stdout: ""
Jan 15 22:17:05.530: INFO: update-demo-nautilus-fz499 is created but not running
Jan 15 22:17:10.530: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:10.738: INFO: stderr: ""
Jan 15 22:17:10.738: INFO: stdout: "update-demo-nautilus-fz499 update-demo-nautilus-x9b5n "
Jan 15 22:17:10.738: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fz499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:10.944: INFO: stderr: ""
Jan 15 22:17:10.944: INFO: stdout: ""
Jan 15 22:17:10.944: INFO: update-demo-nautilus-fz499 is created but not running
Jan 15 22:17:15.945: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:16.154: INFO: stderr: ""
Jan 15 22:17:16.154: INFO: stdout: "update-demo-nautilus-fz499 update-demo-nautilus-x9b5n "
Jan 15 22:17:16.154: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fz499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:16.357: INFO: stderr: ""
Jan 15 22:17:16.357: INFO: stdout: "true"
Jan 15 22:17:16.357: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fz499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:16.562: INFO: stderr: ""
Jan 15 22:17:16.562: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:17:16.562: INFO: validating pod update-demo-nautilus-fz499
Jan 15 22:17:16.596: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:17:16.596: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:17:16.596: INFO: update-demo-nautilus-fz499 is verified up and running
Jan 15 22:17:16.596: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-x9b5n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:16.802: INFO: stderr: ""
Jan 15 22:17:16.802: INFO: stdout: "true"
Jan 15 22:17:16.802: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-x9b5n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:17.008: INFO: stderr: ""
Jan 15 22:17:17.008: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:17:17.008: INFO: validating pod update-demo-nautilus-x9b5n
Jan 15 22:17:17.041: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:17:17.041: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:17:17.041: INFO: update-demo-nautilus-x9b5n is verified up and running
STEP: rolling-update to new replication controller
Jan 15 22:17:17.041: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:25.964: INFO: stderr: ""
Jan 15 22:17:25.964: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting update-demo-nautilus\nreplicationcontroller \"update-demo-nautilus\" rolling updated to \"update-demo-kitten\"\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:17:25.964: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:26.170: INFO: stderr: ""
Jan 15 22:17:26.170: INFO: stdout: "update-demo-kitten-kqj25 update-demo-kitten-l4hk7 update-demo-nautilus-x9b5n "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jan 15 22:17:31.170: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:31.377: INFO: stderr: ""
Jan 15 22:17:31.377: INFO: stdout: "update-demo-kitten-kqj25 update-demo-kitten-l4hk7 "
Jan 15 22:17:31.377: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-kqj25 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:31.581: INFO: stderr: ""
Jan 15 22:17:31.581: INFO: stdout: "true"
Jan 15 22:17:31.581: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-kqj25 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:31.796: INFO: stderr: ""
Jan 15 22:17:31.796: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Jan 15 22:17:31.796: INFO: validating pod update-demo-kitten-kqj25
Jan 15 22:17:31.831: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 15 22:17:31.831: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 15 22:17:31.831: INFO: update-demo-kitten-kqj25 is verified up and running
Jan 15 22:17:31.831: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-l4hk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:32.040: INFO: stderr: ""
Jan 15 22:17:32.040: INFO: stdout: "true"
Jan 15 22:17:32.040: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-l4hk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-47dk2'
Jan 15 22:17:32.244: INFO: stderr: ""
Jan 15 22:17:32.244: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Jan 15 22:17:32.244: INFO: validating pod update-demo-kitten-l4hk7
Jan 15 22:17:32.277: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 15 22:17:32.277: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 15 22:17:32.277: INFO: update-demo-kitten-l4hk7 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:17:32.277: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-47dk2" for this suite.
Jan 15 22:17:55.148: INFO: namespace: e2e-tests-kubectl-47dk2, resource: bindings, ignored listing per whitelist
Jan 15 22:17:55.698: INFO: namespace e2e-tests-kubectl-47dk2 deletion completed in 23.404894922s

• [SLOW TEST:56.950 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should do a rolling update of a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:33
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:17:55.698: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:33
STEP: Creating a pod to test env composition
Jan 15 22:17:55.821: INFO: Waiting up to 5m0s for pod "var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-var-expansion-xm5p6" to be "success or failure"
Jan 15 22:17:55.837: INFO: Pod "var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.84946ms
Jan 15 22:17:57.853: INFO: Pod "var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03168503s
Jan 15 22:17:59.868: INFO: Pod "var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047443321s
STEP: Saw pod success
Jan 15 22:17:59.869: INFO: Pod "var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:17:59.884: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 22:17:59.932: INFO: Waiting for pod var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:17:59.946: INFO: Pod var-expansion-ef9d7d5a-fa41-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:17:59.946: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-xm5p6" for this suite.
Jan 15 22:18:07.037: INFO: namespace: e2e-tests-var-expansion-xm5p6, resource: bindings, ignored listing per whitelist
Jan 15 22:18:07.372: INFO: namespace e2e-tests-var-expansion-xm5p6 deletion completed in 7.409653857s

• [SLOW TEST:11.674 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should allow composing env vars into new env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:33
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:49
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:18:07.372: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:49
STEP: Creating a pod to test override command
Jan 15 22:18:07.514: INFO: Waiting up to 5m0s for pod "client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-containers-4b9ln" to be "success or failure"
Jan 15 22:18:07.529: INFO: Pod "client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.041676ms
Jan 15 22:18:09.545: INFO: Pod "client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030789003s
STEP: Saw pod success
Jan 15 22:18:09.545: INFO: Pod "client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:18:09.560: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:18:09.601: INFO: Waiting for pod client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:18:09.617: INFO: Pod client-containers-f69669b5-fa41-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:18:09.617: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4b9ln" for this suite.
Jan 15 22:18:16.213: INFO: namespace: e2e-tests-containers-4b9ln, resource: bindings, ignored listing per whitelist
Jan 15 22:18:17.043: INFO: namespace e2e-tests-containers-4b9ln deletion completed in 7.410835877s

• [SLOW TEST:9.671 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be able to override the image's default commmand (docker entrypoint) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:49
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a docker exec liveness probe with timeout [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:239
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:18:17.043: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a docker exec liveness probe with timeout [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:239
Jan 15 22:18:17.207: INFO: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:18:17.208: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jfmhz" for this suite.
Jan 15 22:18:24.468: INFO: namespace: e2e-tests-container-probe-jfmhz, resource: bindings, ignored listing per whitelist
Jan 15 22:18:24.635: INFO: namespace e2e-tests-container-probe-jfmhz deletion completed in 7.411963732s

S [SKIPPING] [7.592 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be restarted with a docker exec liveness probe with timeout [Conformance] [It]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:239

  Jan 15 22:18:17.207: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API

  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:310
------------------------------
SSSSSS
------------------------------
[k8s.io] ConfigMap 
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:152
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:18:24.636: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:152
Jan 15 22:18:24.746: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-00de9615-fa42-11e7-b6d5-0e5e74815504
STEP: Creating configMap with name cm-test-opt-upd-00de9665-fa42-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-00de9615-fa42-11e7-b6d5-0e5e74815504
STEP: Updating configmap cm-test-opt-upd-00de9665-fa42-11e7-b6d5-0e5e74815504
STEP: Creating configMap with name cm-test-opt-create-00de968b-fa42-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:19:31.770: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bpdm4" for this suite.
Jan 15 22:19:54.624: INFO: namespace: e2e-tests-configmap-bpdm4, resource: bindings, ignored listing per whitelist
Jan 15 22:19:55.184: INFO: namespace e2e-tests-configmap-bpdm4 deletion completed in 23.397249348s

• [SLOW TEST:90.548 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:152
------------------------------
SSSS
------------------------------
[k8s.io] Secrets 
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:155
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:19:55.184: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:155
Jan 15 22:19:55.301: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-36d83380-fa42-11e7-b6d5-0e5e74815504
STEP: Creating secret with name s-test-opt-upd-36d833d1-fa42-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-36d83380-fa42-11e7-b6d5-0e5e74815504
STEP: Updating secret s-test-opt-upd-36d833d1-fa42-11e7-b6d5-0e5e74815504
STEP: Creating secret with name s-test-opt-create-36d83429-fa42-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:21:02.280: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qktmn" for this suite.
Jan 15 22:21:25.080: INFO: namespace: e2e-tests-secrets-qktmn, resource: bindings, ignored listing per whitelist
Jan 15 22:21:25.702: INFO: namespace e2e-tests-secrets-qktmn deletion completed in 23.404672837s

• [SLOW TEST:90.518 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:155
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:66
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:21:25.702: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:66
STEP: Creating configMap with name configmap-test-volume-map-6cce594e-fa42-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:21:25.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-hcc4p" to be "success or failure"
Jan 15 22:21:25.883: INFO: Pod "pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.245657ms
Jan 15 22:21:27.899: INFO: Pod "pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031099699s
STEP: Saw pod success
Jan 15 22:21:27.899: INFO: Pod "pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:21:27.914: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:21:27.957: INFO: Waiting for pod pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:21:27.972: INFO: Pod pod-configmaps-6cd0d6f4-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:21:27.972: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hcc4p" for this suite.
Jan 15 22:21:35.250: INFO: namespace: e2e-tests-configmap-hcc4p, resource: bindings, ignored listing per whitelist
Jan 15 22:21:35.397: INFO: namespace e2e-tests-configmap-hcc4p deletion completed in 7.40978327s

• [SLOW TEST:9.695 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:66
------------------------------
[k8s.io] Downward API 
  should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:157
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:21:35.398: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:157
STEP: Creating a pod to test downward api env vars
Jan 15 22:21:35.520: INFO: Waiting up to 5m0s for pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-v2shh" to be "success or failure"
Jan 15 22:21:35.535: INFO: Pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.311799ms
Jan 15 22:21:37.551: INFO: Pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031265298s
Jan 15 22:21:39.567: INFO: Pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046884717s
Jan 15 22:21:41.583: INFO: Pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063039633s
STEP: Saw pod success
Jan 15 22:21:41.583: INFO: Pod "downward-api-72912750-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:21:41.599: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downward-api-72912750-fa42-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 22:21:41.641: INFO: Waiting for pod downward-api-72912750-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:21:41.656: INFO: Pod downward-api-72912750-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:21:41.656: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v2shh" for this suite.
Jan 15 22:21:48.926: INFO: namespace: e2e-tests-downward-api-v2shh, resource: bindings, ignored listing per whitelist
Jan 15 22:21:49.076: INFO: namespace e2e-tests-downward-api-v2shh deletion completed in 7.403475694s

• [SLOW TEST:13.678 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide default limits.cpu/memory from node allocatable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:157
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:248
[BeforeEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:21:49.076: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:248
STEP: getting the auto-created API token
Jan 15 22:21:49.771: INFO: created pod pod-service-account-defaultsa
Jan 15 22:21:49.771: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 15 22:21:49.788: INFO: created pod pod-service-account-mountsa
Jan 15 22:21:49.788: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 15 22:21:49.808: INFO: created pod pod-service-account-nomountsa
Jan 15 22:21:49.808: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 15 22:21:49.825: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 15 22:21:49.825: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 15 22:21:49.841: INFO: created pod pod-service-account-mountsa-mountspec
Jan 15 22:21:49.841: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 15 22:21:49.858: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 15 22:21:49.858: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 15 22:21:49.874: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 15 22:21:49.874: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 15 22:21:49.890: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 15 22:21:49.890: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 15 22:21:49.911: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 15 22:21:49.911: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:21:49.911: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-j7wkq" for this suite.
Jan 15 22:21:57.002: INFO: namespace: e2e-tests-svcaccounts-j7wkq, resource: bindings, ignored listing per whitelist
Jan 15 22:21:57.329: INFO: namespace e2e-tests-svcaccounts-j7wkq deletion completed in 7.402700397s

• [SLOW TEST:8.254 seconds]
[sig-auth] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:248
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1156
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:21:57.329: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should check is all data is printed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1156
Jan 15 22:21:57.426: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version'
Jan 15 22:21:57.664: INFO: stderr: ""
Jan 15 22:21:57.664: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.7-beta.0.34+b30876a5539f09\", GitCommit:\"b30876a5539f09684ff9fde266fda10b37738c9c\", GitTreeState:\"clean\", BuildDate:\"2018-01-15T22:11:35Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.5+440f8d36da\", GitCommit:\"440f8d3\", GitTreeState:\"clean\", BuildDate:\"2018-01-15T21:35:33Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:21:57.664: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fz775" for this suite.
Jan 15 22:22:05.068: INFO: namespace: e2e-tests-kubectl-fz775, resource: bindings, ignored listing per whitelist
Jan 15 22:22:05.082: INFO: namespace e2e-tests-kubectl-fz775 deletion completed in 7.402176546s

• [SLOW TEST:7.753 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should check is all data is printed [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1156
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:51
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:22:05.082: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:51
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:22:05.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-v7clm" to be "success or failure"
Jan 15 22:22:05.233: INFO: Pod "downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.987255ms
Jan 15 22:22:07.249: INFO: Pod "downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030681336s
STEP: Saw pod success
Jan 15 22:22:07.249: INFO: Pod "downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:22:07.264: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:22:07.308: INFO: Waiting for pod downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:22:07.323: INFO: Pod downwardapi-volume-84450b68-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:22:07.323: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v7clm" for this suite.
Jan 15 22:22:14.326: INFO: namespace: e2e-tests-downward-api-v7clm, resource: bindings, ignored listing per whitelist
Jan 15 22:22:14.744: INFO: namespace e2e-tests-downward-api-v7clm deletion completed in 7.404910818s

• [SLOW TEST:9.662 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:51
------------------------------
SSS
------------------------------
[k8s.io] Downward API volume 
  should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:156
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:22:14.744: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:156
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:22:14.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-cb4zm" to be "success or failure"
Jan 15 22:22:14.871: INFO: Pod "downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.123346ms
Jan 15 22:22:16.887: INFO: Pod "downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031971152s
STEP: Saw pod success
Jan 15 22:22:16.887: INFO: Pod "downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:22:16.902: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:22:16.946: INFO: Waiting for pod downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:22:16.961: INFO: Pod downwardapi-volume-8a03cba7-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:22:16.961: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cb4zm" for this suite.
Jan 15 22:22:24.112: INFO: namespace: e2e-tests-downward-api-cb4zm, resource: bindings, ignored listing per whitelist
Jan 15 22:22:24.388: INFO: namespace e2e-tests-downward-api-cb4zm deletion completed in 7.410688671s

• [SLOW TEST:9.644 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:156
------------------------------
[k8s.io] Downward API 
  should provide pod IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:68
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:22:24.388: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:68
STEP: Creating a pod to test downward api env vars
Jan 15 22:22:24.539: INFO: Waiting up to 5m0s for pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-42cqb" to be "success or failure"
Jan 15 22:22:24.554: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.164179ms
Jan 15 22:22:26.570: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031125136s
Jan 15 22:22:28.588: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048532604s
Jan 15 22:22:30.604: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064615615s
Jan 15 22:22:32.620: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080645772s
Jan 15 22:22:34.636: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096472561s
Jan 15 22:22:36.652: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.112519424s
STEP: Saw pod success
Jan 15 22:22:36.652: INFO: Pod "downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:22:36.667: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 22:22:36.731: INFO: Waiting for pod downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:22:36.747: INFO: Pod downward-api-8fc816d2-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:22:36.747: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
Jan 15 22:22:36.763: INFO: Condition Ready of node ci-prtest-cc63063-250-ig-n-v6l4 is false instead of true. Reason: KubeletNotReady, message: container runtime is down
STEP: Destroying namespace "e2e-tests-downward-api-42cqb" for this suite.
Jan 15 22:22:43.406: INFO: namespace: e2e-tests-downward-api-42cqb, resource: bindings, ignored listing per whitelist
Jan 15 22:22:44.167: INFO: namespace e2e-tests-downward-api-42cqb deletion completed in 7.403636725s

• [SLOW TEST:19.779 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide pod IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:68
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should project all components that make up the projection API [Conformance] [sig-storage] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:946
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:22:44.167: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should project all components that make up the projection API [Conformance] [sig-storage] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:946
STEP: Creating configMap with name configmap-projected-all-test-volume-9b8ffdc5-fa42-11e7-b6d5-0e5e74815504
STEP: Creating secret with name secret-projected-all-test-volume-9b8ffda0-fa42-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 15 22:22:44.332: INFO: Waiting up to 5m0s for pod "projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-dkt8l" to be "success or failure"
Jan 15 22:22:44.347: INFO: Pod "projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.04039ms
Jan 15 22:22:46.363: INFO: Pod "projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031285761s
Jan 15 22:22:48.379: INFO: Pod "projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046933157s
STEP: Saw pod success
Jan 15 22:22:48.379: INFO: Pod "projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:22:48.394: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 15 22:22:48.442: INFO: Waiting for pod projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:22:48.457: INFO: Pod projected-volume-9b8ffd5d-fa42-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:22:48.457: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dkt8l" for this suite.
Jan 15 22:22:55.252: INFO: namespace: e2e-tests-projected-dkt8l, resource: bindings, ignored listing per whitelist
Jan 15 22:22:55.890: INFO: namespace e2e-tests-projected-dkt8l deletion completed in 7.417401665s

• [SLOW TEST:11.723 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should project all components that make up the projection API [Conformance] [sig-storage] [Projection]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:946
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:492
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:22:55.890: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:492
Jan 15 22:22:56.023: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-a2902743-fa42-11e7-b6d5-0e5e74815504
STEP: Creating configMap with name cm-test-opt-upd-a290278b-fa42-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a2902743-fa42-11e7-b6d5-0e5e74815504
STEP: Updating configmap cm-test-opt-upd-a290278b-fa42-11e7-b6d5-0e5e74815504
STEP: Creating configMap with name cm-test-opt-create-a29027b1-fa42-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:24:29.293: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7f7ch" for this suite.
Jan 15 22:24:51.887: INFO: namespace: e2e-tests-projected-7f7ch, resource: bindings, ignored listing per whitelist
Jan 15 22:24:52.716: INFO: namespace e2e-tests-projected-7f7ch deletion completed in 23.407380825s

• [SLOW TEST:116.826 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:492
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:293
[BeforeEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:24:52.716: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:293
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-bvsrw.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-bvsrw.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-bvsrw.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-bvsrw.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-bvsrw.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-bvsrw.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 15 22:25:13.217: INFO: DNS probes using dns-test-e82b5bc6-fa42-11e7-b6d5-0e5e74815504 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:25:13.240: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-bvsrw" for this suite.
Jan 15 22:25:19.883: INFO: namespace: e2e-tests-dns-bvsrw, resource: bindings, ignored listing per whitelist
Jan 15 22:25:20.666: INFO: namespace e2e-tests-dns-bvsrw deletion completed in 7.411089478s

• [SLOW TEST:27.950 seconds]
[sig-network] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:293
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1467
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:25:20.667: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1458
[It] should update a single-container pod's image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1467
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 22:25:20.790: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tc47r'
Jan 15 22:25:21.559: INFO: stderr: ""
Jan 15 22:25:21.559: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 15 22:25:31.559: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tc47r -o json'
Jan 15 22:25:31.765: INFO: stderr: ""
Jan 15 22:25:31.765: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"privileged\"\n        },\n        \"creationTimestamp\": \"2018-01-15T22:25:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-tc47r\",\n        \"resourceVersion\": \"4663\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-tc47r/pods/e2e-test-nginx-pod\",\n        \"uid\": \"f9462a1a-fa42-11e7-a248-42010a8e0002\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"gcr.io/google-containers/nginx-slim-amd64:0.20\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"privileged\": false\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-92rcx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-56nfj\"\n            }\n        ],\n        \"nodeName\": \"ci-prtest-cc63063-250-ig-n-r6ln\",\n        \"nodeSelector\": {\n            \"role\": \"app\"\n        },\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-92rcx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-92rcx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-01-15T22:25:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-01-15T22:25:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-01-15T22:25:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ba8e0bba7fc07e2dc96d9cd7efc937083f2933107777400dc6e31808c1a7090f\",\n                \"image\": \"gcr.io/google-containers/nginx-slim-amd64:0.20\",\n                \"imageID\": \"docker-pullable://gcr.io/google-containers/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-01-15T22:25:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.142.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.4.12\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-01-15T22:25:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 15 22:25:31.765: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-tc47r'
Jan 15 22:25:32.028: INFO: stderr: ""
Jan 15 22:25:32.028: INFO: stdout: "pod \"e2e-test-nginx-pod\" replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image busybox
[AfterEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1463
Jan 15 22:25:32.045: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tc47r'
Jan 15 22:25:32.273: INFO: stderr: ""
Jan 15 22:25:32.273: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:25:32.273: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tc47r" for this suite.
Jan 15 22:25:55.523: INFO: namespace: e2e-tests-kubectl-tc47r, resource: bindings, ignored listing per whitelist
Jan 15 22:25:55.686: INFO: namespace e2e-tests-kubectl-tc47r deletion completed in 23.397942387s

• [SLOW TEST:35.020 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should update a single-container pod's image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1467
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:38
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:25:55.687: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:38
STEP: Creating a pod to test override arguments
Jan 15 22:25:55.829: INFO: Waiting up to 5m0s for pod "client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-containers-r9vn5" to be "success or failure"
Jan 15 22:25:55.844: INFO: Pod "client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.742087ms
Jan 15 22:25:57.860: INFO: Pod "client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030755789s
STEP: Saw pod success
Jan 15 22:25:57.860: INFO: Pod "client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:25:57.876: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:25:57.936: INFO: Waiting for pod client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:25:57.951: INFO: Pod client-containers-0db96139-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:25:57.951: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-r9vn5" for this suite.
Jan 15 22:26:05.068: INFO: namespace: e2e-tests-containers-r9vn5, resource: bindings, ignored listing per whitelist
Jan 15 22:26:05.408: INFO: namespace e2e-tests-containers-r9vn5 deletion completed in 7.440982708s

• [SLOW TEST:9.721 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be able to override the image's default arguments (docker cmd) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:38
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:100
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:26:05.408: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:100
STEP: Creating the pod
Jan 15 22:26:08.175: INFO: Successfully updated pod "labelsupdate13874f71-fa43-11e7-b6d5-0e5e74815504"
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:26:12.243: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mgs7z" for this suite.
Jan 15 22:26:35.356: INFO: namespace: e2e-tests-downward-api-mgs7z, resource: bindings, ignored listing per whitelist
Jan 15 22:26:35.680: INFO: namespace e2e-tests-downward-api-mgs7z deletion completed in 23.421030335s

• [SLOW TEST:30.272 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:100
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:111
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:26:35.680: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:111
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 15 22:26:35.825: INFO: Waiting up to 5m0s for pod "pod-25907eeb-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-twrnl" to be "success or failure"
Jan 15 22:26:35.840: INFO: Pod "pod-25907eeb-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.874305ms
Jan 15 22:26:37.856: INFO: Pod "pod-25907eeb-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030795194s
STEP: Saw pod success
Jan 15 22:26:37.856: INFO: Pod "pod-25907eeb-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:26:37.871: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-25907eeb-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:26:37.914: INFO: Waiting for pod pod-25907eeb-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:26:37.929: INFO: Pod pod-25907eeb-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:26:37.929: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-twrnl" for this suite.
Jan 15 22:26:45.161: INFO: namespace: e2e-tests-emptydir-twrnl, resource: bindings, ignored listing per whitelist
Jan 15 22:26:45.341: INFO: namespace e2e-tests-emptydir-twrnl deletion completed in 7.395284567s

• [SLOW TEST:9.660 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:111
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:26:45.341: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
Jan 15 22:26:45.472: INFO: (0) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 20.09264ms)
Jan 15 22:26:45.489: INFO: (1) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.524178ms)
Jan 15 22:26:45.505: INFO: (2) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.404453ms)
Jan 15 22:26:45.522: INFO: (3) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.65009ms)
Jan 15 22:26:45.538: INFO: (4) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.22079ms)
Jan 15 22:26:45.555: INFO: (5) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.140191ms)
Jan 15 22:26:45.571: INFO: (6) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.323177ms)
Jan 15 22:26:45.587: INFO: (7) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.344367ms)
Jan 15 22:26:45.604: INFO: (8) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.690711ms)
Jan 15 22:26:45.620: INFO: (9) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.475308ms)
Jan 15 22:26:45.637: INFO: (10) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.34236ms)
Jan 15 22:26:45.653: INFO: (11) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.653562ms)
Jan 15 22:26:45.670: INFO: (12) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.428812ms)
Jan 15 22:26:45.686: INFO: (13) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.238275ms)
Jan 15 22:26:45.703: INFO: (14) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.404305ms)
Jan 15 22:26:45.720: INFO: (15) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.421357ms)
Jan 15 22:26:45.736: INFO: (16) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.196716ms)
Jan 15 22:26:45.753: INFO: (17) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.882737ms)
Jan 15 22:26:45.770: INFO: (18) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.696996ms)
Jan 15 22:26:45.786: INFO: (19) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.096903ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:26:45.786: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-vl94p" for this suite.
Jan 15 22:26:53.205: INFO: namespace: e2e-tests-proxy-vl94p, resource: bindings, ignored listing per whitelist
Jan 15 22:26:53.205: INFO: namespace e2e-tests-proxy-vl94p deletion completed in 7.402957406s

• [SLOW TEST:7.864 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:70
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:26:53.205: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:70
Jan 15 22:26:53.350: INFO: (0) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.391977ms)
Jan 15 22:26:53.367: INFO: (1) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.440441ms)
Jan 15 22:26:53.383: INFO: (2) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.291659ms)
Jan 15 22:26:53.400: INFO: (3) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.858034ms)
Jan 15 22:26:53.416: INFO: (4) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.433034ms)
Jan 15 22:26:53.433: INFO: (5) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.422197ms)
Jan 15 22:26:53.449: INFO: (6) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.256779ms)
Jan 15 22:26:53.466: INFO: (7) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.319659ms)
Jan 15 22:26:53.482: INFO: (8) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.680762ms)
Jan 15 22:26:53.499: INFO: (9) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.663034ms)
Jan 15 22:26:53.516: INFO: (10) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.729979ms)
Jan 15 22:26:53.532: INFO: (11) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.467757ms)
Jan 15 22:26:53.549: INFO: (12) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.583243ms)
Jan 15 22:26:53.565: INFO: (13) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.4092ms)
Jan 15 22:26:53.581: INFO: (14) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.188252ms)
Jan 15 22:26:53.597: INFO: (15) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.987023ms)
Jan 15 22:26:53.614: INFO: (16) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.140006ms)
Jan 15 22:26:53.630: INFO: (17) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.998249ms)
Jan 15 22:26:53.646: INFO: (18) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.124079ms)
Jan 15 22:26:53.662: INFO: (19) /api/v1/nodes/ci-prtest-cc63063-250-ig-n-prvd/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.203638ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:26:53.662: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-s45xv" for this suite.
Jan 15 22:27:00.364: INFO: namespace: e2e-tests-proxy-s45xv, resource: bindings, ignored listing per whitelist
Jan 15 22:27:01.094: INFO: namespace e2e-tests-proxy-s45xv deletion completed in 7.41593442s

• [SLOW TEST:7.889 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:70
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:27:01.094: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:303
[It] should create and stop a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
STEP: creating a replication controller
Jan 15 22:27:01.193: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:02.018: INFO: stderr: ""
Jan 15 22:27:02.018: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:27:02.018: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:02.230: INFO: stderr: ""
Jan 15 22:27:02.230: INFO: stdout: "update-demo-nautilus-9659z update-demo-nautilus-fj9ql "
Jan 15 22:27:02.230: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9659z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:02.439: INFO: stderr: ""
Jan 15 22:27:02.439: INFO: stdout: ""
Jan 15 22:27:02.439: INFO: update-demo-nautilus-9659z is created but not running
Jan 15 22:27:07.439: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:07.648: INFO: stderr: ""
Jan 15 22:27:07.648: INFO: stdout: "update-demo-nautilus-9659z update-demo-nautilus-fj9ql "
Jan 15 22:27:07.648: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9659z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:07.853: INFO: stderr: ""
Jan 15 22:27:07.853: INFO: stdout: "true"
Jan 15 22:27:07.853: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9659z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:08.060: INFO: stderr: ""
Jan 15 22:27:08.060: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:27:08.060: INFO: validating pod update-demo-nautilus-9659z
Jan 15 22:27:08.093: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:27:08.093: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:27:08.093: INFO: update-demo-nautilus-9659z is verified up and running
Jan 15 22:27:08.093: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fj9ql -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:08.300: INFO: stderr: ""
Jan 15 22:27:08.300: INFO: stdout: "true"
Jan 15 22:27:08.300: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-fj9ql -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:08.509: INFO: stderr: ""
Jan 15 22:27:08.509: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:27:08.509: INFO: validating pod update-demo-nautilus-fj9ql
Jan 15 22:27:08.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:27:08.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:27:08.543: INFO: update-demo-nautilus-fj9ql is verified up and running
STEP: using delete to clean up resources
Jan 15 22:27:08.543: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:08.832: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:27:08.832: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Jan 15 22:27:08.832: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vv7pg'
Jan 15 22:27:09.016: INFO: stderr: "No resources found.\n"
Jan 15 22:27:09.016: INFO: stdout: ""
Jan 15 22:27:09.016: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-vv7pg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 15 22:27:09.225: INFO: stderr: ""
Jan 15 22:27:09.225: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:27:09.225: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vv7pg" for this suite.
Jan 15 22:27:30.214: INFO: namespace: e2e-tests-kubectl-vv7pg, resource: bindings, ignored listing per whitelist
Jan 15 22:27:30.652: INFO: namespace e2e-tests-kubectl-vv7pg deletion completed in 21.410131596s

• [SLOW TEST:29.558 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create and stop a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Projected 
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:173
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:27:30.652: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:173
Jan 15 22:27:30.779: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-4654b5db-fa43-11e7-b6d5-0e5e74815504
STEP: Creating secret with name s-test-opt-upd-4654b619-fa43-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4654b5db-fa43-11e7-b6d5-0e5e74815504
STEP: Updating secret s-test-opt-upd-4654b619-fa43-11e7-b6d5-0e5e74815504
STEP: Creating secret with name s-test-opt-create-4654b64f-fa43-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:28:53.995: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qd2bs" for this suite.
Jan 15 22:29:17.351: INFO: namespace: e2e-tests-projected-qd2bs, resource: bindings, ignored listing per whitelist
Jan 15 22:29:17.429: INFO: namespace e2e-tests-projected-qd2bs deletion completed in 23.418387371s

• [SLOW TEST:106.777 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  optional updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:173
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1039
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:29:17.429: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1029
STEP: creating the pod
Jan 15 22:29:17.549: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:17.903: INFO: stderr: ""
Jan 15 22:29:17.903: INFO: stdout: "pod \"pause\" created\n"
Jan 15 22:29:17.903: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 15 22:29:17.903: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-cg2s6" to be "running and ready"
Jan 15 22:29:17.918: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.704028ms
Jan 15 22:29:19.934: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.031287263s
Jan 15 22:29:19.934: INFO: Pod "pause" satisfied condition "running and ready"
Jan 15 22:29:19.934: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1039
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 15 22:29:19.934: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:20.122: INFO: stderr: ""
Jan 15 22:29:20.122: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 15 22:29:20.122: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:20.328: INFO: stderr: ""
Jan 15 22:29:20.329: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          3s        testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 15 22:29:20.329: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:20.516: INFO: stderr: ""
Jan 15 22:29:20.516: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 15 22:29:20.516: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:20.733: INFO: stderr: ""
Jan 15 22:29:20.733: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          3s        \n"
[AfterEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1035
STEP: using delete to clean up resources
Jan 15 22:29:20.733: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:20.959: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:29:20.959: INFO: stdout: "pod \"pause\" deleted\n"
Jan 15 22:29:20.959: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-cg2s6'
Jan 15 22:29:21.140: INFO: stderr: "No resources found.\n"
Jan 15 22:29:21.140: INFO: stdout: ""
Jan 15 22:29:21.140: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-cg2s6 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 15 22:29:21.301: INFO: stderr: ""
Jan 15 22:29:21.301: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:29:21.301: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cg2s6" for this suite.
Jan 15 22:29:28.238: INFO: namespace: e2e-tests-kubectl-cg2s6, resource: bindings, ignored listing per whitelist
Jan 15 22:29:28.751: INFO: namespace e2e-tests-kubectl-cg2s6 deletion completed in 7.434120228s

• [SLOW TEST:11.322 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should update the label on a resource [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1039
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:58
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:29:28.751: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:58
STEP: Creating a pod to test override all
Jan 15 22:29:28.878: INFO: Waiting up to 5m0s for pod "client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-containers-7kn7h" to be "success or failure"
Jan 15 22:29:28.893: INFO: Pod "client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.960427ms
Jan 15 22:29:30.909: INFO: Pod "client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031189448s
Jan 15 22:29:32.925: INFO: Pod "client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04721786s
STEP: Saw pod success
Jan 15 22:29:32.925: INFO: Pod "client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:29:32.941: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:29:32.990: INFO: Waiting for pod client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:29:33.005: INFO: Pod client-containers-8cb64997-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:29:33.005: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7kn7h" for this suite.
Jan 15 22:29:40.343: INFO: namespace: e2e-tests-containers-7kn7h, resource: bindings, ignored listing per whitelist
Jan 15 22:29:40.430: INFO: namespace e2e-tests-containers-7kn7h deletion completed in 7.408864905s

• [SLOW TEST:11.679 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be able to override the image's default command and arguments [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:58
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:688
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:29:40.430: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:688
STEP: Creating configMap with name projected-configmap-test-volume-93abbea7-fa43-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:29:40.568: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-s6h84" to be "success or failure"
Jan 15 22:29:40.583: INFO: Pod "pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.095714ms
Jan 15 22:29:42.599: INFO: Pod "pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030708256s
STEP: Saw pod success
Jan 15 22:29:42.599: INFO: Pod "pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:29:42.615: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:29:42.656: INFO: Waiting for pod pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:29:42.671: INFO: Pod pod-projected-configmaps-93ae1ef4-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:29:42.671: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s6h84" for this suite.
Jan 15 22:29:49.999: INFO: namespace: e2e-tests-projected-s6h84, resource: bindings, ignored listing per whitelist
Jan 15 22:29:50.088: INFO: namespace e2e-tests-projected-s6h84 deletion completed in 7.401725691s

• [SLOW TEST:9.658 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:688
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:29:50.089: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1173
[It] should create an rc or deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 22:29:50.220: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-qv8qk'
Jan 15 22:29:50.953: INFO: stderr: ""
Jan 15 22:29:50.953: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
Jan 15 22:29:50.969: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qv8qk'
Jan 15 22:29:54.362: INFO: stderr: ""
Jan 15 22:29:54.362: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:29:54.362: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qv8qk" for this suite.
Jan 15 22:30:01.121: INFO: namespace: e2e-tests-kubectl-qv8qk, resource: bindings, ignored listing per whitelist
Jan 15 22:30:01.782: INFO: namespace e2e-tests-kubectl-qv8qk deletion completed in 7.404718308s

• [SLOW TEST:11.694 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create an rc or deployment from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
------------------------------
SS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:75
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:30:01.783: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:75
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 15 22:30:01.916: INFO: Waiting up to 5m0s for pod "pod-a0664206-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-h6bn9" to be "success or failure"
Jan 15 22:30:01.955: INFO: Pod "pod-a0664206-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 38.869846ms
Jan 15 22:30:03.971: INFO: Pod "pod-a0664206-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054709546s
STEP: Saw pod success
Jan 15 22:30:03.971: INFO: Pod "pod-a0664206-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:30:03.986: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-a0664206-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:30:04.028: INFO: Waiting for pod pod-a0664206-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:30:04.043: INFO: Pod pod-a0664206-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:30:04.043: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h6bn9" for this suite.
Jan 15 22:30:10.701: INFO: namespace: e2e-tests-emptydir-h6bn9, resource: bindings, ignored listing per whitelist
Jan 15 22:30:11.461: INFO: namespace e2e-tests-emptydir-h6bn9 deletion completed in 7.402249922s

• [SLOW TEST:9.678 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:75
------------------------------
SSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:95
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:30:11.461: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:95
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 15 22:30:11.553: INFO: Waiting up to 5m0s for pod "pod-a6251cee-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-85qxd" to be "success or failure"
Jan 15 22:30:11.570: INFO: Pod "pod-a6251cee-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 17.749606ms
Jan 15 22:30:13.586: INFO: Pod "pod-a6251cee-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033402227s
STEP: Saw pod success
Jan 15 22:30:13.586: INFO: Pod "pod-a6251cee-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:30:13.601: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-a6251cee-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:30:13.661: INFO: Waiting for pod pod-a6251cee-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:30:13.676: INFO: Pod pod-a6251cee-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:30:13.676: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-85qxd" for this suite.
Jan 15 22:30:21.008: INFO: namespace: e2e-tests-emptydir-85qxd, resource: bindings, ignored listing per whitelist
Jan 15 22:30:21.098: INFO: namespace e2e-tests-emptydir-85qxd deletion completed in 7.405899248s

• [SLOW TEST:9.637 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:95
------------------------------
[sig-network] Service endpoints latency 
  should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service_latency.go:48
[BeforeEach] [sig-network] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:30:21.098: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service_latency.go:48
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-j2s4t
I0115 22:30:21.221031   17569 runners.go:177] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-j2s4t, replica count: 1
I0115 22:30:22.221271   17569 runners.go:177] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 15 22:30:22.342: INFO: Created: latency-svc-jfbdk
Jan 15 22:30:22.358: INFO: Got endpoints: latency-svc-jfbdk [37.364946ms]
Jan 15 22:30:22.383: INFO: Created: latency-svc-t5t2x
Jan 15 22:30:22.407: INFO: Got endpoints: latency-svc-t5t2x [48.292262ms]
Jan 15 22:30:22.414: INFO: Created: latency-svc-nphsg
Jan 15 22:30:22.427: INFO: Created: latency-svc-plhjk
Jan 15 22:30:22.430: INFO: Created: latency-svc-x98ss
Jan 15 22:30:22.435: INFO: Created: latency-svc-vqv4z
Jan 15 22:30:22.441: INFO: Created: latency-svc-nx6sn
Jan 15 22:30:22.446: INFO: Got endpoints: latency-svc-nx6sn [87.080661ms]
Jan 15 22:30:22.446: INFO: Got endpoints: latency-svc-x98ss [87.395168ms]
Jan 15 22:30:22.446: INFO: Got endpoints: latency-svc-nphsg [87.424969ms]
Jan 15 22:30:22.446: INFO: Got endpoints: latency-svc-vqv4z [87.37236ms]
Jan 15 22:30:22.446: INFO: Got endpoints: latency-svc-plhjk [87.487366ms]
Jan 15 22:30:22.449: INFO: Created: latency-svc-j67cx
Jan 15 22:30:22.455: INFO: Got endpoints: latency-svc-j67cx [96.790897ms]
Jan 15 22:30:22.459: INFO: Created: latency-svc-25h2k
Jan 15 22:30:22.474: INFO: Got endpoints: latency-svc-25h2k [115.417319ms]
Jan 15 22:30:22.478: INFO: Created: latency-svc-h592r
Jan 15 22:30:22.480: INFO: Created: latency-svc-bk99b
Jan 15 22:30:22.480: INFO: Got endpoints: latency-svc-h592r [121.409368ms]
Jan 15 22:30:22.482: INFO: Created: latency-svc-zllp2
Jan 15 22:30:22.482: INFO: Got endpoints: latency-svc-bk99b [123.719748ms]
Jan 15 22:30:22.488: INFO: Created: latency-svc-lsmwn
Jan 15 22:30:22.492: INFO: Got endpoints: latency-svc-zllp2 [133.516527ms]
Jan 15 22:30:22.497: INFO: Got endpoints: latency-svc-lsmwn [41.647803ms]
Jan 15 22:30:22.500: INFO: Created: latency-svc-glqns
Jan 15 22:30:22.504: INFO: Created: latency-svc-bwkg9
Jan 15 22:30:22.506: INFO: Got endpoints: latency-svc-glqns [147.347268ms]
Jan 15 22:30:22.512: INFO: Created: latency-svc-tjtcv
Jan 15 22:30:22.513: INFO: Got endpoints: latency-svc-bwkg9 [154.427092ms]
Jan 15 22:30:22.520: INFO: Got endpoints: latency-svc-tjtcv [161.657757ms]
Jan 15 22:30:22.521: INFO: Created: latency-svc-cvv2p
Jan 15 22:30:22.529: INFO: Created: latency-svc-kjt5z
Jan 15 22:30:22.531: INFO: Created: latency-svc-79s9z
Jan 15 22:30:22.533: INFO: Got endpoints: latency-svc-cvv2p [126.08286ms]
Jan 15 22:30:22.537: INFO: Got endpoints: latency-svc-kjt5z [90.692264ms]
Jan 15 22:30:22.539: INFO: Got endpoints: latency-svc-79s9z [93.343184ms]
Jan 15 22:30:22.540: INFO: Created: latency-svc-r6cxn
Jan 15 22:30:22.546: INFO: Got endpoints: latency-svc-r6cxn [99.521691ms]
Jan 15 22:30:22.546: INFO: Created: latency-svc-ftpf8
Jan 15 22:30:22.551: INFO: Got endpoints: latency-svc-ftpf8 [104.726339ms]
Jan 15 22:30:22.564: INFO: Created: latency-svc-5fhkf
Jan 15 22:30:22.572: INFO: Got endpoints: latency-svc-5fhkf [125.602201ms]
Jan 15 22:30:22.578: INFO: Created: latency-svc-l9krz
Jan 15 22:30:22.578: INFO: Created: latency-svc-mmsn5
Jan 15 22:30:22.578: INFO: Got endpoints: latency-svc-l9krz [219.127101ms]
Jan 15 22:30:22.578: INFO: Created: latency-svc-dlvch
Jan 15 22:30:22.581: INFO: Created: latency-svc-h8jb4
Jan 15 22:30:22.582: INFO: Got endpoints: latency-svc-dlvch [101.643358ms]
Jan 15 22:30:22.582: INFO: Got endpoints: latency-svc-mmsn5 [107.843971ms]
Jan 15 22:30:22.588: INFO: Created: latency-svc-mdn2x
Jan 15 22:30:22.597: INFO: Created: latency-svc-qhd6t
Jan 15 22:30:22.599: INFO: Got endpoints: latency-svc-mdn2x [106.257339ms]
Jan 15 22:30:22.599: INFO: Got endpoints: latency-svc-h8jb4 [116.185394ms]
Jan 15 22:30:22.607: INFO: Created: latency-svc-q86ln
Jan 15 22:30:22.607: INFO: Got endpoints: latency-svc-qhd6t [110.280716ms]
Jan 15 22:30:22.611: INFO: Created: latency-svc-hjcf2
Jan 15 22:30:22.616: INFO: Got endpoints: latency-svc-q86ln [109.880857ms]
Jan 15 22:30:22.616: INFO: Created: latency-svc-7wrzh
Jan 15 22:30:22.620: INFO: Created: latency-svc-4l5pp
Jan 15 22:30:22.623: INFO: Created: latency-svc-zv5fn
Jan 15 22:30:22.636: INFO: Got endpoints: latency-svc-zv5fn [99.034948ms]
Jan 15 22:30:22.636: INFO: Got endpoints: latency-svc-hjcf2 [122.579368ms]
Jan 15 22:30:22.641: INFO: Created: latency-svc-9wqdl
Jan 15 22:30:22.646: INFO: Got endpoints: latency-svc-4l5pp [112.663896ms]
Jan 15 22:30:22.646: INFO: Got endpoints: latency-svc-7wrzh [125.24194ms]
Jan 15 22:30:22.648: INFO: Got endpoints: latency-svc-9wqdl [108.526245ms]
Jan 15 22:30:22.654: INFO: Created: latency-svc-ptb7w
Jan 15 22:30:22.658: INFO: Got endpoints: latency-svc-ptb7w [112.384606ms]
Jan 15 22:30:22.668: INFO: Created: latency-svc-bcstp
Jan 15 22:30:22.668: INFO: Created: latency-svc-pgpmv
Jan 15 22:30:22.674: INFO: Got endpoints: latency-svc-bcstp [123.202997ms]
Jan 15 22:30:22.675: INFO: Got endpoints: latency-svc-pgpmv [103.542119ms]
Jan 15 22:30:22.681: INFO: Created: latency-svc-x6cl6
Jan 15 22:30:22.684: INFO: Created: latency-svc-xjmxm
Jan 15 22:30:22.694: INFO: Created: latency-svc-2mmmf
Jan 15 22:30:22.701: INFO: Got endpoints: latency-svc-x6cl6 [55.395098ms]
Jan 15 22:30:22.704: INFO: Created: latency-svc-phxmt
Jan 15 22:30:22.718: INFO: Created: latency-svc-mg6sn
Jan 15 22:30:22.722: INFO: Created: latency-svc-jmb4k
Jan 15 22:30:22.733: INFO: Created: latency-svc-5wh7v
Jan 15 22:30:22.740: INFO: Created: latency-svc-6w86t
Jan 15 22:30:22.748: INFO: Created: latency-svc-xh7vw
Jan 15 22:30:22.751: INFO: Created: latency-svc-x8qcl
Jan 15 22:30:22.754: INFO: Got endpoints: latency-svc-xjmxm [172.06454ms]
Jan 15 22:30:22.762: INFO: Created: latency-svc-khmcl
Jan 15 22:30:22.769: INFO: Created: latency-svc-mpssl
Jan 15 22:30:22.776: INFO: Created: latency-svc-z6t9j
Jan 15 22:30:22.779: INFO: Created: latency-svc-79c88
Jan 15 22:30:22.787: INFO: Created: latency-svc-6w4mn
Jan 15 22:30:22.792: INFO: Created: latency-svc-mxh6l
Jan 15 22:30:22.795: INFO: Created: latency-svc-2d9bw
Jan 15 22:30:22.798: INFO: Got endpoints: latency-svc-2mmmf [216.258017ms]
Jan 15 22:30:22.820: INFO: Created: latency-svc-xrsgb
Jan 15 22:30:22.847: INFO: Got endpoints: latency-svc-phxmt [248.873103ms]
Jan 15 22:30:22.867: INFO: Created: latency-svc-r4t5w
Jan 15 22:30:22.900: INFO: Got endpoints: latency-svc-mg6sn [301.63105ms]
Jan 15 22:30:22.920: INFO: Created: latency-svc-9bzmp
Jan 15 22:30:22.947: INFO: Got endpoints: latency-svc-jmb4k [339.864601ms]
Jan 15 22:30:22.966: INFO: Created: latency-svc-gnpw2
Jan 15 22:30:22.997: INFO: Got endpoints: latency-svc-5wh7v [381.34606ms]
Jan 15 22:30:23.016: INFO: Created: latency-svc-772p7
Jan 15 22:30:23.049: INFO: Got endpoints: latency-svc-6w86t [413.725039ms]
Jan 15 22:30:23.070: INFO: Created: latency-svc-kwpvc
Jan 15 22:30:23.098: INFO: Got endpoints: latency-svc-xh7vw [462.402793ms]
Jan 15 22:30:23.118: INFO: Created: latency-svc-x7mlp
Jan 15 22:30:23.148: INFO: Got endpoints: latency-svc-x8qcl [499.559417ms]
Jan 15 22:30:23.168: INFO: Created: latency-svc-5cdjh
Jan 15 22:30:23.198: INFO: Got endpoints: latency-svc-khmcl [552.279672ms]
Jan 15 22:30:23.218: INFO: Created: latency-svc-v2jnj
Jan 15 22:30:23.250: INFO: Got endpoints: latency-svc-mpssl [671.690394ms]
Jan 15 22:30:23.271: INFO: Created: latency-svc-x22k2
Jan 15 22:30:23.299: INFO: Got endpoints: latency-svc-z6t9j [640.613934ms]
Jan 15 22:30:23.319: INFO: Created: latency-svc-n76xs
Jan 15 22:30:23.348: INFO: Got endpoints: latency-svc-79c88 [673.673014ms]
Jan 15 22:30:23.367: INFO: Created: latency-svc-6p4lf
Jan 15 22:30:23.397: INFO: Got endpoints: latency-svc-6w4mn [722.16344ms]
Jan 15 22:30:23.418: INFO: Created: latency-svc-fjjts
Jan 15 22:30:23.448: INFO: Got endpoints: latency-svc-mxh6l [746.932557ms]
Jan 15 22:30:23.467: INFO: Created: latency-svc-jlh7r
Jan 15 22:30:23.498: INFO: Got endpoints: latency-svc-2d9bw [744.410435ms]
Jan 15 22:30:23.518: INFO: Created: latency-svc-lcwcz
Jan 15 22:30:23.548: INFO: Got endpoints: latency-svc-xrsgb [749.516272ms]
Jan 15 22:30:23.569: INFO: Created: latency-svc-ldgzm
Jan 15 22:30:23.597: INFO: Got endpoints: latency-svc-r4t5w [750.020773ms]
Jan 15 22:30:23.617: INFO: Created: latency-svc-4zbtk
Jan 15 22:30:23.648: INFO: Got endpoints: latency-svc-9bzmp [747.720466ms]
Jan 15 22:30:23.671: INFO: Created: latency-svc-rrfbv
Jan 15 22:30:23.700: INFO: Got endpoints: latency-svc-gnpw2 [752.949031ms]
Jan 15 22:30:23.723: INFO: Created: latency-svc-f49q9
Jan 15 22:30:23.747: INFO: Got endpoints: latency-svc-772p7 [749.959684ms]
Jan 15 22:30:23.769: INFO: Created: latency-svc-98vtd
Jan 15 22:30:23.798: INFO: Got endpoints: latency-svc-kwpvc [748.789313ms]
Jan 15 22:30:23.819: INFO: Created: latency-svc-4dx84
Jan 15 22:30:23.850: INFO: Got endpoints: latency-svc-x7mlp [751.681546ms]
Jan 15 22:30:23.870: INFO: Created: latency-svc-zbhg9
Jan 15 22:30:23.898: INFO: Got endpoints: latency-svc-5cdjh [750.802629ms]
Jan 15 22:30:23.920: INFO: Created: latency-svc-vg77r
Jan 15 22:30:23.949: INFO: Got endpoints: latency-svc-v2jnj [751.262409ms]
Jan 15 22:30:23.974: INFO: Created: latency-svc-jbjql
Jan 15 22:30:23.998: INFO: Got endpoints: latency-svc-x22k2 [748.866697ms]
Jan 15 22:30:24.020: INFO: Created: latency-svc-lzqfn
Jan 15 22:30:24.049: INFO: Got endpoints: latency-svc-n76xs [750.505587ms]
Jan 15 22:30:24.070: INFO: Created: latency-svc-tqsbc
Jan 15 22:30:24.099: INFO: Got endpoints: latency-svc-6p4lf [751.730442ms]
Jan 15 22:30:24.121: INFO: Created: latency-svc-4clvk
Jan 15 22:30:24.149: INFO: Got endpoints: latency-svc-fjjts [751.955072ms]
Jan 15 22:30:24.173: INFO: Created: latency-svc-pd25f
Jan 15 22:30:24.201: INFO: Got endpoints: latency-svc-jlh7r [752.497632ms]
Jan 15 22:30:24.225: INFO: Created: latency-svc-pqbls
Jan 15 22:30:24.251: INFO: Got endpoints: latency-svc-lcwcz [752.30394ms]
Jan 15 22:30:24.278: INFO: Created: latency-svc-5lth9
Jan 15 22:30:24.299: INFO: Got endpoints: latency-svc-ldgzm [750.872745ms]
Jan 15 22:30:24.322: INFO: Created: latency-svc-xqjdk
Jan 15 22:30:24.350: INFO: Got endpoints: latency-svc-4zbtk [752.846774ms]
Jan 15 22:30:24.371: INFO: Created: latency-svc-pzxct
Jan 15 22:30:24.399: INFO: Got endpoints: latency-svc-rrfbv [751.122225ms]
Jan 15 22:30:24.422: INFO: Created: latency-svc-sm2hh
Jan 15 22:30:24.452: INFO: Got endpoints: latency-svc-f49q9 [752.026966ms]
Jan 15 22:30:24.473: INFO: Created: latency-svc-zzgbk
Jan 15 22:30:24.499: INFO: Got endpoints: latency-svc-98vtd [751.924346ms]
Jan 15 22:30:24.521: INFO: Created: latency-svc-5d2r7
Jan 15 22:30:24.548: INFO: Got endpoints: latency-svc-4dx84 [750.113973ms]
Jan 15 22:30:24.569: INFO: Created: latency-svc-dwjnh
Jan 15 22:30:24.598: INFO: Got endpoints: latency-svc-zbhg9 [748.174902ms]
Jan 15 22:30:24.620: INFO: Created: latency-svc-mnjl4
Jan 15 22:30:24.650: INFO: Got endpoints: latency-svc-vg77r [751.579937ms]
Jan 15 22:30:24.671: INFO: Created: latency-svc-z4mdl
Jan 15 22:30:24.699: INFO: Got endpoints: latency-svc-jbjql [750.23557ms]
Jan 15 22:30:24.722: INFO: Created: latency-svc-b9rhk
Jan 15 22:30:24.748: INFO: Got endpoints: latency-svc-lzqfn [749.482118ms]
Jan 15 22:30:24.769: INFO: Created: latency-svc-xcn2z
Jan 15 22:30:24.800: INFO: Got endpoints: latency-svc-tqsbc [750.837565ms]
Jan 15 22:30:24.823: INFO: Created: latency-svc-xzrhk
Jan 15 22:30:24.850: INFO: Got endpoints: latency-svc-4clvk [750.150391ms]
Jan 15 22:30:24.871: INFO: Created: latency-svc-2n69z
Jan 15 22:30:24.898: INFO: Got endpoints: latency-svc-pd25f [748.984422ms]
Jan 15 22:30:24.921: INFO: Created: latency-svc-ghqds
Jan 15 22:30:24.949: INFO: Got endpoints: latency-svc-pqbls [748.418119ms]
Jan 15 22:30:24.972: INFO: Created: latency-svc-sd2st
Jan 15 22:30:24.998: INFO: Got endpoints: latency-svc-5lth9 [747.640956ms]
Jan 15 22:30:25.022: INFO: Created: latency-svc-jzjrh
Jan 15 22:30:25.048: INFO: Got endpoints: latency-svc-xqjdk [749.832689ms]
Jan 15 22:30:25.070: INFO: Created: latency-svc-crfmb
Jan 15 22:30:25.099: INFO: Got endpoints: latency-svc-pzxct [748.197453ms]
Jan 15 22:30:25.119: INFO: Created: latency-svc-pfzwf
Jan 15 22:30:25.150: INFO: Got endpoints: latency-svc-sm2hh [750.913682ms]
Jan 15 22:30:25.171: INFO: Created: latency-svc-p6jqm
Jan 15 22:30:25.199: INFO: Got endpoints: latency-svc-zzgbk [746.555991ms]
Jan 15 22:30:25.220: INFO: Created: latency-svc-5n7mn
Jan 15 22:30:25.259: INFO: Got endpoints: latency-svc-5d2r7 [759.649983ms]
Jan 15 22:30:25.292: INFO: Created: latency-svc-cvcv6
Jan 15 22:30:25.299: INFO: Got endpoints: latency-svc-dwjnh [750.277494ms]
Jan 15 22:30:25.320: INFO: Created: latency-svc-89mk5
Jan 15 22:30:25.348: INFO: Got endpoints: latency-svc-mnjl4 [749.896741ms]
Jan 15 22:30:25.368: INFO: Created: latency-svc-2fgqt
Jan 15 22:30:25.398: INFO: Got endpoints: latency-svc-z4mdl [748.145943ms]
Jan 15 22:30:25.420: INFO: Created: latency-svc-hd8hf
Jan 15 22:30:25.451: INFO: Got endpoints: latency-svc-b9rhk [751.043075ms]
Jan 15 22:30:25.475: INFO: Created: latency-svc-hgbvw
Jan 15 22:30:25.499: INFO: Got endpoints: latency-svc-xcn2z [751.483889ms]
Jan 15 22:30:25.521: INFO: Created: latency-svc-c7dm6
Jan 15 22:30:25.548: INFO: Got endpoints: latency-svc-xzrhk [748.184345ms]
Jan 15 22:30:25.572: INFO: Created: latency-svc-slw5q
Jan 15 22:30:25.601: INFO: Got endpoints: latency-svc-2n69z [751.20304ms]
Jan 15 22:30:25.625: INFO: Created: latency-svc-srkgm
Jan 15 22:30:25.651: INFO: Got endpoints: latency-svc-ghqds [752.566905ms]
Jan 15 22:30:25.676: INFO: Created: latency-svc-khdgr
Jan 15 22:30:25.699: INFO: Got endpoints: latency-svc-sd2st [750.384309ms]
Jan 15 22:30:25.721: INFO: Created: latency-svc-2gj2l
Jan 15 22:30:25.749: INFO: Got endpoints: latency-svc-jzjrh [750.986753ms]
Jan 15 22:30:25.771: INFO: Created: latency-svc-jjq2q
Jan 15 22:30:25.799: INFO: Got endpoints: latency-svc-crfmb [750.404767ms]
Jan 15 22:30:25.820: INFO: Created: latency-svc-mbd7q
Jan 15 22:30:25.848: INFO: Got endpoints: latency-svc-pfzwf [749.637804ms]
Jan 15 22:30:25.872: INFO: Created: latency-svc-4tvgx
Jan 15 22:30:25.900: INFO: Got endpoints: latency-svc-p6jqm [749.797993ms]
Jan 15 22:30:25.925: INFO: Created: latency-svc-lsbrn
Jan 15 22:30:25.948: INFO: Got endpoints: latency-svc-5n7mn [749.438521ms]
Jan 15 22:30:25.969: INFO: Created: latency-svc-j9jbq
Jan 15 22:30:25.999: INFO: Got endpoints: latency-svc-cvcv6 [740.435931ms]
Jan 15 22:30:26.020: INFO: Created: latency-svc-nzk59
Jan 15 22:30:26.050: INFO: Got endpoints: latency-svc-89mk5 [751.20645ms]
Jan 15 22:30:26.075: INFO: Created: latency-svc-f24j8
Jan 15 22:30:26.099: INFO: Got endpoints: latency-svc-2fgqt [750.59636ms]
Jan 15 22:30:26.122: INFO: Created: latency-svc-h7dtx
Jan 15 22:30:26.149: INFO: Got endpoints: latency-svc-hd8hf [751.139282ms]
Jan 15 22:30:26.173: INFO: Created: latency-svc-gcm9m
Jan 15 22:30:26.198: INFO: Got endpoints: latency-svc-hgbvw [747.80334ms]
Jan 15 22:30:26.222: INFO: Created: latency-svc-7hx2v
Jan 15 22:30:26.248: INFO: Got endpoints: latency-svc-c7dm6 [748.130066ms]
Jan 15 22:30:26.268: INFO: Created: latency-svc-plzl7
Jan 15 22:30:26.298: INFO: Got endpoints: latency-svc-slw5q [750.126061ms]
Jan 15 22:30:26.320: INFO: Created: latency-svc-z62zs
Jan 15 22:30:26.349: INFO: Got endpoints: latency-svc-srkgm [748.017584ms]
Jan 15 22:30:26.371: INFO: Created: latency-svc-j69dp
Jan 15 22:30:26.398: INFO: Got endpoints: latency-svc-khdgr [747.240396ms]
Jan 15 22:30:26.422: INFO: Created: latency-svc-jgjvb
Jan 15 22:30:26.449: INFO: Got endpoints: latency-svc-2gj2l [749.555746ms]
Jan 15 22:30:26.471: INFO: Created: latency-svc-449w5
Jan 15 22:30:26.501: INFO: Got endpoints: latency-svc-jjq2q [751.669845ms]
Jan 15 22:30:26.528: INFO: Created: latency-svc-zs8wv
Jan 15 22:30:26.549: INFO: Got endpoints: latency-svc-mbd7q [750.076722ms]
Jan 15 22:30:26.574: INFO: Created: latency-svc-2hft5
Jan 15 22:30:26.599: INFO: Got endpoints: latency-svc-4tvgx [750.957126ms]
Jan 15 22:30:26.639: INFO: Created: latency-svc-9q2cf
Jan 15 22:30:26.649: INFO: Got endpoints: latency-svc-lsbrn [748.556334ms]
Jan 15 22:30:26.676: INFO: Created: latency-svc-jprsb
Jan 15 22:30:26.703: INFO: Got endpoints: latency-svc-j9jbq [755.002244ms]
Jan 15 22:30:26.728: INFO: Created: latency-svc-5ccwb
Jan 15 22:30:26.753: INFO: Got endpoints: latency-svc-nzk59 [753.301804ms]
Jan 15 22:30:26.775: INFO: Created: latency-svc-66zsz
Jan 15 22:30:26.798: INFO: Got endpoints: latency-svc-f24j8 [747.993232ms]
Jan 15 22:30:26.822: INFO: Created: latency-svc-cl5zf
Jan 15 22:30:26.851: INFO: Got endpoints: latency-svc-h7dtx [751.95105ms]
Jan 15 22:30:26.872: INFO: Created: latency-svc-l2lwn
Jan 15 22:30:26.898: INFO: Got endpoints: latency-svc-gcm9m [748.580527ms]
Jan 15 22:30:26.919: INFO: Created: latency-svc-r5kkv
Jan 15 22:30:26.950: INFO: Got endpoints: latency-svc-7hx2v [751.651644ms]
Jan 15 22:30:26.970: INFO: Created: latency-svc-c8gpr
Jan 15 22:30:27.000: INFO: Got endpoints: latency-svc-plzl7 [752.300749ms]
Jan 15 22:30:27.020: INFO: Created: latency-svc-lhkrc
Jan 15 22:30:27.049: INFO: Got endpoints: latency-svc-z62zs [750.451421ms]
Jan 15 22:30:27.071: INFO: Created: latency-svc-c52p5
Jan 15 22:30:27.101: INFO: Got endpoints: latency-svc-j69dp [751.601024ms]
Jan 15 22:30:27.125: INFO: Created: latency-svc-jnxn9
Jan 15 22:30:27.148: INFO: Got endpoints: latency-svc-jgjvb [750.092578ms]
Jan 15 22:30:27.170: INFO: Created: latency-svc-pgl7f
Jan 15 22:30:27.199: INFO: Got endpoints: latency-svc-449w5 [749.716191ms]
Jan 15 22:30:27.223: INFO: Created: latency-svc-ksd9k
Jan 15 22:30:27.250: INFO: Got endpoints: latency-svc-zs8wv [748.439912ms]
Jan 15 22:30:27.271: INFO: Created: latency-svc-bvjht
Jan 15 22:30:27.300: INFO: Got endpoints: latency-svc-2hft5 [750.593131ms]
Jan 15 22:30:27.324: INFO: Created: latency-svc-62szr
Jan 15 22:30:27.349: INFO: Got endpoints: latency-svc-9q2cf [750.110874ms]
Jan 15 22:30:27.372: INFO: Created: latency-svc-wgrcb
Jan 15 22:30:27.411: INFO: Got endpoints: latency-svc-jprsb [761.939563ms]
Jan 15 22:30:27.437: INFO: Created: latency-svc-p6sth
Jan 15 22:30:27.448: INFO: Got endpoints: latency-svc-5ccwb [744.75717ms]
Jan 15 22:30:27.472: INFO: Created: latency-svc-f9hmc
Jan 15 22:30:27.501: INFO: Got endpoints: latency-svc-66zsz [748.172655ms]
Jan 15 22:30:27.523: INFO: Created: latency-svc-fcgrp
Jan 15 22:30:27.549: INFO: Got endpoints: latency-svc-cl5zf [751.126945ms]
Jan 15 22:30:27.571: INFO: Created: latency-svc-mmzx6
Jan 15 22:30:27.600: INFO: Got endpoints: latency-svc-l2lwn [749.690697ms]
Jan 15 22:30:27.623: INFO: Created: latency-svc-2879k
Jan 15 22:30:27.650: INFO: Got endpoints: latency-svc-r5kkv [752.01451ms]
Jan 15 22:30:27.673: INFO: Created: latency-svc-g6pgg
Jan 15 22:30:27.700: INFO: Got endpoints: latency-svc-c8gpr [749.633648ms]
Jan 15 22:30:27.722: INFO: Created: latency-svc-rm8ts
Jan 15 22:30:27.748: INFO: Got endpoints: latency-svc-lhkrc [748.45091ms]
Jan 15 22:30:27.773: INFO: Created: latency-svc-jrrw8
Jan 15 22:30:27.798: INFO: Got endpoints: latency-svc-c52p5 [749.158588ms]
Jan 15 22:30:27.821: INFO: Created: latency-svc-52twg
Jan 15 22:30:27.849: INFO: Got endpoints: latency-svc-jnxn9 [748.592043ms]
Jan 15 22:30:27.872: INFO: Created: latency-svc-nssb6
Jan 15 22:30:27.898: INFO: Got endpoints: latency-svc-pgl7f [749.361382ms]
Jan 15 22:30:27.920: INFO: Created: latency-svc-gdn7r
Jan 15 22:30:27.949: INFO: Got endpoints: latency-svc-ksd9k [750.029542ms]
Jan 15 22:30:27.970: INFO: Created: latency-svc-qm4hc
Jan 15 22:30:28.001: INFO: Got endpoints: latency-svc-bvjht [751.468327ms]
Jan 15 22:30:28.024: INFO: Created: latency-svc-pj4ns
Jan 15 22:30:28.049: INFO: Got endpoints: latency-svc-62szr [749.60346ms]
Jan 15 22:30:28.071: INFO: Created: latency-svc-rvstq
Jan 15 22:30:28.098: INFO: Got endpoints: latency-svc-wgrcb [748.616315ms]
Jan 15 22:30:28.120: INFO: Created: latency-svc-p47fc
Jan 15 22:30:28.149: INFO: Got endpoints: latency-svc-p6sth [738.361176ms]
Jan 15 22:30:28.170: INFO: Created: latency-svc-l7tp5
Jan 15 22:30:28.199: INFO: Got endpoints: latency-svc-f9hmc [750.703195ms]
Jan 15 22:30:28.222: INFO: Created: latency-svc-ng952
Jan 15 22:30:28.249: INFO: Got endpoints: latency-svc-fcgrp [748.317239ms]
Jan 15 22:30:28.270: INFO: Created: latency-svc-wzxrl
Jan 15 22:30:28.298: INFO: Got endpoints: latency-svc-mmzx6 [749.312951ms]
Jan 15 22:30:28.321: INFO: Created: latency-svc-px6gx
Jan 15 22:30:28.348: INFO: Got endpoints: latency-svc-2879k [748.098516ms]
Jan 15 22:30:28.371: INFO: Created: latency-svc-4mh7z
Jan 15 22:30:28.400: INFO: Got endpoints: latency-svc-g6pgg [750.096533ms]
Jan 15 22:30:28.423: INFO: Created: latency-svc-8lhtw
Jan 15 22:30:28.450: INFO: Got endpoints: latency-svc-rm8ts [750.2308ms]
Jan 15 22:30:28.471: INFO: Created: latency-svc-6pv2x
Jan 15 22:30:28.499: INFO: Got endpoints: latency-svc-jrrw8 [750.767888ms]
Jan 15 22:30:28.522: INFO: Created: latency-svc-dw2g9
Jan 15 22:30:28.550: INFO: Got endpoints: latency-svc-52twg [752.020831ms]
Jan 15 22:30:28.574: INFO: Created: latency-svc-2h2jh
Jan 15 22:30:28.598: INFO: Got endpoints: latency-svc-nssb6 [749.259342ms]
Jan 15 22:30:28.622: INFO: Created: latency-svc-8w9hx
Jan 15 22:30:28.649: INFO: Got endpoints: latency-svc-gdn7r [751.469012ms]
Jan 15 22:30:28.671: INFO: Created: latency-svc-9lkhc
Jan 15 22:30:28.698: INFO: Got endpoints: latency-svc-qm4hc [749.567568ms]
Jan 15 22:30:28.719: INFO: Created: latency-svc-498cr
Jan 15 22:30:28.749: INFO: Got endpoints: latency-svc-pj4ns [748.339608ms]
Jan 15 22:30:28.773: INFO: Created: latency-svc-j4759
Jan 15 22:30:28.799: INFO: Got endpoints: latency-svc-rvstq [750.076176ms]
Jan 15 22:30:28.826: INFO: Created: latency-svc-jtl6r
Jan 15 22:30:28.850: INFO: Got endpoints: latency-svc-p47fc [751.630023ms]
Jan 15 22:30:28.872: INFO: Created: latency-svc-zhmkt
Jan 15 22:30:28.898: INFO: Got endpoints: latency-svc-l7tp5 [749.180386ms]
Jan 15 22:30:28.918: INFO: Created: latency-svc-f9pxm
Jan 15 22:30:28.952: INFO: Got endpoints: latency-svc-ng952 [753.006736ms]
Jan 15 22:30:28.974: INFO: Created: latency-svc-dnvpc
Jan 15 22:30:28.998: INFO: Got endpoints: latency-svc-wzxrl [748.281442ms]
Jan 15 22:30:29.021: INFO: Created: latency-svc-wlcd7
Jan 15 22:30:29.051: INFO: Got endpoints: latency-svc-px6gx [752.469628ms]
Jan 15 22:30:29.074: INFO: Created: latency-svc-9cb2h
Jan 15 22:30:29.098: INFO: Got endpoints: latency-svc-4mh7z [749.63865ms]
Jan 15 22:30:29.119: INFO: Created: latency-svc-b9z7v
Jan 15 22:30:29.149: INFO: Got endpoints: latency-svc-8lhtw [748.676992ms]
Jan 15 22:30:29.170: INFO: Created: latency-svc-pjxlf
Jan 15 22:30:29.199: INFO: Got endpoints: latency-svc-6pv2x [748.652719ms]
Jan 15 22:30:29.220: INFO: Created: latency-svc-kdj4v
Jan 15 22:30:29.251: INFO: Got endpoints: latency-svc-dw2g9 [751.858385ms]
Jan 15 22:30:29.273: INFO: Created: latency-svc-gqjdb
Jan 15 22:30:29.298: INFO: Got endpoints: latency-svc-2h2jh [747.737539ms]
Jan 15 22:30:29.320: INFO: Created: latency-svc-fprz4
Jan 15 22:30:29.349: INFO: Got endpoints: latency-svc-8w9hx [750.869005ms]
Jan 15 22:30:29.373: INFO: Created: latency-svc-bpdg7
Jan 15 22:30:29.399: INFO: Got endpoints: latency-svc-9lkhc [749.695612ms]
Jan 15 22:30:29.421: INFO: Created: latency-svc-4drj6
Jan 15 22:30:29.448: INFO: Got endpoints: latency-svc-498cr [750.018662ms]
Jan 15 22:30:29.477: INFO: Created: latency-svc-8m2fx
Jan 15 22:30:29.498: INFO: Got endpoints: latency-svc-j4759 [748.483216ms]
Jan 15 22:30:29.520: INFO: Created: latency-svc-7xsms
Jan 15 22:30:29.548: INFO: Got endpoints: latency-svc-jtl6r [748.730134ms]
Jan 15 22:30:29.570: INFO: Created: latency-svc-25pmn
Jan 15 22:30:29.599: INFO: Got endpoints: latency-svc-zhmkt [748.883961ms]
Jan 15 22:30:29.622: INFO: Created: latency-svc-8nkh9
Jan 15 22:30:29.648: INFO: Got endpoints: latency-svc-f9pxm [749.470814ms]
Jan 15 22:30:29.669: INFO: Created: latency-svc-9vbn8
Jan 15 22:30:29.700: INFO: Got endpoints: latency-svc-dnvpc [747.561384ms]
Jan 15 22:30:29.722: INFO: Created: latency-svc-zzb4m
Jan 15 22:30:29.748: INFO: Got endpoints: latency-svc-wlcd7 [750.803171ms]
Jan 15 22:30:29.771: INFO: Created: latency-svc-7f46p
Jan 15 22:30:29.801: INFO: Got endpoints: latency-svc-9cb2h [750.256079ms]
Jan 15 22:30:29.820: INFO: Created: latency-svc-5942b
Jan 15 22:30:29.849: INFO: Got endpoints: latency-svc-b9z7v [750.623121ms]
Jan 15 22:30:29.869: INFO: Created: latency-svc-jdnf6
Jan 15 22:30:29.900: INFO: Got endpoints: latency-svc-pjxlf [751.437396ms]
Jan 15 22:30:29.923: INFO: Created: latency-svc-5kq72
Jan 15 22:30:29.949: INFO: Got endpoints: latency-svc-kdj4v [750.490771ms]
Jan 15 22:30:29.971: INFO: Created: latency-svc-6srgf
Jan 15 22:30:29.998: INFO: Got endpoints: latency-svc-gqjdb [747.253237ms]
Jan 15 22:30:30.019: INFO: Created: latency-svc-gzpqb
Jan 15 22:30:30.048: INFO: Got endpoints: latency-svc-fprz4 [750.041893ms]
Jan 15 22:30:30.069: INFO: Created: latency-svc-vb5mv
Jan 15 22:30:30.100: INFO: Got endpoints: latency-svc-bpdg7 [750.351572ms]
Jan 15 22:30:30.123: INFO: Created: latency-svc-vct6p
Jan 15 22:30:30.149: INFO: Got endpoints: latency-svc-4drj6 [749.86572ms]
Jan 15 22:30:30.200: INFO: Got endpoints: latency-svc-8m2fx [751.859879ms]
Jan 15 22:30:30.249: INFO: Got endpoints: latency-svc-7xsms [750.616642ms]
Jan 15 22:30:30.299: INFO: Got endpoints: latency-svc-25pmn [751.143877ms]
Jan 15 22:30:30.351: INFO: Got endpoints: latency-svc-8nkh9 [752.351801ms]
Jan 15 22:30:30.398: INFO: Got endpoints: latency-svc-9vbn8 [750.416252ms]
Jan 15 22:30:30.450: INFO: Got endpoints: latency-svc-zzb4m [750.396886ms]
Jan 15 22:30:30.499: INFO: Got endpoints: latency-svc-7f46p [749.993698ms]
Jan 15 22:30:30.549: INFO: Got endpoints: latency-svc-5942b [747.953892ms]
Jan 15 22:30:30.598: INFO: Got endpoints: latency-svc-jdnf6 [749.201944ms]
Jan 15 22:30:30.649: INFO: Got endpoints: latency-svc-5kq72 [749.088887ms]
Jan 15 22:30:30.698: INFO: Got endpoints: latency-svc-6srgf [748.905706ms]
Jan 15 22:30:30.748: INFO: Got endpoints: latency-svc-gzpqb [749.530599ms]
Jan 15 22:30:30.799: INFO: Got endpoints: latency-svc-vb5mv [750.684737ms]
Jan 15 22:30:30.848: INFO: Got endpoints: latency-svc-vct6p [748.480491ms]
Jan 15 22:30:30.848: INFO: Latencies: [41.647803ms 48.292262ms 55.395098ms 87.080661ms 87.37236ms 87.395168ms 87.424969ms 87.487366ms 90.692264ms 93.343184ms 96.790897ms 99.034948ms 99.521691ms 101.643358ms 103.542119ms 104.726339ms 106.257339ms 107.843971ms 108.526245ms 109.880857ms 110.280716ms 112.384606ms 112.663896ms 115.417319ms 116.185394ms 121.409368ms 122.579368ms 123.202997ms 123.719748ms 125.24194ms 125.602201ms 126.08286ms 133.516527ms 147.347268ms 154.427092ms 161.657757ms 172.06454ms 216.258017ms 219.127101ms 248.873103ms 301.63105ms 339.864601ms 381.34606ms 413.725039ms 462.402793ms 499.559417ms 552.279672ms 640.613934ms 671.690394ms 673.673014ms 722.16344ms 738.361176ms 740.435931ms 744.410435ms 744.75717ms 746.555991ms 746.932557ms 747.240396ms 747.253237ms 747.561384ms 747.640956ms 747.720466ms 747.737539ms 747.80334ms 747.953892ms 747.993232ms 748.017584ms 748.098516ms 748.130066ms 748.145943ms 748.172655ms 748.174902ms 748.184345ms 748.197453ms 748.281442ms 748.317239ms 748.339608ms 748.418119ms 748.439912ms 748.45091ms 748.480491ms 748.483216ms 748.556334ms 748.580527ms 748.592043ms 748.616315ms 748.652719ms 748.676992ms 748.730134ms 748.789313ms 748.866697ms 748.883961ms 748.905706ms 748.984422ms 749.088887ms 749.158588ms 749.180386ms 749.201944ms 749.259342ms 749.312951ms 749.361382ms 749.438521ms 749.470814ms 749.482118ms 749.516272ms 749.530599ms 749.555746ms 749.567568ms 749.60346ms 749.633648ms 749.637804ms 749.63865ms 749.690697ms 749.695612ms 749.716191ms 749.797993ms 749.832689ms 749.86572ms 749.896741ms 749.959684ms 749.993698ms 750.018662ms 750.020773ms 750.029542ms 750.041893ms 750.076176ms 750.076722ms 750.092578ms 750.096533ms 750.110874ms 750.113973ms 750.126061ms 750.150391ms 750.2308ms 750.23557ms 750.256079ms 750.277494ms 750.351572ms 750.384309ms 750.396886ms 750.404767ms 750.416252ms 750.451421ms 750.490771ms 750.505587ms 750.593131ms 750.59636ms 750.616642ms 750.623121ms 750.684737ms 750.703195ms 750.767888ms 750.802629ms 750.803171ms 750.837565ms 750.869005ms 750.872745ms 750.913682ms 750.957126ms 750.986753ms 751.043075ms 751.122225ms 751.126945ms 751.139282ms 751.143877ms 751.20304ms 751.20645ms 751.262409ms 751.437396ms 751.468327ms 751.469012ms 751.483889ms 751.579937ms 751.601024ms 751.630023ms 751.651644ms 751.669845ms 751.681546ms 751.730442ms 751.858385ms 751.859879ms 751.924346ms 751.95105ms 751.955072ms 752.01451ms 752.020831ms 752.026966ms 752.300749ms 752.30394ms 752.351801ms 752.469628ms 752.497632ms 752.566905ms 752.846774ms 752.949031ms 753.006736ms 753.301804ms 755.002244ms 759.649983ms 761.939563ms]
Jan 15 22:30:30.848: INFO: 50 %ile: 749.361382ms
Jan 15 22:30:30.848: INFO: 90 %ile: 751.859879ms
Jan 15 22:30:30.848: INFO: 99 %ile: 759.649983ms
Jan 15 22:30:30.848: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:30:30.849: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-j2s4t" for this suite.
Jan 15 22:30:55.704: INFO: namespace: e2e-tests-svc-latency-j2s4t, resource: bindings, ignored listing per whitelist
Jan 15 22:30:56.268: INFO: namespace e2e-tests-svc-latency-j2s4t deletion completed in 25.403026348s

• [SLOW TEST:35.169 seconds]
[sig-network] Service endpoints latency
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service_latency.go:48
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:39
[BeforeEach] [k8s.io] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:30:56.268: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:39
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-c0dade27-fa43-11e7-b6d5-0e5e74815504,GenerateName:,Namespace:e2e-tests-events-gwth6,SelfLink:/api/v1/namespaces/e2e-tests-events-gwth6/pods/send-events-c0dade27-fa43-11e7-b6d5-0e5e74815504,UID:c0dd6667-fa43-11e7-a248-42010a8e0002,ResourceVersion:7473,Generation:0,CreationTimestamp:2018-01-15 22:30:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 341153707,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cjr9j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjr9j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cjr9j true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{role: app,},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci-prtest-cc63063-250-ig-n-v6l4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-01-15 22:30:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-01-15 22:30:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-01-15 22:30:56 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.5,PodIP:172.16.2.20,StartTime:2018-01-15 22:30:56 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-01-15 22:30:57 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 docker://56cccd4383b3de7addf3e2e342d8a85dbf1177e4cfed3d7c68fbe0dc5b8fe823}],QOSClass:BestEffort,InitContainerStatuses:[],},}
STEP: checking for scheduler event about the pod
Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:31:02.481: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-gwth6" for this suite.
Jan 15 22:31:25.501: INFO: namespace: e2e-tests-events-gwth6, resource: bindings, ignored listing per whitelist
Jan 15 22:31:25.919: INFO: namespace e2e-tests-events-gwth6 deletion completed in 23.409840981s

• [SLOW TEST:29.651 seconds]
[k8s.io] Events
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be sent by kubelets and the scheduler about pods scheduling and running [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/events.go:39
------------------------------
SSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:115
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:31:25.919: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:115
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 15 22:31:26.060: INFO: Waiting up to 5m0s for pod "pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-5wc4n" to be "success or failure"
Jan 15 22:31:26.075: INFO: Pod "pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.216388ms
Jan 15 22:31:28.091: INFO: Pod "pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030778188s
STEP: Saw pod success
Jan 15 22:31:28.091: INFO: Pod "pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:31:28.106: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:31:28.152: INFO: Waiting for pod pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:31:28.167: INFO: Pod pod-d28eaab9-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:31:28.167: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5wc4n" for this suite.
Jan 15 22:31:35.499: INFO: namespace: e2e-tests-emptydir-5wc4n, resource: bindings, ignored listing per whitelist
Jan 15 22:31:35.603: INFO: namespace e2e-tests-emptydir-5wc4n deletion completed in 7.407308722s

• [SLOW TEST:9.684 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:115
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:31:35.603: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1355
[It] should create a job from an image when restart is OnFailure [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 22:31:35.722: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-vv6nn'
Jan 15 22:31:36.485: INFO: stderr: ""
Jan 15 22:31:36.485: INFO: stdout: "job \"e2e-test-nginx-job\" created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1360
Jan 15 22:31:36.502: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-vv6nn'
Jan 15 22:31:38.841: INFO: stderr: ""
Jan 15 22:31:38.841: INFO: stdout: "job \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:31:38.841: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vv6nn" for this suite.
Jan 15 22:31:46.163: INFO: namespace: e2e-tests-kubectl-vv6nn, resource: bindings, ignored listing per whitelist
Jan 15 22:31:46.282: INFO: namespace e2e-tests-kubectl-vv6nn deletion completed in 7.411980758s

• [SLOW TEST:10.679 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create a job from an image when restart is OnFailure [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:47
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:31:46.283: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:47
STEP: Creating projection with secret that has name projected-secret-test-deb284cf-fa43-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:31:46.443: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-rpt4v" to be "success or failure"
Jan 15 22:31:46.457: INFO: Pod "pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.851475ms
Jan 15 22:31:48.473: INFO: Pod "pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030739312s
STEP: Saw pod success
Jan 15 22:31:48.473: INFO: Pod "pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:31:48.489: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:31:48.533: INFO: Waiting for pod pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:31:48.549: INFO: Pod pod-projected-secrets-deb4fa86-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:31:48.549: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rpt4v" for this suite.
Jan 15 22:31:55.443: INFO: namespace: e2e-tests-projected-rpt4v, resource: bindings, ignored listing per whitelist
Jan 15 22:31:56.002: INFO: namespace e2e-tests-projected-rpt4v deletion completed in 7.424667708s

• [SLOW TEST:9.720 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:47
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:57
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:31:56.003: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:57
STEP: Creating configMap with name configmap-test-volume-map-e47c800e-fa43-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:31:56.184: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-kpksr" to be "success or failure"
Jan 15 22:31:56.208: INFO: Pod "pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 23.47337ms
Jan 15 22:31:58.224: INFO: Pod "pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039209139s
STEP: Saw pod success
Jan 15 22:31:58.224: INFO: Pod "pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:31:58.239: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:31:58.282: INFO: Waiting for pod pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:31:58.297: INFO: Pod pod-configmaps-e4824fe9-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:31:58.297: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-kpksr" for this suite.
Jan 15 22:32:05.598: INFO: namespace: e2e-tests-configmap-kpksr, resource: bindings, ignored listing per whitelist
Jan 15 22:32:05.736: INFO: namespace e2e-tests-configmap-kpksr deletion completed in 7.409210907s

• [SLOW TEST:9.733 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:57
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Projected 
  should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:781
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:32:05.736: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:781
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:32:05.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-h7vrl" to be "success or failure"
Jan 15 22:32:05.877: INFO: Pod "downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.354504ms
Jan 15 22:32:07.893: INFO: Pod "downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031033399s
STEP: Saw pod success
Jan 15 22:32:07.893: INFO: Pod "downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:32:07.908: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:32:07.961: INFO: Waiting for pod downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:32:07.976: INFO: Pod downwardapi-volume-ea47adb1-fa43-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:32:07.976: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h7vrl" for this suite.
Jan 15 22:32:14.761: INFO: namespace: e2e-tests-projected-h7vrl, resource: bindings, ignored listing per whitelist
Jan 15 22:32:15.402: INFO: namespace e2e-tests-projected-h7vrl deletion completed in 7.397425603s

• [SLOW TEST:9.666 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:781
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:182
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:32:15.403: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:182
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-bstn7
Jan 15 22:32:17.556: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-bstn7
STEP: checking the pod's current state and verifying that restartCount is present
Jan 15 22:32:17.571: INFO: Initial restart count of pod liveness-http is 0
Jan 15 22:32:29.682: INFO: Restart count of pod e2e-tests-container-probe-bstn7/liveness-http is now 1 (12.111025242s elapsed)
Jan 15 22:32:49.838: INFO: Restart count of pod e2e-tests-container-probe-bstn7/liveness-http is now 2 (32.26689097s elapsed)
Jan 15 22:33:09.997: INFO: Restart count of pod e2e-tests-container-probe-bstn7/liveness-http is now 3 (52.425824088s elapsed)
Jan 15 22:33:30.157: INFO: Restart count of pod e2e-tests-container-probe-bstn7/liveness-http is now 4 (1m12.585278606s elapsed)
Jan 15 22:34:44.737: INFO: Restart count of pod e2e-tests-container-probe-bstn7/liveness-http is now 5 (2m27.165706283s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:34:44.757: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bstn7" for this suite.
Jan 15 22:34:52.053: INFO: namespace: e2e-tests-container-probe-bstn7, resource: bindings, ignored listing per whitelist
Jan 15 22:34:52.206: INFO: namespace e2e-tests-container-probe-bstn7 deletion completed in 7.420319844s

• [SLOW TEST:156.803 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should have monotonically increasing restart count [Conformance] [Slow]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:182
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1074
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:34:52.206: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1065
STEP: creating an rc
Jan 15 22:34:52.307: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-zp2qh'
Jan 15 22:34:52.654: INFO: stderr: ""
Jan 15 22:34:52.654: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
[It] should be able to retrieve and filter logs [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1074
STEP: Waiting for Redis master to start.
Jan 15 22:34:53.686: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:34:53.686: INFO: Found 0 / 1
Jan 15 22:34:54.686: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:34:54.686: INFO: Found 1 / 1
Jan 15 22:34:54.686: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 15 22:34:54.701: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:34:54.701: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 15 22:34:54.701: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh'
Jan 15 22:34:54.931: INFO: stderr: ""
Jan 15 22:34:54.931: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Jan 22:34:54.084 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Jan 22:34:54.084 # Server started, Redis version 3.2.8\n1:M 15 Jan 22:34:54.084 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Jan 22:34:54.084 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 15 22:34:54.931: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh --tail=1'
Jan 15 22:34:55.156: INFO: stderr: ""
Jan 15 22:34:55.156: INFO: stdout: "1:M 15 Jan 22:34:54.084 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 15 22:34:55.156: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh --limit-bytes=1'
Jan 15 22:34:55.382: INFO: stderr: ""
Jan 15 22:34:55.382: INFO: stdout: " "
STEP: exposing timestamps
Jan 15 22:34:55.382: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh --tail=1 --timestamps'
Jan 15 22:34:55.605: INFO: stderr: ""
Jan 15 22:34:55.605: INFO: stdout: "2018-01-15T22:34:54.084931869Z 1:M 15 Jan 22:34:54.084 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 15 22:34:58.106: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh --since=1s'
Jan 15 22:34:58.331: INFO: stderr: ""
Jan 15 22:34:58.331: INFO: stdout: ""
Jan 15 22:34:58.331: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-k62vn redis-master --namespace=e2e-tests-kubectl-zp2qh --since=24h'
Jan 15 22:34:58.553: INFO: stderr: ""
Jan 15 22:34:58.553: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Jan 22:34:54.084 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Jan 22:34:54.084 # Server started, Redis version 3.2.8\n1:M 15 Jan 22:34:54.084 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Jan 22:34:54.084 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1070
STEP: using delete to clean up resources
Jan 15 22:34:58.553: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zp2qh'
Jan 15 22:34:58.838: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:34:58.838: INFO: stdout: "replicationcontroller \"redis-master\" deleted\n"
Jan 15 22:34:58.838: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-zp2qh'
Jan 15 22:34:59.015: INFO: stderr: "No resources found.\n"
Jan 15 22:34:59.015: INFO: stdout: ""
Jan 15 22:34:59.015: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-zp2qh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 15 22:34:59.176: INFO: stderr: ""
Jan 15 22:34:59.176: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:34:59.176: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zp2qh" for this suite.
Jan 15 22:35:22.420: INFO: namespace: e2e-tests-kubectl-zp2qh, resource: bindings, ignored listing per whitelist
Jan 15 22:35:22.613: INFO: namespace e2e-tests-kubectl-zp2qh deletion completed in 23.40813085s

• [SLOW TEST:30.407 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should be able to retrieve and filter logs [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1074
------------------------------
SSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:107
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:35:22.613: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 15 22:35:22.736: INFO: Waiting up to 5m0s for pod "pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-mkv2w" to be "success or failure"
Jan 15 22:35:22.750: INFO: Pod "pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.864483ms
Jan 15 22:35:24.766: INFO: Pod "pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030597245s
STEP: Saw pod success
Jan 15 22:35:24.766: INFO: Pod "pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:35:24.781: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:35:24.822: INFO: Waiting for pod pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:35:24.837: INFO: Pod pod-5fa0dda1-fa44-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:35:24.837: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mkv2w" for this suite.
Jan 15 22:35:31.751: INFO: namespace: e2e-tests-emptydir-mkv2w, resource: bindings, ignored listing per whitelist
Jan 15 22:35:32.281: INFO: namespace e2e-tests-emptydir-mkv2w deletion completed in 7.415166862s

• [SLOW TEST:9.668 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:107
------------------------------
[k8s.io] Projected 
  should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:895
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:35:32.281: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:895
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:35:32.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-tsr64" to be "success or failure"
Jan 15 22:35:32.416: INFO: Pod "downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.11697ms
Jan 15 22:35:34.431: INFO: Pod "downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030744371s
STEP: Saw pod success
Jan 15 22:35:34.431: INFO: Pod "downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:35:34.447: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:35:34.488: INFO: Waiting for pod downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:35:34.503: INFO: Pod downwardapi-volume-656341ca-fa44-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:35:34.503: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tsr64" for this suite.
Jan 15 22:35:41.703: INFO: namespace: e2e-tests-projected-tsr64, resource: bindings, ignored listing per whitelist
Jan 15 22:35:41.951: INFO: namespace e2e-tests-projected-tsr64 deletion completed in 7.419213653s

• [SLOW TEST:9.670 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's cpu limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:895
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] PreStop 
  should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:178
[BeforeEach] [k8s.io] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:35:41.951: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:178
STEP: Creating server pod server in namespace e2e-tests-prestop-2b6dq
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-2b6dq
STEP: Deleting pre-stop pod
Jan 15 22:35:59.247: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:35:59.265: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-2b6dq" for this suite.
Jan 15 22:36:37.893: INFO: namespace: e2e-tests-prestop-2b6dq, resource: bindings, ignored listing per whitelist
Jan 15 22:36:38.706: INFO: namespace e2e-tests-prestop-2b6dq deletion completed in 39.412456876s

• [SLOW TEST:56.754 seconds]
[k8s.io] PreStop
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should call prestop when killing a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pre_stop.go:178
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1127
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:36:38.706: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should add annotations for pods in rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1127
STEP: creating Redis RC
Jan 15 22:36:38.833: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-plsrt'
Jan 15 22:36:39.178: INFO: stderr: ""
Jan 15 22:36:39.178: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Jan 15 22:36:40.194: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:36:40.194: INFO: Found 0 / 1
Jan 15 22:36:41.194: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:36:41.194: INFO: Found 1 / 1
Jan 15 22:36:41.194: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 15 22:36:41.210: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:36:41.210: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 15 22:36:41.210: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig patch pod redis-master-lz4cp --namespace=e2e-tests-kubectl-plsrt -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 15 22:36:41.398: INFO: stderr: ""
Jan 15 22:36:41.398: INFO: stdout: "pod \"redis-master-lz4cp\" patched\n"
STEP: checking annotations
Jan 15 22:36:41.414: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:36:41.414: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:36:41.414: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-plsrt" for this suite.
Jan 15 22:37:04.738: INFO: namespace: e2e-tests-kubectl-plsrt, resource: bindings, ignored listing per whitelist
Jan 15 22:37:04.842: INFO: namespace e2e-tests-kubectl-plsrt deletion completed in 23.399187368s

• [SLOW TEST:26.136 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should add annotations for pods in rc [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1127
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:725
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:37:04.842: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should check if v1 is in available api versions [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:725
STEP: validating api versions
Jan 15 22:37:04.975: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig api-versions'
Jan 15 22:37:05.152: INFO: stderr: ""
Jan 15 22:37:05.152: INFO: stdout: "apiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsecurity.openshift.io/v1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:37:05.152: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ttct2" for this suite.
Jan 15 22:37:12.110: INFO: namespace: e2e-tests-kubectl-ttct2, resource: bindings, ignored listing per whitelist
Jan 15 22:37:12.597: INFO: namespace e2e-tests-kubectl-ttct2 deletion completed in 7.417095849s

• [SLOW TEST:7.756 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should check if v1 is in available api versions [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:725
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Projected 
  should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:839
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:37:12.598: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:839
STEP: Creating the pod
Jan 15 22:37:15.319: INFO: Successfully updated pod "labelsupdatea12f2b49-fa44-11e7-b6d5-0e5e74815504"
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:38:32.225: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j7f87" for this suite.
Jan 15 22:38:54.834: INFO: namespace: e2e-tests-projected-j7f87, resource: bindings, ignored listing per whitelist
Jan 15 22:38:55.670: INFO: namespace e2e-tests-projected-j7f87 deletion completed in 23.416009948s

• [SLOW TEST:103.072 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should update labels on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:839
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:866
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:38:55.670: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:866
STEP: Creating the pod
Jan 15 22:38:58.420: INFO: Successfully updated pod "annotationupdatede9ced49-fa44-11e7-b6d5-0e5e74815504"
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:39:00.464: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xvjtz" for this suite.
Jan 15 22:39:23.755: INFO: namespace: e2e-tests-projected-xvjtz, resource: bindings, ignored listing per whitelist
Jan 15 22:39:23.887: INFO: namespace e2e-tests-projected-xvjtz deletion completed in 23.393983557s

• [SLOW TEST:28.217 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should update annotations on modification [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:866
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:39:23.887: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
Jan 15 22:39:44.050: INFO: Container started at 2018-01-15 22:39:24 +0000 UTC, pod became ready at 2018-01-15 22:39:43 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:39:44.050: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vww9k" for this suite.
Jan 15 22:40:06.939: INFO: namespace: e2e-tests-container-probe-vww9k, resource: bindings, ignored listing per whitelist
Jan 15 22:40:07.478: INFO: namespace e2e-tests-container-probe-vww9k deletion completed in 23.398682303s

• [SLOW TEST:43.591 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  with readiness probe should not be ready before initial delay and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:84
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:07.478: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:84
STEP: Creating secret with name projected-secret-test-096cfeed-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:40:07.625: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-f92nv" to be "success or failure"
Jan 15 22:40:07.640: INFO: Pod "pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.905874ms
Jan 15 22:40:09.656: INFO: Pod "pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030691275s
STEP: Saw pod success
Jan 15 22:40:09.656: INFO: Pod "pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:40:09.672: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:40:09.716: INFO: Waiting for pod pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:40:09.731: INFO: Pod pod-projected-secrets-096f6954-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:40:09.731: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f92nv" for this suite.
Jan 15 22:40:16.717: INFO: namespace: e2e-tests-projected-f92nv, resource: bindings, ignored listing per whitelist
Jan 15 22:40:17.167: INFO: namespace e2e-tests-projected-f92nv deletion completed in 7.407438156s

• [SLOW TEST:9.689 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:84
------------------------------
SS
------------------------------
[k8s.io] ConfigMap 
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:373
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:17.167: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:373
STEP: Creating configMap e2e-tests-configmap-7ssv7/configmap-test-0f30821c-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:40:17.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-7ssv7" to be "success or failure"
Jan 15 22:40:17.312: INFO: Pod "pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.696869ms
Jan 15 22:40:19.327: INFO: Pod "pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03042383s
Jan 15 22:40:21.343: INFO: Pod "pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046059674s
STEP: Saw pod success
Jan 15 22:40:21.343: INFO: Pod "pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:40:21.358: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504 container env-test: <nil>
STEP: delete the pod
Jan 15 22:40:21.400: INFO: Waiting for pod pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:40:21.415: INFO: Pod pod-configmaps-0f32c64d-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:40:21.415: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7ssv7" for this suite.
Jan 15 22:40:28.860: INFO: namespace: e2e-tests-configmap-7ssv7, resource: bindings, ignored listing per whitelist
Jan 15 22:40:28.875: INFO: namespace e2e-tests-configmap-7ssv7 deletion completed in 7.430946517s

• [SLOW TEST:11.708 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:373
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:370
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:28.875: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:370
STEP: Creating configMap with name projected-configmap-test-volume-163658cc-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:40:29.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-zfzqw" to be "success or failure"
Jan 15 22:40:29.094: INFO: Pod "pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.631767ms
Jan 15 22:40:31.111: INFO: Pod "pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031644479s
STEP: Saw pod success
Jan 15 22:40:31.111: INFO: Pod "pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:40:31.126: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:40:31.167: INFO: Waiting for pod pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:40:31.182: INFO: Pod pod-projected-configmaps-1638d607-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:40:31.182: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zfzqw" for this suite.
Jan 15 22:40:38.483: INFO: namespace: e2e-tests-projected-zfzqw, resource: bindings, ignored listing per whitelist
Jan 15 22:40:38.632: INFO: namespace e2e-tests-projected-zfzqw deletion completed in 7.422033444s

• [SLOW TEST:9.757 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:370
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:38
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:38.632: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:38
STEP: Creating projection with secret that has name projected-secret-test-1bfbcad7-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:40:38.773: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-jf7vk" to be "success or failure"
Jan 15 22:40:38.788: INFO: Pod "pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.637524ms
Jan 15 22:40:40.804: INFO: Pod "pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031147543s
STEP: Saw pod success
Jan 15 22:40:40.804: INFO: Pod "pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:40:40.819: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:40:40.859: INFO: Waiting for pod pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:40:40.874: INFO: Pod pod-projected-secrets-1bff796c-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:40:40.874: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jf7vk" for this suite.
Jan 15 22:40:47.943: INFO: namespace: e2e-tests-projected-jf7vk, resource: bindings, ignored listing per whitelist
Jan 15 22:40:48.304: INFO: namespace e2e-tests-projected-jf7vk deletion completed in 7.400783249s

• [SLOW TEST:9.671 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:38
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:87
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:48.304: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:87
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 15 22:40:48.428: INFO: Waiting up to 5m0s for pod "pod-21c153d1-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-mn7pp" to be "success or failure"
Jan 15 22:40:48.444: INFO: Pod "pod-21c153d1-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.483999ms
Jan 15 22:40:50.460: INFO: Pod "pod-21c153d1-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031632489s
STEP: Saw pod success
Jan 15 22:40:50.460: INFO: Pod "pod-21c153d1-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:40:50.476: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-21c153d1-fa45-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:40:50.520: INFO: Waiting for pod pod-21c153d1-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:40:50.535: INFO: Pod pod-21c153d1-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:40:50.535: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mn7pp" for this suite.
Jan 15 22:40:57.523: INFO: namespace: e2e-tests-emptydir-mn7pp, resource: bindings, ignored listing per whitelist
Jan 15 22:40:57.970: INFO: namespace e2e-tests-emptydir-mn7pp deletion completed in 7.404947652s

• [SLOW TEST:9.666 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0644,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:87
------------------------------
[k8s.io] HostPath 
  should give a volume the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:43
[BeforeEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:40:57.970: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:43
STEP: Creating a pod to test hostPath mode
Jan 15 22:40:58.113: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-4vjfl" to be "success or failure"
Jan 15 22:40:58.128: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.91295ms
Jan 15 22:41:00.143: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030696343s
STEP: Saw pod success
Jan 15 22:41:00.144: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 15 22:41:00.159: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 15 22:41:00.206: INFO: Waiting for pod pod-host-path-test to disappear
Jan 15 22:41:00.221: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [k8s.io] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:41:00.221: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-4vjfl" for this suite.
Jan 15 22:41:07.611: INFO: namespace: e2e-tests-hostpath-4vjfl, resource: bindings, ignored listing per whitelist
Jan 15 22:41:07.656: INFO: namespace e2e-tests-hostpath-4vjfl deletion completed in 7.406407425s

• [SLOW TEST:9.686 seconds]
[k8s.io] HostPath
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should give a volume the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:43
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:948
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:41:07.657: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should create services for rc [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:948
STEP: creating Redis RC
Jan 15 22:41:07.812: INFO: namespace e2e-tests-kubectl-4xqjc
Jan 15 22:41:07.812: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4xqjc'
Jan 15 22:41:08.677: INFO: stderr: ""
Jan 15 22:41:08.677: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Jan 15 22:41:09.693: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:41:09.693: INFO: Found 1 / 1
Jan 15 22:41:09.693: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 15 22:41:09.708: INFO: Selector matched 1 pods for map[app:redis]
Jan 15 22:41:09.708: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 15 22:41:09.708: INFO: wait on redis-master startup in e2e-tests-kubectl-4xqjc 
Jan 15 22:41:09.708: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-rntcm redis-master --namespace=e2e-tests-kubectl-4xqjc'
Jan 15 22:41:09.928: INFO: stderr: ""
Jan 15 22:41:09.928: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Jan 22:41:09.487 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Jan 22:41:09.487 # Server started, Redis version 3.2.8\n1:M 15 Jan 22:41:09.488 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Jan 22:41:09.488 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 15 22:41:09.928: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-4xqjc'
Jan 15 22:41:10.155: INFO: stderr: ""
Jan 15 22:41:10.155: INFO: stdout: "service \"rm2\" exposed\n"
Jan 15 22:41:10.179: INFO: Service rm2 in namespace e2e-tests-kubectl-4xqjc found.
STEP: exposing service
Jan 15 22:41:12.210: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-4xqjc'
Jan 15 22:41:12.402: INFO: stderr: ""
Jan 15 22:41:12.402: INFO: stdout: "service \"rm3\" exposed\n"
Jan 15 22:41:12.418: INFO: Service rm3 in namespace e2e-tests-kubectl-4xqjc found.
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:41:14.448: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4xqjc" for this suite.
Jan 15 22:41:37.553: INFO: namespace: e2e-tests-kubectl-4xqjc, resource: bindings, ignored listing per whitelist
Jan 15 22:41:37.885: INFO: namespace e2e-tests-kubectl-4xqjc deletion completed in 23.407763519s

• [SLOW TEST:30.228 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create services for rc [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:948
------------------------------
SSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:79
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:41:37.885: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:79
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 15 22:41:38.013: INFO: Waiting up to 5m0s for pod "pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-t2rqx" to be "success or failure"
Jan 15 22:41:38.028: INFO: Pod "pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.934029ms
Jan 15 22:41:40.044: INFO: Pod "pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03031839s
STEP: Saw pod success
Jan 15 22:41:40.044: INFO: Pod "pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:41:40.059: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:41:40.101: INFO: Waiting for pod pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:41:40.116: INFO: Pod pod-3f4f8e7c-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:41:40.116: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t2rqx" for this suite.
Jan 15 22:41:46.905: INFO: namespace: e2e-tests-emptydir-t2rqx, resource: bindings, ignored listing per whitelist
Jan 15 22:41:47.550: INFO: namespace e2e-tests-emptydir-t2rqx deletion completed in 7.405740512s

• [SLOW TEST:9.666 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:79
------------------------------
SSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:119
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:41:47.551: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:119
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 15 22:41:47.700: INFO: Waiting up to 5m0s for pod "pod-45156b0b-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-nc8ld" to be "success or failure"
Jan 15 22:41:47.716: INFO: Pod "pod-45156b0b-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.264541ms
Jan 15 22:41:49.731: INFO: Pod "pod-45156b0b-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030845384s
STEP: Saw pod success
Jan 15 22:41:49.731: INFO: Pod "pod-45156b0b-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:41:49.747: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-45156b0b-fa45-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:41:49.790: INFO: Waiting for pod pod-45156b0b-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:41:49.805: INFO: Pod pod-45156b0b-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:41:49.805: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nc8ld" for this suite.
Jan 15 22:41:56.849: INFO: namespace: e2e-tests-emptydir-nc8ld, resource: bindings, ignored listing per whitelist
Jan 15 22:41:57.232: INFO: namespace e2e-tests-emptydir-nc8ld deletion completed in 7.398963158s

• [SLOW TEST:9.682 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0666,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:119
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:51
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:41:57.233: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:51
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 15 22:42:03.472: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-lrjdw PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:42:03.472: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:42:03.741: INFO: Exec stderr: ""
Jan 15 22:42:03.741: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-lrjdw PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:42:03.741: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:42:03.938: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 15 22:42:03.938: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-lrjdw PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:42:03.938: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:42:04.128: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 15 22:42:04.128: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-lrjdw PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:42:04.128: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:42:04.332: INFO: Exec stderr: ""
Jan 15 22:42:04.333: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-lrjdw PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:42:04.333: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:42:04.524: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:42:04.524: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-lrjdw" for this suite.
Jan 15 22:42:51.694: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-lrjdw, resource: bindings, ignored listing per whitelist
Jan 15 22:42:51.971: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-lrjdw deletion completed in 47.41833316s

• [SLOW TEST:54.738 seconds]
[k8s.io] KubeletManagedEtcHosts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should test kubelet managed /etc/hosts file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet_etc_hosts.go:51
------------------------------
S
------------------------------
[k8s.io] Projected 
  updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:42:51.971: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
Jan 15 22:42:52.142: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b8185d8-fa45-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6b8185d8-fa45-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:42:56.306: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r2hzk" for this suite.
Jan 15 22:43:19.224: INFO: namespace: e2e-tests-projected-r2hzk, resource: bindings, ignored listing per whitelist
Jan 15 22:43:19.729: INFO: namespace e2e-tests-projected-r2hzk deletion completed in 23.393594608s

• [SLOW TEST:27.758 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:409
------------------------------
SSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:54
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:43:19.729: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:54
STEP: Creating projection with secret that has name projected-secret-test-map-7c03bb86-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:43:19.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-74q8b" to be "success or failure"
Jan 15 22:43:19.888: INFO: Pod "pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.086612ms
Jan 15 22:43:21.903: INFO: Pod "pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03075464s
STEP: Saw pod success
Jan 15 22:43:21.903: INFO: Pod "pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:43:21.918: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:43:21.963: INFO: Waiting for pod pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:43:21.978: INFO: Pod pod-projected-secrets-7c06192b-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:43:21.978: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-74q8b" for this suite.
Jan 15 22:43:29.217: INFO: namespace: e2e-tests-projected-74q8b, resource: bindings, ignored listing per whitelist
Jan 15 22:43:29.411: INFO: namespace e2e-tests-projected-74q8b deletion completed in 7.404460345s

• [SLOW TEST:9.682 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:54
------------------------------
[k8s.io] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:192
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:43:29.412: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:43:29.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-slbtr" to be "success or failure"
Jan 15 22:43:29.548: INFO: Pod "downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.510773ms
Jan 15 22:43:31.563: INFO: Pod "downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030773834s
STEP: Saw pod success
Jan 15 22:43:31.563: INFO: Pod "downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:43:31.578: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:43:31.620: INFO: Waiting for pod downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:43:31.635: INFO: Pod downwardapi-volume-81c7d5fb-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:43:31.635: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-slbtr" for this suite.
Jan 15 22:43:38.289: INFO: namespace: e2e-tests-downward-api-slbtr, resource: bindings, ignored listing per whitelist
Jan 15 22:43:39.063: INFO: namespace e2e-tests-downward-api-slbtr deletion completed in 7.398772454s

• [SLOW TEST:9.651 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:192
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:43:39.063: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should support --unix-socket=/path [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
STEP: Starting the proxy
Jan 15 22:43:39.199: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix739124242/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:43:39.286: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h9nkx" for this suite.
Jan 15 22:43:46.124: INFO: namespace: e2e-tests-kubectl-h9nkx, resource: bindings, ignored listing per whitelist
Jan 15 22:43:46.720: INFO: namespace e2e-tests-kubectl-h9nkx deletion completed in 7.406205942s

• [SLOW TEST:7.658 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should support --unix-socket=/path [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
------------------------------
SSS
------------------------------
[k8s.io] Downward API volume 
  should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:43:46.721: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:43:46.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-txk6m" to be "success or failure"
Jan 15 22:43:46.857: INFO: Pod "downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.254488ms
Jan 15 22:43:48.872: INFO: Pod "downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030681393s
STEP: Saw pod success
Jan 15 22:43:48.872: INFO: Pod "downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:43:48.888: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:43:48.928: INFO: Waiting for pod downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:43:48.943: INFO: Pod downwardapi-volume-8c1927e3-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:43:48.943: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-txk6m" for this suite.
Jan 15 22:43:55.805: INFO: namespace: e2e-tests-downward-api-txk6m, resource: bindings, ignored listing per whitelist
Jan 15 22:43:56.376: INFO: namespace e2e-tests-downward-api-txk6m deletion completed in 7.403887145s

• [SLOW TEST:9.655 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide podname only [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:396
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:43:56.376: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:396
STEP: Creating configMap with name projected-configmap-test-volume-map-91e217c4-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:43:56.568: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-272gk" to be "success or failure"
Jan 15 22:43:56.583: INFO: Pod "pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.334102ms
Jan 15 22:43:58.599: INFO: Pod "pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030931658s
STEP: Saw pod success
Jan 15 22:43:58.599: INFO: Pod "pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:43:58.614: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:43:58.658: INFO: Waiting for pod pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:43:58.673: INFO: Pod pod-projected-configmaps-91e4bb45-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:43:58.673: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-272gk" for this suite.
Jan 15 22:44:05.392: INFO: namespace: e2e-tests-projected-272gk, resource: bindings, ignored listing per whitelist
Jan 15 22:44:06.135: INFO: namespace e2e-tests-projected-272gk deletion completed in 7.43302817s

• [SLOW TEST:9.759 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:396
------------------------------
SSSSSSS
------------------------------
[k8s.io] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:931
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:44:06.135: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:931
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:44:06.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-djvc9" to be "success or failure"
Jan 15 22:44:06.267: INFO: Pod "downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.197775ms
Jan 15 22:44:08.282: INFO: Pod "downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0307275s
STEP: Saw pod success
Jan 15 22:44:08.282: INFO: Pod "downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:44:08.297: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:44:08.341: INFO: Waiting for pod downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:44:08.356: INFO: Pod downwardapi-volume-97aab156-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:44:08.356: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-djvc9" for this suite.
Jan 15 22:44:15.392: INFO: namespace: e2e-tests-projected-djvc9, resource: bindings, ignored listing per whitelist
Jan 15 22:44:15.780: INFO: namespace e2e-tests-projected-djvc9 deletion completed in 7.395526457s

• [SLOW TEST:9.645 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:931
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1278
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:44:15.781: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
[It] should support rolling-update to same image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1278
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 22:44:15.882: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:16.653: INFO: stderr: ""
Jan 15 22:44:16.653: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jan 15 22:44:16.683: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:27.907: INFO: stderr: ""
Jan 15 22:44:27.907: INFO: stdout: "Created e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f\nScaling up e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f to e2e-test-nginx-rc\nreplicationcontroller \"e2e-test-nginx-rc\" rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 15 22:44:27.907: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:28.112: INFO: stderr: ""
Jan 15 22:44:28.112: INFO: stdout: "e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f-wzvbp "
Jan 15 22:44:28.113: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f-wzvbp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:28.317: INFO: stderr: ""
Jan 15 22:44:28.317: INFO: stdout: "true"
Jan 15 22:44:28.317: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f-wzvbp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:28.521: INFO: stderr: ""
Jan 15 22:44:28.521: INFO: stdout: "gcr.io/google-containers/nginx-slim-amd64:0.20"
Jan 15 22:44:28.521: INFO: e2e-test-nginx-rc-86e9ebe769e78b954d1e51669090451f-wzvbp is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
Jan 15 22:44:28.521: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n64v9'
Jan 15 22:44:28.810: INFO: stderr: ""
Jan 15 22:44:28.810: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:44:28.810: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n64v9" for this suite.
Jan 15 22:44:35.657: INFO: namespace: e2e-tests-kubectl-n64v9, resource: bindings, ignored listing per whitelist
Jan 15 22:44:36.240: INFO: namespace e2e-tests-kubectl-n64v9 deletion completed in 7.401362268s

• [SLOW TEST:20.460 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should support rolling-update to same image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1278
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:316
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:44:36.240: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:303
[It] should scale a replication controller [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:316
STEP: creating a replication controller
Jan 15 22:44:36.347: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:36.606: INFO: stderr: ""
Jan 15 22:44:36.606: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:44:36.607: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:36.812: INFO: stderr: ""
Jan 15 22:44:36.812: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-s6ng5 "
Jan 15 22:44:36.812: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:37.018: INFO: stderr: ""
Jan 15 22:44:37.018: INFO: stdout: ""
Jan 15 22:44:37.018: INFO: update-demo-nautilus-9dpww is created but not running
Jan 15 22:44:42.018: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:42.227: INFO: stderr: ""
Jan 15 22:44:42.227: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-s6ng5 "
Jan 15 22:44:42.227: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:42.431: INFO: stderr: ""
Jan 15 22:44:42.431: INFO: stdout: "true"
Jan 15 22:44:42.431: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:42.636: INFO: stderr: ""
Jan 15 22:44:42.636: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:44:42.636: INFO: validating pod update-demo-nautilus-9dpww
Jan 15 22:44:42.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:44:42.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:44:42.669: INFO: update-demo-nautilus-9dpww is verified up and running
Jan 15 22:44:42.669: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-s6ng5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:42.873: INFO: stderr: ""
Jan 15 22:44:42.873: INFO: stdout: "true"
Jan 15 22:44:42.874: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-s6ng5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:43.079: INFO: stderr: ""
Jan 15 22:44:43.079: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:44:43.079: INFO: validating pod update-demo-nautilus-s6ng5
Jan 15 22:44:43.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:44:43.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:44:43.112: INFO: update-demo-nautilus-s6ng5 is verified up and running
STEP: scaling down the replication controller
Jan 15 22:44:43.112: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:43.362: INFO: stderr: ""
Jan 15 22:44:43.362: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:44:43.362: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:43.569: INFO: stderr: ""
Jan 15 22:44:43.569: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-s6ng5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 15 22:44:48.569: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:48.781: INFO: stderr: ""
Jan 15 22:44:48.781: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-s6ng5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 15 22:44:53.781: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:53.990: INFO: stderr: ""
Jan 15 22:44:53.990: INFO: stdout: "update-demo-nautilus-9dpww "
Jan 15 22:44:53.990: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:54.194: INFO: stderr: ""
Jan 15 22:44:54.194: INFO: stdout: "true"
Jan 15 22:44:54.194: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:54.399: INFO: stderr: ""
Jan 15 22:44:54.399: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:44:54.399: INFO: validating pod update-demo-nautilus-9dpww
Jan 15 22:44:54.430: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:44:54.430: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:44:54.430: INFO: update-demo-nautilus-9dpww is verified up and running
STEP: scaling up the replication controller
Jan 15 22:44:54.430: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:54.703: INFO: stderr: ""
Jan 15 22:44:54.703: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 15 22:44:54.703: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:54.910: INFO: stderr: ""
Jan 15 22:44:54.910: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-c9dcm "
Jan 15 22:44:54.911: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:55.117: INFO: stderr: ""
Jan 15 22:44:55.117: INFO: stdout: "true"
Jan 15 22:44:55.117: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:55.322: INFO: stderr: ""
Jan 15 22:44:55.322: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:44:55.322: INFO: validating pod update-demo-nautilus-9dpww
Jan 15 22:44:55.352: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:44:55.352: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:44:55.352: INFO: update-demo-nautilus-9dpww is verified up and running
Jan 15 22:44:55.352: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-c9dcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:44:55.558: INFO: stderr: ""
Jan 15 22:44:55.558: INFO: stdout: ""
Jan 15 22:44:55.558: INFO: update-demo-nautilus-c9dcm is created but not running
Jan 15 22:45:00.558: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:00.767: INFO: stderr: ""
Jan 15 22:45:00.767: INFO: stdout: "update-demo-nautilus-9dpww update-demo-nautilus-c9dcm "
Jan 15 22:45:00.767: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:00.975: INFO: stderr: ""
Jan 15 22:45:00.975: INFO: stdout: "true"
Jan 15 22:45:00.975: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-9dpww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:01.182: INFO: stderr: ""
Jan 15 22:45:01.182: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:45:01.182: INFO: validating pod update-demo-nautilus-9dpww
Jan 15 22:45:01.214: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:45:01.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:45:01.214: INFO: update-demo-nautilus-9dpww is verified up and running
Jan 15 22:45:01.214: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-c9dcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:01.426: INFO: stderr: ""
Jan 15 22:45:01.426: INFO: stdout: "true"
Jan 15 22:45:01.426: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-c9dcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:01.641: INFO: stderr: ""
Jan 15 22:45:01.641: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jan 15 22:45:01.641: INFO: validating pod update-demo-nautilus-c9dcm
Jan 15 22:45:01.676: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 15 22:45:01.676: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 15 22:45:01.676: INFO: update-demo-nautilus-c9dcm is verified up and running
STEP: using delete to clean up resources
Jan 15 22:45:01.676: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:01.999: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:45:01.999: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Jan 15 22:45:01.999: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vlzmk'
Jan 15 22:45:02.181: INFO: stderr: "No resources found.\n"
Jan 15 22:45:02.181: INFO: stdout: ""
Jan 15 22:45:02.181: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-vlzmk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 15 22:45:02.392: INFO: stderr: ""
Jan 15 22:45:02.392: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:45:02.392: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vlzmk" for this suite.
Jan 15 22:45:25.195: INFO: namespace: e2e-tests-kubectl-vlzmk, resource: bindings, ignored listing per whitelist
Jan 15 22:45:25.822: INFO: namespace e2e-tests-kubectl-vlzmk deletion completed in 23.40067525s

• [SLOW TEST:49.582 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should scale a replication controller [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:316
------------------------------
SS
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:49
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:45:25.822: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:49
STEP: Creating configMap with name configmap-test-volume-c727785e-fa45-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:45:25.939: INFO: Waiting up to 5m0s for pod "pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-z4rq6" to be "success or failure"
Jan 15 22:45:25.955: INFO: Pod "pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.949115ms
Jan 15 22:45:27.971: INFO: Pod "pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031450692s
STEP: Saw pod success
Jan 15 22:45:27.971: INFO: Pod "pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:45:27.986: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:45:28.027: INFO: Waiting for pod pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:45:28.042: INFO: Pod pod-configmaps-c72a31e9-fa45-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:45:28.042: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-z4rq6" for this suite.
Jan 15 22:45:34.897: INFO: namespace: e2e-tests-configmap-z4rq6, resource: bindings, ignored listing per whitelist
Jan 15 22:45:35.477: INFO: namespace e2e-tests-configmap-z4rq6 deletion completed in 7.406537966s

• [SLOW TEST:9.655 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:49
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:347
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:45:35.477: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Jan 15 22:45:35.620: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 15 22:46:35.750: INFO: Waiting for terminating namespaces to be deleted...
Jan 15 22:46:35.781: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 15 22:46:35.826: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 15 22:46:35.826: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 15 22:46:35.841: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jan 15 22:46:35.841: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-prvd before test
Jan 15 22:46:35.875: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-r6ln before test
Jan 15 22:46:35.915: INFO: registry-console-1-mwx65 from default started at 2018-01-15 22:09:36 +0000 UTC (1 container statuses recorded)
Jan 15 22:46:35.915: INFO: 	Container registry-console ready: true, restart count 0
Jan 15 22:46:35.915: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-v6l4 before test
[It] validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:347
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f2280846-fa45-11e7-b6d5-0e5e74815504 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f2280846-fa45-11e7-b6d5-0e5e74815504 off the node ci-prtest-cc63063-250-ig-n-v6l4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f2280846-fa45-11e7-b6d5-0e5e74815504
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:46:40.205: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-fvcv9" for this suite.
Jan 15 22:47:02.924: INFO: namespace: e2e-tests-sched-pred-fvcv9, resource: bindings, ignored listing per whitelist
Jan 15 22:47:03.630: INFO: namespace e2e-tests-sched-pred-fvcv9 deletion completed in 23.409563656s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:88.153 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:347
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  volume on default medium should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:99
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:03.631: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:99
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 15 22:47:03.759: INFO: Waiting up to 5m0s for pod "pod-01781b7c-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-xkf82" to be "success or failure"
Jan 15 22:47:03.775: INFO: Pod "pod-01781b7c-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.409473ms
Jan 15 22:47:05.790: INFO: Pod "pod-01781b7c-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031003862s
STEP: Saw pod success
Jan 15 22:47:05.791: INFO: Pod "pod-01781b7c-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:47:05.807: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-01781b7c-fa46-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:47:05.853: INFO: Waiting for pod pod-01781b7c-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:47:05.868: INFO: Pod pod-01781b7c-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:47:05.868: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xkf82" for this suite.
Jan 15 22:47:13.120: INFO: namespace: e2e-tests-emptydir-xkf82, resource: bindings, ignored listing per whitelist
Jan 15 22:47:13.344: INFO: namespace e2e-tests-emptydir-xkf82 deletion completed in 7.446683879s

• [SLOW TEST:9.713 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  volume on default medium should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:99
------------------------------
SSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:123
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:13.344: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:123
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 15 22:47:13.458: INFO: Waiting up to 5m0s for pod "pod-0740787b-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-tj5bp" to be "success or failure"
Jan 15 22:47:13.474: INFO: Pod "pod-0740787b-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.238882ms
Jan 15 22:47:15.490: INFO: Pod "pod-0740787b-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031197044s
STEP: Saw pod success
Jan 15 22:47:15.490: INFO: Pod "pod-0740787b-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:47:15.505: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-0740787b-fa46-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:47:15.553: INFO: Waiting for pod pod-0740787b-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:47:15.569: INFO: Pod pod-0740787b-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:47:15.569: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tj5bp" for this suite.
Jan 15 22:47:22.983: INFO: namespace: e2e-tests-emptydir-tj5bp, resource: bindings, ignored listing per whitelist
Jan 15 22:47:22.998: INFO: namespace e2e-tests-emptydir-tj5bp deletion completed in 7.400076427s

• [SLOW TEST:9.654 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0777,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:123
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:41
[BeforeEach] [sig-apps] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:22.998: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:41
STEP: Creating replication controller my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504
Jan 15 22:47:23.138: INFO: Pod name my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504: Found 1 pods out of 1
Jan 15 22:47:23.138: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504" are running
Jan 15 22:47:25.169: INFO: Pod "my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504-gk4zb" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-01-15 22:47:23 +0000 UTC Reason: Message:}])
Jan 15 22:47:25.169: INFO: Trying to dial the pod
Jan 15 22:47:30.233: INFO: Controller my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504: Got expected result from replica 1 [my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504-gk4zb]: "my-hostname-basic-0d0306eb-fa46-11e7-b6d5-0e5e74815504-gk4zb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:47:30.233: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-chkq2" for this suite.
Jan 15 22:47:36.843: INFO: namespace: e2e-tests-replication-controller-chkq2, resource: bindings, ignored listing per whitelist
Jan 15 22:47:37.662: INFO: namespace e2e-tests-replication-controller-chkq2 deletion completed in 7.401314075s

• [SLOW TEST:14.665 seconds]
[sig-apps] ReplicationController
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:41
------------------------------
SSSSS
------------------------------
[k8s.io] Secrets 
  should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:82
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:37.663: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:82
STEP: Creating secret with name secret-test-15c268a4-fa46-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:47:37.814: INFO: Waiting up to 5m0s for pod "pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-t2lj2" to be "success or failure"
Jan 15 22:47:37.830: INFO: Pod "pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.986821ms
Jan 15 22:47:39.846: INFO: Pod "pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031536849s
STEP: Saw pod success
Jan 15 22:47:39.846: INFO: Pod "pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:47:39.861: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:47:39.905: INFO: Waiting for pod pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:47:39.920: INFO: Pod pod-secrets-15c4d793-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:47:39.920: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t2lj2" for this suite.
Jan 15 22:47:46.911: INFO: namespace: e2e-tests-secrets-t2lj2, resource: bindings, ignored listing per whitelist
Jan 15 22:47:47.338: INFO: namespace e2e-tests-secrets-t2lj2 deletion completed in 7.389046969s

• [SLOW TEST:9.676 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable in multiple volumes in a pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:82
------------------------------
SSSSSSS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:45
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:47.338: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:45
STEP: Creating secret with name secret-test-1b8ad39a-fa46-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 22:47:47.517: INFO: Waiting up to 5m0s for pod "pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-2tmf5" to be "success or failure"
Jan 15 22:47:47.532: INFO: Pod "pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.184711ms
Jan 15 22:47:49.548: INFO: Pod "pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030845761s
STEP: Saw pod success
Jan 15 22:47:49.548: INFO: Pod "pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:47:49.563: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 22:47:49.607: INFO: Waiting for pod pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:47:49.622: INFO: Pod pod-secrets-1b8d45d4-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:47:49.622: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2tmf5" for this suite.
Jan 15 22:47:56.780: INFO: namespace: e2e-tests-secrets-2tmf5, resource: bindings, ignored listing per whitelist
Jan 15 22:47:57.069: INFO: namespace e2e-tests-secrets-2tmf5 deletion completed in 7.417949862s

• [SLOW TEST:9.731 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:45
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:47:57.069: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should create a job from an image, then delete the job [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
STEP: executing a command with run --rm and attach with stdin
Jan 15 22:47:57.213: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig --namespace=e2e-tests-kubectl-fmp6d run e2e-test-rm-busybox-job --image=busybox --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 15 22:48:02.280: INFO: stderr: "If you don't see a command prompt, try pressing enter.\n"
Jan 15 22:48:02.280: INFO: stdout: "abcd1234stdin closed\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:48:02.295: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fmp6d" for this suite.
Jan 15 22:48:08.994: INFO: namespace: e2e-tests-kubectl-fmp6d, resource: bindings, ignored listing per whitelist
Jan 15 22:48:09.730: INFO: namespace e2e-tests-kubectl-fmp6d deletion completed in 7.406153489s

• [SLOW TEST:12.661 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create a job from an image, then delete the job [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
------------------------------
[k8s.io] Pods 
  should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:269
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:48:09.730: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:269
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 15 22:48:12.452: INFO: Successfully updated pod "pod-update-28de3795-fa46-11e7-b6d5-0e5e74815504"
STEP: verifying the updated pod is in kubernetes
Jan 15 22:48:12.482: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:48:12.482: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fjqlw" for this suite.
Jan 15 22:48:35.122: INFO: namespace: e2e-tests-pods-fjqlw, resource: bindings, ignored listing per whitelist
Jan 15 22:48:35.911: INFO: namespace e2e-tests-pods-fjqlw deletion completed in 23.399380387s

• [SLOW TEST:26.181 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:269
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:306
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:48:35.911: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Jan 15 22:48:36.015: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 15 22:49:36.144: INFO: Waiting for terminating namespaces to be deleted...
Jan 15 22:49:36.174: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 15 22:49:36.219: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 15 22:49:36.219: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 15 22:49:36.234: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jan 15 22:49:36.235: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-prvd before test
Jan 15 22:49:36.267: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-r6ln before test
Jan 15 22:49:36.298: INFO: registry-console-1-mwx65 from default started at 2018-01-15 22:09:36 +0000 UTC (1 container statuses recorded)
Jan 15 22:49:36.298: INFO: 	Container registry-console ready: true, restart count 0
Jan 15 22:49:36.298: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-v6l4 before test
[It] validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:306
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.150a1d6c7fd3349f], Reason = [FailedScheduling], Message = [No nodes are available that match all of the predicates: MatchNodeSelector (4), NodeUnschedulable (1).]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:49:37.423: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-fd77w" for this suite.
Jan 15 22:50:00.279: INFO: namespace: e2e-tests-sched-pred-fd77w, resource: bindings, ignored listing per whitelist
Jan 15 22:50:00.861: INFO: namespace e2e-tests-sched-pred-fd77w deletion completed in 23.407981152s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:84.950 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:306
------------------------------
SSSS
------------------------------
[k8s.io] Projected 
  should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:913
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:50:00.861: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:913
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:50:00.958: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-7jnfb" to be "success or failure"
Jan 15 22:50:00.974: INFO: Pod "downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.370089ms
Jan 15 22:50:02.990: INFO: Pod "downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031875843s
STEP: Saw pod success
Jan 15 22:50:02.990: INFO: Pod "downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:50:03.005: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:50:03.047: INFO: Waiting for pod downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:50:03.062: INFO: Pod downwardapi-volume-6b15f207-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:50:03.062: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7jnfb" for this suite.
Jan 15 22:50:10.101: INFO: namespace: e2e-tests-projected-7jnfb, resource: bindings, ignored listing per whitelist
Jan 15 22:50:10.500: INFO: namespace e2e-tests-projected-7jnfb deletion completed in 7.409095012s

• [SLOW TEST:9.639 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:913
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:50:10.500: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
STEP: Creating configMap with name projected-configmap-test-volume-map-70d94d3a-fa46-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:50:10.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-w6s2k" to be "success or failure"
Jan 15 22:50:10.652: INFO: Pod "pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.043567ms
Jan 15 22:50:12.667: INFO: Pod "pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030612444s
STEP: Saw pod success
Jan 15 22:50:12.667: INFO: Pod "pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:50:12.682: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:50:12.725: INFO: Waiting for pod pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:50:12.741: INFO: Pod pod-projected-configmaps-70dbb88a-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:50:12.741: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w6s2k" for this suite.
Jan 15 22:50:19.630: INFO: namespace: e2e-tests-projected-w6s2k, resource: bindings, ignored listing per whitelist
Jan 15 22:50:20.174: INFO: namespace e2e-tests-projected-w6s2k deletion completed in 7.404969335s

• [SLOW TEST:9.674 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:401
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:374
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:50:20.175: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:374
STEP: Creating configMap with name projected-configmap-test-volume-769e605a-fa46-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:50:20.319: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-xc544" to be "success or failure"
Jan 15 22:50:20.335: INFO: Pod "pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.227047ms
Jan 15 22:50:22.351: INFO: Pod "pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031847054s
STEP: Saw pod success
Jan 15 22:50:22.351: INFO: Pod "pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:50:22.367: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:50:22.409: INFO: Waiting for pod pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:50:22.424: INFO: Pod pod-projected-configmaps-76a0d7dd-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:50:22.424: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xc544" for this suite.
Jan 15 22:50:29.139: INFO: namespace: e2e-tests-projected-xc544, resource: bindings, ignored listing per whitelist
Jan 15 22:50:29.857: INFO: namespace e2e-tests-projected-xc544 deletion completed in 7.40409295s

• [SLOW TEST:9.682 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:374
------------------------------
SSSSS
------------------------------
[k8s.io] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:202
[BeforeEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:50:29.857: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:199
[It] should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:202
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:50:30.002: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-btk72" for this suite.
Jan 15 22:50:53.002: INFO: namespace: e2e-tests-pods-btk72, resource: bindings, ignored listing per whitelist
Jan 15 22:50:53.434: INFO: namespace e2e-tests-pods-btk72 deletion completed in 23.402724503s

• [SLOW TEST:23.577 seconds]
[k8s.io] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should be submitted and removed [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:202
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:75
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:50:53.434: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:75
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-z5c6r in namespace e2e-tests-proxy-z2fnp
I0115 22:50:53.579537   17569 runners.go:177] Created replication controller with name: proxy-service-z5c6r, namespace: e2e-tests-proxy-z2fnp, replica count: 1
I0115 22:50:54.579921   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0115 22:50:55.580141   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0115 22:50:56.580379   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:50:57.580656   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:50:58.580936   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:50:59.581149   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:51:00.581368   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:51:01.581601   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0115 22:51:02.581858   17569 runners.go:177] proxy-service-z5c6r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 15 22:51:02.597: INFO: setup took 9.057261256s, starting test cases
STEP: running 34 cases, 20 attempts per case, 680 total attempts
Jan 15 22:51:02.619: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 22.023112ms)
Jan 15 22:51:02.619: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 22.197675ms)
Jan 15 22:51:02.624: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 27.395033ms)
Jan 15 22:51:02.626: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 29.113319ms)
Jan 15 22:51:02.626: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 29.583315ms)
Jan 15 22:51:02.627: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 29.707725ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 46.431941ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 46.69801ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 46.423388ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 46.646907ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 46.625444ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 46.447436ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 46.754811ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 46.666589ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 47.20677ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 47.17276ms)
Jan 15 22:51:02.644: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 47.150006ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 59.29157ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 59.433145ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 59.435776ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 59.632647ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 59.346418ms)
Jan 15 22:51:02.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 59.32241ms)
Jan 15 22:51:02.660: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 63.008383ms)
Jan 15 22:51:02.663: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 65.957191ms)
Jan 15 22:51:02.664: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 66.843179ms)
Jan 15 22:51:02.664: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 66.768925ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 76.059366ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 76.079944ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 75.973729ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 76.12659ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 76.380768ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 76.166085ms)
Jan 15 22:51:02.673: INFO: (0) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 75.972301ms)
Jan 15 22:51:02.696: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 22.621923ms)
Jan 15 22:51:02.696: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 22.559021ms)
Jan 15 22:51:02.697: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 23.116395ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 35.333462ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 35.283162ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 35.799037ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 35.569568ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 35.711558ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 35.261182ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 35.809413ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 35.634069ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 35.687518ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 35.464111ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 35.330867ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 35.808388ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 35.627367ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 35.750564ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 35.583967ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 35.439382ms)
Jan 15 22:51:02.709: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 35.302547ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 35.739199ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 35.364407ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 35.519028ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 35.421474ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 35.379898ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 35.304931ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 35.416286ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 35.355578ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 35.324871ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 35.476337ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 35.355861ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.386466ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 35.879337ms)
Jan 15 22:51:02.710: INFO: (1) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 36.1497ms)
Jan 15 22:51:02.728: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 18.157225ms)
Jan 15 22:51:02.729: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 18.557007ms)
Jan 15 22:51:02.729: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 18.619957ms)
Jan 15 22:51:02.737: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 26.225203ms)
Jan 15 22:51:02.741: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 31.065212ms)
Jan 15 22:51:02.741: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 31.095126ms)
Jan 15 22:51:02.741: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 30.936631ms)
Jan 15 22:51:02.741: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 31.157211ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 31.148972ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 30.852391ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 30.854232ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 31.143226ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 31.277124ms)
Jan 15 22:51:02.742: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 31.593242ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 39.296586ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 39.240365ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 39.219943ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 39.308994ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 39.221066ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 39.208101ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 39.323769ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 39.361571ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 39.331674ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 39.502509ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 39.309278ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 39.220594ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 39.472786ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 39.328088ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 39.27692ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 39.459726ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 39.48585ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 39.60803ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 39.524653ms)
Jan 15 22:51:02.750: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 39.444619ms)
Jan 15 22:51:02.774: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 24.16218ms)
Jan 15 22:51:02.774: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 24.379734ms)
Jan 15 22:51:02.776: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 25.768043ms)
Jan 15 22:51:02.776: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 25.85108ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 35.822795ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 35.702907ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 35.618489ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 35.641513ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 35.837859ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 35.766597ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 35.829282ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 35.979466ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 35.890115ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 35.868885ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 35.670738ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 35.77526ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 36.02353ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 35.88602ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.909582ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.745378ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 35.82328ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 35.874768ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 36.130286ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 35.809913ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 35.825192ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 36.208963ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 35.962925ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 36.099071ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 35.860829ms)
Jan 15 22:51:02.786: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 35.923186ms)
Jan 15 22:51:02.787: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 36.855044ms)
Jan 15 22:51:02.787: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 36.831704ms)
Jan 15 22:51:02.791: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 40.685205ms)
Jan 15 22:51:02.791: INFO: (3) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 40.947819ms)
Jan 15 22:51:02.823: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 31.895903ms)
Jan 15 22:51:02.823: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 31.27043ms)
Jan 15 22:51:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 44.108558ms)
Jan 15 22:51:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 44.512941ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.218066ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 44.667573ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 45.005115ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 45.315535ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 44.512882ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 44.477416ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 44.928969ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 44.440473ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 44.420684ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 45.249889ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 45.067858ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.579471ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 45.114872ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 45.221514ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 44.431407ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 44.887821ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 45.427561ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 45.179716ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 45.076725ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 45.004523ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 44.772085ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 44.564332ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.991287ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 45.565657ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 44.432105ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 44.908645ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 44.557069ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 44.398351ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.470448ms)
Jan 15 22:51:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 44.726963ms)
Jan 15 22:51:02.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 22.493307ms)
Jan 15 22:51:02.862: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 24.86641ms)
Jan 15 22:51:02.862: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 24.956315ms)
Jan 15 22:51:02.863: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 25.210336ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 35.540324ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 35.506685ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 35.437536ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 35.192686ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 35.095995ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 35.082911ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.18767ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 35.22673ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.457052ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 35.359725ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 35.550966ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 35.464805ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 35.595663ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 35.766716ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 35.618189ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 35.509341ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 35.468569ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 35.432706ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 35.79121ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 35.583084ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 35.405568ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 35.490623ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 35.601101ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 35.646629ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 35.407345ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 35.723938ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 35.480656ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 35.756742ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 35.466611ms)
Jan 15 22:51:02.873: INFO: (5) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 35.657838ms)
Jan 15 22:51:02.902: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 29.227282ms)
Jan 15 22:51:02.902: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 29.038634ms)
Jan 15 22:51:02.903: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 29.220251ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 41.592068ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 42.010635ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 42.018044ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 41.90412ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 41.895559ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 41.710173ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 41.721285ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 42.034592ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 41.669349ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 41.661524ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 42.113373ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 41.849414ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 42.130845ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 42.200646ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 42.269348ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 42.051747ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 42.125624ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 42.080356ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 42.045927ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 42.23635ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 42.275339ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 42.205566ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 42.168565ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 41.95488ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 42.143926ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 42.231284ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 42.090339ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 42.164839ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 42.16012ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 42.061033ms)
Jan 15 22:51:02.915: INFO: (6) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 41.965257ms)
Jan 15 22:51:02.942: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 26.119159ms)
Jan 15 22:51:02.943: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 27.055979ms)
Jan 15 22:51:02.943: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 27.305888ms)
Jan 15 22:51:02.943: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 27.287016ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 39.49888ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 39.5785ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 39.214236ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 39.690065ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 39.56084ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 39.487469ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 39.63122ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 39.606293ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 39.780122ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 39.698567ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 39.435345ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 39.611742ms)
Jan 15 22:51:02.955: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 39.321552ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 39.494127ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 39.530141ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 39.388067ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 39.42488ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 39.395107ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 39.845325ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 39.561387ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 39.601549ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 39.638804ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 39.457264ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 39.401808ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 39.77663ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 39.75541ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 39.41ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 39.675545ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 39.460247ms)
Jan 15 22:51:02.956: INFO: (7) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 39.832138ms)
Jan 15 22:51:02.974: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 17.809197ms)
Jan 15 22:51:02.974: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 18.121229ms)
Jan 15 22:51:02.974: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 18.625853ms)
Jan 15 22:51:02.974: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 18.804509ms)
Jan 15 22:51:02.975: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 19.006637ms)
Jan 15 22:51:02.994: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 38.051562ms)
Jan 15 22:51:02.995: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 38.837956ms)
Jan 15 22:51:02.995: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 38.984715ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 50.768453ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 50.840724ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 50.623992ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 50.662986ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 50.653523ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 50.721109ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 50.717608ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 50.9372ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 50.825806ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 50.679306ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 51.04427ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 51.095788ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 51.041668ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 50.789281ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 50.97539ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 51.217005ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 50.999952ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 50.746927ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 50.880818ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 51.048199ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 50.762308ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 50.895513ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 50.942506ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 50.98453ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 50.97113ms)
Jan 15 22:51:03.007: INFO: (8) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 50.972442ms)
Jan 15 22:51:03.038: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 30.630498ms)
Jan 15 22:51:03.042: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 34.613911ms)
Jan 15 22:51:03.042: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 34.526661ms)
Jan 15 22:51:03.042: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 34.711822ms)
Jan 15 22:51:03.042: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 34.637632ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 43.671701ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 44.204053ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 44.136638ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 43.867009ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 43.757599ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 43.969589ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.047996ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 44.069492ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 44.251685ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 44.137128ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 44.214104ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 44.064111ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 44.122934ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 43.991991ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 44.28935ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 44.080969ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 44.214608ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 44.084686ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 44.029707ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 44.099516ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 44.343763ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 44.275974ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 44.406028ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 44.05776ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 44.324442ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 44.489795ms)
Jan 15 22:51:03.051: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 44.359816ms)
Jan 15 22:51:03.052: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 44.1563ms)
Jan 15 22:51:03.052: INFO: (9) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 44.560735ms)
Jan 15 22:51:03.083: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 31.158743ms)
Jan 15 22:51:03.084: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 31.632503ms)
Jan 15 22:51:03.084: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 31.778772ms)
Jan 15 22:51:03.084: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 31.796117ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 44.305955ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 44.258388ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 44.304884ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 44.169945ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 44.070354ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.228505ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 44.19383ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 44.186709ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 44.471461ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 44.200739ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 44.262927ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 44.724071ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 44.611865ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 44.394023ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 44.350864ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 44.438191ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 44.413437ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 44.636999ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.607781ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 44.4255ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.382887ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.586818ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 44.533139ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 44.676556ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 44.723389ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 44.614028ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 44.730763ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 44.514118ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 44.653544ms)
Jan 15 22:51:03.096: INFO: (10) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 44.561597ms)
Jan 15 22:51:03.122: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 25.572054ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 31.10893ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 31.475151ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 31.091758ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 31.461685ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 31.415762ms)
Jan 15 22:51:03.128: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 31.122856ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 38.817326ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 38.832219ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 38.894843ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 39.012121ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 39.198574ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 39.139076ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 39.16734ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 39.013779ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 39.149027ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 38.975547ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 39.133154ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 39.359595ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 39.01229ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 39.077608ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 39.054156ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 39.091077ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 39.167415ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 39.346581ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 39.413425ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 39.169186ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 39.05824ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 39.323984ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 39.239294ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 39.287987ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 39.432306ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 39.33216ms)
Jan 15 22:51:03.136: INFO: (11) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 38.988351ms)
Jan 15 22:51:03.160: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 23.903257ms)
Jan 15 22:51:03.160: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 24.20923ms)
Jan 15 22:51:03.160: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 24.07137ms)
Jan 15 22:51:03.162: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 26.06398ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 36.866431ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 37.020759ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 36.653148ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 36.766707ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 36.922604ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 36.870279ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 36.950782ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 36.891302ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 37.117314ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 37.023955ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 36.770211ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 36.827907ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 36.696958ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 36.870465ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 36.923777ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 36.803313ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 36.77816ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 36.723452ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 36.903257ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 37.143568ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 37.060887ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 37.175769ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 36.98127ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 37.278761ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 36.928198ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 36.901271ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 36.825858ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 37.081755ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 37.15403ms)
Jan 15 22:51:03.173: INFO: (12) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 37.056092ms)
Jan 15 22:51:03.195: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 21.91353ms)
Jan 15 22:51:03.197: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 23.162858ms)
Jan 15 22:51:03.197: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 23.304239ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 34.990408ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 34.867171ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 34.91403ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 34.740693ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 34.799211ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 34.938988ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 34.867397ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 34.751985ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 34.892034ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 34.806209ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 34.71949ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 34.72674ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 34.712087ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 34.840472ms)
Jan 15 22:51:03.208: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 35.194095ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 34.974307ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 34.868984ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 35.240409ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 35.044629ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 35.048704ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 34.951526ms)
Jan 15 22:51:03.209: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 34.982628ms)
Jan 15 22:51:03.210: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 35.954298ms)
Jan 15 22:51:03.210: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 36.000778ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 36.849009ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 37.045585ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 37.212565ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 37.304437ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 36.917275ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 37.329966ms)
Jan 15 22:51:03.211: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 36.996177ms)
Jan 15 22:51:03.238: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 27.290811ms)
Jan 15 22:51:03.239: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 27.68571ms)
Jan 15 22:51:03.239: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 28.11935ms)
Jan 15 22:51:03.239: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 27.920936ms)
Jan 15 22:51:03.239: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 28.27773ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 40.518397ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 40.490527ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 40.64029ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 40.150286ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 40.746278ms)
Jan 15 22:51:03.251: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 40.340682ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 40.435548ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 40.674271ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 40.351027ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 40.295337ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 40.823272ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 40.70842ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 40.322775ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 40.666489ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 40.762462ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 40.847884ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 40.686231ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 40.676258ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 40.383749ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 40.483824ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 40.570632ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 40.959215ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 40.575339ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 40.627628ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 40.829332ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 40.773246ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 40.527447ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 40.599136ms)
Jan 15 22:51:03.252: INFO: (14) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 40.719578ms)
Jan 15 22:51:03.277: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 25.197669ms)
Jan 15 22:51:03.277: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 25.379953ms)
Jan 15 22:51:03.278: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 25.89529ms)
Jan 15 22:51:03.278: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 25.913097ms)
Jan 15 22:51:03.278: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 25.78657ms)
Jan 15 22:51:03.290: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 38.378042ms)
Jan 15 22:51:03.290: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 38.152029ms)
Jan 15 22:51:03.290: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 38.33409ms)
Jan 15 22:51:03.290: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 38.557529ms)
Jan 15 22:51:03.290: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 38.167099ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 38.328466ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 38.690749ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 38.36483ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 38.74102ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 38.375491ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 38.521031ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 38.540297ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 38.530257ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 38.559628ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 38.665256ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 38.512275ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 38.491811ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 38.711122ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 38.639442ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 38.535812ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 38.868668ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 38.65828ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 38.593692ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 38.834965ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 38.818115ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 38.91415ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 38.790186ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 38.805281ms)
Jan 15 22:51:03.291: INFO: (15) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 38.94837ms)
Jan 15 22:51:03.319: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 27.976456ms)
Jan 15 22:51:03.319: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 27.874797ms)
Jan 15 22:51:03.319: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 28.04622ms)
Jan 15 22:51:03.320: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 28.418406ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 40.423425ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 40.559366ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 40.526042ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 40.750846ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 40.860307ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 40.519171ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 40.957804ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 40.644152ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 40.582422ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 40.789276ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 40.635293ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 40.680322ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 40.615242ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 40.567172ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 40.954633ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 40.960061ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 40.623958ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 40.732904ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 40.614598ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 41.101674ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 41.003118ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 40.811332ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 40.872919ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 40.954472ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 40.919607ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 40.702879ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 40.82677ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 40.722337ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 40.849307ms)
Jan 15 22:51:03.332: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 40.76703ms)
Jan 15 22:51:03.361: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 28.606431ms)
Jan 15 22:51:03.361: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 28.609564ms)
Jan 15 22:51:03.361: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 28.599425ms)
Jan 15 22:51:03.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 29.216352ms)
Jan 15 22:51:03.362: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 29.391134ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 41.503863ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 41.489318ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 41.491099ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 41.672421ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 41.375157ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 41.268989ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 41.469253ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 41.540955ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 41.635492ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 41.636952ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 41.423213ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 41.870227ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 41.413795ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 41.743943ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 41.634856ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 41.523756ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 41.495532ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 41.663413ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 41.659106ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 41.501124ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 41.517809ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 41.6456ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 41.675896ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 41.607659ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 41.530712ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 41.773208ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 41.854176ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 41.835946ms)
Jan 15 22:51:03.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 41.917467ms)
Jan 15 22:51:03.397: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 22.104369ms)
Jan 15 22:51:03.397: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 22.91993ms)
Jan 15 22:51:03.398: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 23.204491ms)
Jan 15 22:51:03.398: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 23.403107ms)
Jan 15 22:51:03.398: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 23.601891ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 34.929863ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 34.852544ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 34.930173ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 35.160234ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 34.813518ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 35.210733ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 34.788656ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 35.09739ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 34.905799ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 34.971385ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 35.183735ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 34.91146ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 34.935383ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 35.235171ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 34.850191ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 34.988631ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 35.063977ms)
Jan 15 22:51:03.410: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 34.816913ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 35.678698ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 35.729803ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 35.72419ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 36.198011ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 36.000823ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 36.165083ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 36.249482ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 36.437801ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 36.25577ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 36.045238ms)
Jan 15 22:51:03.411: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 36.085304ms)
Jan 15 22:51:03.437: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/rewri... (200; 25.54325ms)
Jan 15 22:51:03.438: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/proxy/... (200; 26.186457ms)
Jan 15 22:51:03.443: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 31.77945ms)
Jan 15 22:51:03.443: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/: foo (200; 31.992603ms)
Jan 15 22:51:03.446: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/: <a href="/api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:1080/... (200; 34.54744ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:1080/proxy/rewri... (200; 38.735855ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/proxy/: tls baz (200; 39.094584ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/proxy/: foo (200; 39.027889ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/: bar (200; 39.179784ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/: tls qux (200; 39.339535ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:462/proxy/: tls qux (200; 39.251292ms)
Jan 15 22:51:03.450: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/: foo (200; 38.946353ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname2/proxy/: tls qux (200; 39.01869ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:443/: tls baz (200; 39.389746ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:162/: bar (200; 39.155687ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp/proxy/rewriteme"... (200; 39.191889ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:443/proxy/... (200; 39.346703ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:444/: tls qux (200; 38.916151ms)
Jan 15 22:51:03.451: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/https:proxy-service-z5c6r-lwtwp:460/proxy/: tls baz (200; 39.269164ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:80/: foo (200; 44.925522ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.598146ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/: foo (200; 44.432687ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:81/: bar (200; 44.870989ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:80/: foo (200; 44.698367ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname1/proxy/: foo (200; 44.603694ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname1/: foo (200; 44.453026ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/https:proxy-service-z5c6r:tlsportname1/: tls baz (200; 44.648822ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:81/: bar (200; 44.48027ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/: bar (200; 44.656399ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/proxy/: bar (200; 44.527403ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:160/proxy/: foo (200; 44.558743ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/http:proxy-service-z5c6r:portname2/proxy/: bar (200; 44.820674ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/proxy/namespaces/e2e-tests-proxy-z2fnp/pods/http:proxy-service-z5c6r-lwtwp:162/: bar (200; 44.60357ms)
Jan 15 22:51:03.456: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-z2fnp/services/proxy-service-z5c6r:portname2/proxy/: bar (200; 44.954651ms)
STEP: deleting { ReplicationController} proxy-service-z5c6r in namespace e2e-tests-proxy-z2fnp
Jan 15 22:51:03.670: INFO: Deleting { ReplicationController} proxy-service-z5c6r took: 97.829148ms
Jan 15 22:51:03.670: INFO: Terminating { ReplicationController} proxy-service-z5c6r pods took: 24.013µs
Jan 15 22:51:10.870: INFO: Garbage collecting { ReplicationController} proxy-service-z5c6r pods took: 7.298007981s
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:51:10.870: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-z2fnp" for this suite.
Jan 15 22:51:17.891: INFO: namespace: e2e-tests-proxy-z2fnp, resource: bindings, ignored listing per whitelist
Jan 15 22:51:18.292: INFO: namespace e2e-tests-proxy-z2fnp deletion completed in 7.393564608s

• [SLOW TEST:24.859 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:75
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0 [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1531
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:51:18.293: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should support proxy with --port 0 [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1531
STEP: starting the proxy server
Jan 15 22:51:18.400: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:51:18.579: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qjzpc" for this suite.
Jan 15 22:51:25.994: INFO: namespace: e2e-tests-kubectl-qjzpc, resource: bindings, ignored listing per whitelist
Jan 15 22:51:26.009: INFO: namespace e2e-tests-kubectl-qjzpc deletion completed in 7.401972846s

• [SLOW TEST:7.717 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should support proxy with --port 0 [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1531
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:233
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:51:26.009: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Jan 15 22:51:26.104: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 15 22:52:26.259: INFO: Waiting for terminating namespaces to be deleted...
Jan 15 22:52:26.290: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 15 22:52:26.335: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 15 22:52:26.335: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 15 22:52:26.350: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jan 15 22:52:26.350: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-prvd before test
Jan 15 22:52:26.381: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-r6ln before test
Jan 15 22:52:26.415: INFO: registry-console-1-mwx65 from default started at 2018-01-15 22:09:36 +0000 UTC (1 container statuses recorded)
Jan 15 22:52:26.415: INFO: 	Container registry-console ready: true, restart count 0
Jan 15 22:52:26.415: INFO: 
Logging pods the kubelet thinks is on node ci-prtest-cc63063-250-ig-n-v6l4 before test
[It] validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:233
Jan 15 22:52:26.479: INFO: Pod registry-console-1-mwx65 requesting resource cpu=0m on Node ci-prtest-cc63063-250-ig-n-r6ln
Jan 15 22:52:26.479: INFO: Using pod capacity: 500m
Jan 15 22:52:26.479: INFO: Node: ci-prtest-cc63063-250-ig-n-prvd has cpu allocatable: 2000m
Jan 15 22:52:26.479: INFO: Node: ci-prtest-cc63063-250-ig-n-r6ln has cpu allocatable: 2000m
Jan 15 22:52:26.479: INFO: Node: ci-prtest-cc63063-250-ig-n-v6l4 has cpu allocatable: 2000m
STEP: Starting additional 12 Pods to fully saturate the cluster CPU and trying to start another one
Jan 15 22:52:26.695: INFO: Waiting for running...
STEP: Considering event: 
Type = [Normal], Name = [overcommit-0.150a1d941afd3315], Reason = [Scheduled], Message = [Successfully assigned overcommit-0 to ci-prtest-cc63063-250-ig-n-prvd]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-0.150a1d94298fd556], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-0.150a1d945799b3b1], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-0.150a1d945a52aae9], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-0.150a1d94616738c0], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-1.150a1d941bef7117], Reason = [Scheduled], Message = [Successfully assigned overcommit-1 to ci-prtest-cc63063-250-ig-n-v6l4]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-1.150a1d942ee6a5f4], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-1.150a1d94750c81fb], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-1.150a1d9477be9292], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-1.150a1d947f88d2b9], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d94250ad569], Reason = [Scheduled], Message = [Successfully assigned overcommit-10 to ci-prtest-cc63063-250-ig-n-r6ln]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d943246a091], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d9463be12c1], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d94751b4d35], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d9477f3736d], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-10.150a1d9482dfcdad], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-11.150a1d9425f877d3], Reason = [Scheduled], Message = [Successfully assigned overcommit-11 to ci-prtest-cc63063-250-ig-n-v6l4]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-11.150a1d9434ab1c8c], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-11.150a1d94814dfc27], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-11.150a1d9482e93c3a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-11.150a1d948a4ca87c], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d941cec8d9b], Reason = [Scheduled], Message = [Successfully assigned overcommit-2 to ci-prtest-cc63063-250-ig-n-r6ln]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d942c6f0961], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d9452b928f7], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d946d6d7d81], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d946ff12930], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-2.150a1d947bfd1ed1], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-3.150a1d941e04f020], Reason = [Scheduled], Message = [Successfully assigned overcommit-3 to ci-prtest-cc63063-250-ig-n-v6l4]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-3.150a1d942edaf8f6], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-3.150a1d945a98e4a9], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-3.150a1d945d6079e8], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-3.150a1d946434b63e], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-4.150a1d941f18c629], Reason = [Scheduled], Message = [Successfully assigned overcommit-4 to ci-prtest-cc63063-250-ig-n-prvd]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-4.150a1d942b920394], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-4.150a1d9464493454], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-4.150a1d9466f1d1da], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-4.150a1d9471659a32], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-5.150a1d9420136b9f], Reason = [Scheduled], Message = [Successfully assigned overcommit-5 to ci-prtest-cc63063-250-ig-n-r6ln]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-5.150a1d9433451e12], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-5.150a1d9474e29d72], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-5.150a1d94774a6828], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-5.150a1d94807861d7], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-6.150a1d94210e7209], Reason = [Scheduled], Message = [Successfully assigned overcommit-6 to ci-prtest-cc63063-250-ig-n-prvd]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-6.150a1d94309d255e], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-6.150a1d94797cf4be], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-6.150a1d947bb24b6a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-6.150a1d948294092d], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-7.150a1d94220b7065], Reason = [Scheduled], Message = [Successfully assigned overcommit-7 to ci-prtest-cc63063-250-ig-n-v6l4]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-7.150a1d9434b12d28], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-7.150a1d946799c9b2], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-7.150a1d946a502606], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-7.150a1d9472405df5], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d94230cd314], Reason = [Scheduled], Message = [Successfully assigned overcommit-8 to ci-prtest-cc63063-250-ig-n-r6ln]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d943350162a], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d946a8dfe3b], Reason = [Pulling], Message = [pulling image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d9481cf7af4], Reason = [Pulled], Message = [Successfully pulled image "gcr.io/google_containers/pause-amd64:3.0"]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d948404450a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-8.150a1d948aa0a002], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-9.150a1d94241565f8], Reason = [Scheduled], Message = [Successfully assigned overcommit-9 to ci-prtest-cc63063-250-ig-n-prvd]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-9.150a1d942f82d2f8], Reason = [SuccessfulMountVolume], Message = [MountVolume.SetUp succeeded for volume "default-token-8nt9r" ]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-9.150a1d946e2b6fa1], Reason = [Pulled], Message = [Container image "gcr.io/google_containers/pause-amd64:3.0" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-9.150a1d9471cfdb33], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [overcommit-9.150a1d94789afbf5], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.150a1d955512b4fa], Reason = [FailedScheduling], Message = [No nodes are available that match all of the predicates: Insufficient cpu (3), MatchNodeSelector (1), NodeUnschedulable (1).]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:52:32.814: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-cf42h" for this suite.
Jan 15 22:52:56.011: INFO: namespace: e2e-tests-sched-pred-cf42h, resource: bindings, ignored listing per whitelist
Jan 15 22:52:56.235: INFO: namespace e2e-tests-sched-pred-cf42h deletion completed in 23.40575462s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:90.226 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:233
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:103
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:52:56.236: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:103
STEP: Creating a pod to test substitution in container's args
Jan 15 22:52:56.351: INFO: Waiting up to 5m0s for pod "var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-var-expansion-2jvxd" to be "success or failure"
Jan 15 22:52:56.366: INFO: Pod "var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.966903ms
Jan 15 22:52:58.382: INFO: Pod "var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030652636s
Jan 15 22:53:00.398: INFO: Pod "var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046360719s
STEP: Saw pod success
Jan 15 22:53:00.398: INFO: Pod "var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:53:00.413: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 22:53:00.455: INFO: Waiting for pod var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:53:00.470: INFO: Pod var-expansion-d3a044ed-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:53:00.470: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-2jvxd" for this suite.
Jan 15 22:53:07.113: INFO: namespace: e2e-tests-var-expansion-2jvxd, resource: bindings, ignored listing per whitelist
Jan 15 22:53:07.912: INFO: namespace e2e-tests-var-expansion-2jvxd deletion completed in 7.412873353s

• [SLOW TEST:11.676 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should allow substituting values in a container's args [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:103
------------------------------
S
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:83
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:53:07.912: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:83
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 15 22:53:08.040: INFO: Waiting up to 5m0s for pod "pod-da993b5b-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-m9tb5" to be "success or failure"
Jan 15 22:53:08.055: INFO: Pod "pod-da993b5b-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.068178ms
Jan 15 22:53:10.070: INFO: Pod "pod-da993b5b-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030606747s
STEP: Saw pod success
Jan 15 22:53:10.070: INFO: Pod "pod-da993b5b-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:53:10.085: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-da993b5b-fa46-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 22:53:10.131: INFO: Waiting for pod pod-da993b5b-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:53:10.151: INFO: Pod pod-da993b5b-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:53:10.151: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m9tb5" for this suite.
Jan 15 22:53:17.151: INFO: namespace: e2e-tests-emptydir-m9tb5, resource: bindings, ignored listing per whitelist
Jan 15 22:53:17.619: INFO: namespace e2e-tests-emptydir-m9tb5 deletion completed in 7.43992629s

• [SLOW TEST:9.708 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0777,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:83
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:359
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:53:17.619: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:359
Jan 15 22:53:19.854: INFO: Waiting up to 5m0s for pod "client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-pods-4nwsb" to be "success or failure"
Jan 15 22:53:19.885: INFO: Pod "client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 30.404097ms
Jan 15 22:53:21.900: INFO: Pod "client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045936599s
Jan 15 22:53:23.916: INFO: Pod "client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061566595s
STEP: Saw pod success
Jan 15 22:53:23.916: INFO: Pod "client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:53:23.931: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504 container env3cont: <nil>
STEP: delete the pod
Jan 15 22:53:23.972: INFO: Waiting for pod client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:53:23.988: INFO: Pod client-envvars-e1a346e8-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:53:23.988: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4nwsb" for this suite.
Jan 15 22:53:47.153: INFO: namespace: e2e-tests-pods-4nwsb, resource: bindings, ignored listing per whitelist
Jan 15 22:53:47.416: INFO: namespace e2e-tests-pods-4nwsb deletion completed in 23.39936177s

• [SLOW TEST:29.796 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should contain environment variables for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:359
------------------------------
SSS
------------------------------
[k8s.io] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance] [Flaky]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:50
[BeforeEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:53:47.416: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:46
[It] should be submitted and removed [Conformance] [Flaky]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:50
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 15 22:53:49.615: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 15 22:53:54.819: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:53:54.835: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-w6czv" for this suite.
Jan 15 22:54:01.805: INFO: namespace: e2e-tests-pods-w6czv, resource: bindings, ignored listing per whitelist
Jan 15 22:54:02.299: INFO: namespace e2e-tests-pods-w6czv deletion completed in 7.435218427s

• [SLOW TEST:14.883 seconds]
[k8s.io] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should be submitted and removed [Conformance] [Flaky]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/pods.go:50
------------------------------
SSS
------------------------------
[k8s.io] Downward API 
  should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:54:02.299: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
STEP: Creating a pod to test downward api env vars
Jan 15 22:54:02.415: INFO: Waiting up to 5m0s for pod "downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-p29dp" to be "success or failure"
Jan 15 22:54:02.430: INFO: Pod "downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.612731ms
Jan 15 22:54:04.446: INFO: Pod "downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031288185s
Jan 15 22:54:06.462: INFO: Pod "downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047125626s
STEP: Saw pod success
Jan 15 22:54:06.462: INFO: Pod "downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:54:06.477: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 22:54:06.536: INFO: Waiting for pod downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:54:06.551: INFO: Pod downward-api-fb01ca95-fa46-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:54:06.551: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p29dp" for this suite.
Jan 15 22:54:13.708: INFO: namespace: e2e-tests-downward-api-p29dp, resource: bindings, ignored listing per whitelist
Jan 15 22:54:14.042: INFO: namespace e2e-tests-downward-api-p29dp deletion completed in 7.449090151s

• [SLOW TEST:11.743 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide pod name and namespace as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:47
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:54:14.042: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:47
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4424k
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 15 22:54:14.153: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jan 15 22:54:40.515: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.6.41:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4424k PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:54:40.515: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:54:40.724: INFO: Found all expected endpoints: [netserver-0]
Jan 15 22:54:40.739: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.2.45:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4424k PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:54:40.739: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:54:40.939: INFO: Found all expected endpoints: [netserver-1]
Jan 15 22:54:40.954: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -q -s --connect-timeout 1 http://172.16.4.42:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4424k PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:54:40.954: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:54:41.143: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:54:41.143: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4424k" for this suite.
Jan 15 22:55:04.432: INFO: namespace: e2e-tests-pod-network-test-4424k, resource: bindings, ignored listing per whitelist
Jan 15 22:55:04.597: INFO: namespace e2e-tests-pod-network-test-4424k deletion completed in 23.411896125s

• [SLOW TEST:50.555 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:47
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:318
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:55:04.597: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:318
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 15 22:55:07.329: INFO: Successfully updated pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504"
Jan 15 22:55:07.329: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-pods-jsfff" to be "terminated due to deadline exceeded"
Jan 15 22:55:07.344: INFO: Pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504": Phase="Running", Reason="", readiness=true. Elapsed: 15.194008ms
Jan 15 22:55:09.359: INFO: Pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504": Phase="Running", Reason="", readiness=true. Elapsed: 2.030562863s
Jan 15 22:55:11.375: INFO: Pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.046155979s
Jan 15 22:55:11.375: INFO: Pod "pod-update-activedeadlineseconds-20274abb-fa47-11e7-b6d5-0e5e74815504" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:55:11.375: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-jsfff" for this suite.
Jan 15 22:55:18.372: INFO: namespace: e2e-tests-pods-jsfff, resource: bindings, ignored listing per whitelist
Jan 15 22:55:18.802: INFO: namespace e2e-tests-pods-jsfff deletion completed in 7.397259279s

• [SLOW TEST:14.205 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should allow activeDeadlineSeconds to be updated [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:318
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:133
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:55:18.802: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:49
[It] should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:133
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-zbf4d
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-zbf4d to expose endpoints map[]
Jan 15 22:55:18.932: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-zbf4d exposes endpoints map[] (14.616148ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-zbf4d
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-zbf4d to expose endpoints map[pod1:[100]]
Jan 15 22:55:20.043: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-zbf4d exposes endpoints map[pod1:[100]] (1.063083142s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-zbf4d
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-zbf4d to expose endpoints map[pod1:[100] pod2:[101]]
Jan 15 22:55:22.212: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-zbf4d exposes endpoints map[pod1:[100] pod2:[101]] (2.137683056s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-zbf4d
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-zbf4d to expose endpoints map[pod2:[101]]
Jan 15 22:55:22.259: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-zbf4d exposes endpoints map[pod2:[101]] (30.524423ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-zbf4d
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-zbf4d to expose endpoints map[]
Jan 15 22:55:22.293: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-zbf4d exposes endpoints map[] (17.161992ms elapsed)
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:55:22.319: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-zbf4d" for this suite.
Jan 15 22:55:29.549: INFO: namespace: e2e-tests-services-zbf4d, resource: bindings, ignored listing per whitelist
Jan 15 22:55:29.741: INFO: namespace e2e-tests-services-zbf4d deletion completed in 7.393779773s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:54

• [SLOW TEST:10.940 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:133
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:33
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:55:29.741: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:33
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-2xhwb
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 15 22:55:29.872: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jan 15 22:55:52.210: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.49:8080/dial?request=hostName&protocol=http&host=172.16.4.44&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2xhwb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:55:52.210: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:55:52.422: INFO: Waiting for endpoints: map[]
Jan 15 22:55:52.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.49:8080/dial?request=hostName&protocol=http&host=172.16.2.48&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2xhwb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:55:52.437: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:55:52.621: INFO: Waiting for endpoints: map[]
Jan 15 22:55:52.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.49:8080/dial?request=hostName&protocol=http&host=172.16.6.43&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2xhwb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 22:55:52.637: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 22:55:52.821: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:55:52.821: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-2xhwb" for this suite.
Jan 15 22:56:15.589: INFO: namespace: e2e-tests-pod-network-test-2xhwb, resource: bindings, ignored listing per whitelist
Jan 15 22:56:16.264: INFO: namespace e2e-tests-pod-network-test-2xhwb deletion completed in 23.413286974s

• [SLOW TEST:46.522 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:33
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:79
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:56:16.264: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:79
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:57:16.403: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qzqdz" for this suite.
Jan 15 22:57:29.770: INFO: namespace: e2e-tests-container-probe-qzqdz, resource: bindings, ignored listing per whitelist
Jan 15 22:57:29.829: INFO: namespace e2e-tests-container-probe-qzqdz deletion completed in 13.397885301s

• [SLOW TEST:73.566 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  with readiness probe that fails should never be ready and never restart [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:79
------------------------------
SSSS
------------------------------
[k8s.io] Downward API volume 
  should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:183
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:57:29.829: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:183
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:57:29.986: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-2f2rz" to be "success or failure"
Jan 15 22:57:30.001: INFO: Pod "downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.179832ms
Jan 15 22:57:32.017: INFO: Pod "downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030666566s
STEP: Saw pod success
Jan 15 22:57:32.017: INFO: Pod "downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:57:32.032: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:57:32.074: INFO: Waiting for pod downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:57:32.089: INFO: Pod downwardapi-volume-76bade15-fa47-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:57:32.089: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2f2rz" for this suite.
Jan 15 22:57:38.825: INFO: namespace: e2e-tests-downward-api-2f2rz, resource: bindings, ignored listing per whitelist
Jan 15 22:57:39.502: INFO: namespace e2e-tests-downward-api-2f2rz deletion completed in 7.384471426s

• [SLOW TEST:9.672 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:183
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:357
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:57:39.502: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should create and stop a working application [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:357
STEP: creating all guestbook components
Jan 15 22:57:39.612: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend-amd64:v5
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 15 22:57:39.612: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:40.462: INFO: stderr: ""
Jan 15 22:57:40.462: INFO: stdout: "deployment \"frontend\" created\n"
Jan 15 22:57:40.462: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 15 22:57:40.462: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:40.728: INFO: stderr: ""
Jan 15 22:57:40.728: INFO: stdout: "service \"frontend\" created\n"
Jan 15 22:57:40.728: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 15 22:57:40.728: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:40.992: INFO: stderr: ""
Jan 15 22:57:40.992: INFO: stdout: "deployment \"redis-master\" created\n"
Jan 15 22:57:40.992: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 15 22:57:40.992: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:41.260: INFO: stderr: ""
Jan 15 22:57:41.260: INFO: stdout: "service \"redis-master\" created\n"
Jan 15 22:57:41.260: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave-amd64:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 15 22:57:41.260: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:41.520: INFO: stderr: ""
Jan 15 22:57:41.520: INFO: stdout: "deployment \"redis-slave\" created\n"
Jan 15 22:57:41.520: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 15 22:57:41.520: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:57:41.798: INFO: stderr: ""
Jan 15 22:57:41.798: INFO: stdout: "service \"redis-slave\" created\n"
STEP: validating guestbook app
Jan 15 22:57:41.798: INFO: Waiting for all frontend pods to be Running.
Jan 15 22:57:56.799: INFO: Waiting for frontend to serve content.
Jan 15 22:57:59.848: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\StreamCo in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jan 15 22:58:04.890: INFO: Trying to add a new entry to the guestbook.
Jan 15 22:58:04.926: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 15 22:58:04.969: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:08.393: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:08.393: INFO: stdout: "deployment \"frontend\" deleted\n"
STEP: using delete to clean up resources
Jan 15 22:58:08.393: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:08.578: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:08.578: INFO: stdout: "service \"frontend\" deleted\n"
STEP: using delete to clean up resources
Jan 15 22:58:08.578: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:11.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:11.962: INFO: stdout: "deployment \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Jan 15 22:58:11.962: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:12.144: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:12.144: INFO: stdout: "service \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Jan 15 22:58:12.145: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:15.538: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:15.538: INFO: stdout: "deployment \"redis-slave\" deleted\n"
STEP: using delete to clean up resources
Jan 15 22:58:15.538: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x8ggg'
Jan 15 22:58:15.722: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 15 22:58:15.722: INFO: stdout: "service \"redis-slave\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:58:15.722: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x8ggg" for this suite.
Jan 15 22:58:54.748: INFO: namespace: e2e-tests-kubectl-x8ggg, resource: bindings, ignored listing per whitelist
Jan 15 22:58:55.148: INFO: namespace e2e-tests-kubectl-x8ggg deletion completed in 39.395704943s

• [SLOW TEST:75.646 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create and stop a working application [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:357
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:39
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:58:55.148: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:39
STEP: Creating configMap with name configmap-test-volume-a98fd7fa-fa47-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:58:55.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-zwfb4" to be "success or failure"
Jan 15 22:58:55.296: INFO: Pod "pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.033022ms
Jan 15 22:58:57.311: INFO: Pod "pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030786423s
STEP: Saw pod success
Jan 15 22:58:57.312: INFO: Pod "pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:58:57.327: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:58:57.367: INFO: Waiting for pod pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:58:57.382: INFO: Pod pod-configmaps-a9921c05-fa47-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:58:57.382: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zwfb4" for this suite.
Jan 15 22:59:04.384: INFO: namespace: e2e-tests-configmap-zwfb4, resource: bindings, ignored listing per whitelist
Jan 15 22:59:04.835: INFO: namespace e2e-tests-configmap-zwfb4 deletion completed in 7.42358425s

• [SLOW TEST:9.687 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:39
------------------------------
[k8s.io] Projected 
  should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:790
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:04.835: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:790
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:59:05.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-bzkst" to be "success or failure"
Jan 15 22:59:05.033: INFO: Pod "downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 20.304116ms
Jan 15 22:59:07.049: INFO: Pod "downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036373914s
STEP: Saw pod success
Jan 15 22:59:07.049: INFO: Pod "downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:59:07.065: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:59:07.113: INFO: Waiting for pod downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:59:07.128: INFO: Pod downwardapi-volume-af5c79e2-fa47-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:59:07.128: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bzkst" for this suite.
Jan 15 22:59:14.030: INFO: namespace: e2e-tests-projected-bzkst, resource: bindings, ignored listing per whitelist
Jan 15 22:59:14.571: INFO: namespace e2e-tests-projected-bzkst deletion completed in 7.414197546s

• [SLOW TEST:9.737 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should set DefaultMode on files [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:790
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1434
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:14.571: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
[It] should create a pod from an image when restart is Never [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1434
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 22:59:14.725: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-fdvxt'
Jan 15 22:59:15.567: INFO: stderr: ""
Jan 15 22:59:15.567: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1430
Jan 15 22:59:15.583: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-fdvxt'
Jan 15 22:59:15.812: INFO: stderr: ""
Jan 15 22:59:15.812: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:59:15.812: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fdvxt" for this suite.
Jan 15 22:59:22.716: INFO: namespace: e2e-tests-kubectl-fdvxt, resource: bindings, ignored listing per whitelist
Jan 15 22:59:23.245: INFO: namespace e2e-tests-kubectl-fdvxt deletion completed in 7.404013773s

• [SLOW TEST:8.674 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create a pod from an image when restart is Never [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1434
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:148
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:23.245: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:148
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 15 22:59:25.480: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-ba529bf3-fa47-11e7-b6d5-0e5e74815504", GenerateName:"", Namespace:"e2e-tests-pods-kfvpz", SelfLink:"/api/v1/namespaces/e2e-tests-pods-kfvpz/pods/pod-submit-remove-ba529bf3-fa47-11e7-b6d5-0e5e74815504", UID:"ba58998e-fa47-11e7-a248-42010a8e0002", ResourceVersion:"15590", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63651653963, loc:(*time.Location)(0x5d131c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"368759413"}, Annotations:map[string]string{"openshift.io/scc":"privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-n69ps", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc421c89f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"gcr.io/google-containers/nginx-slim-amd64:0.20", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n69ps", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4216624c0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc421b86968), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"role":"app"}, ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ci-prtest-cc63063-250-ig-n-prvd", HostNetwork:false, HostPID:false, HostIPC:false, SecurityContext:(*v1.PodSecurityContext)(0xc421662800), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-t4j8j"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63651653963, loc:(*time.Location)(0x5d131c0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63651653964, loc:(*time.Location)(0x5d131c0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63651653963, loc:(*time.Location)(0x5d131c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.142.0.3", PodIP:"172.16.6.47", StartTime:(*v1.Time)(0xc421a6a980), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc421a6a9a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"gcr.io/google-containers/nginx-slim-amd64:0.20", ImageID:"docker-pullable://gcr.io/google-containers/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b", ContainerID:"docker://4a4bc3a47a64ca18befc865650794e216887e745c8b23331cc3d5db322f27506"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 15 22:59:30.529: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:59:30.545: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kfvpz" for this suite.
Jan 15 22:59:37.627: INFO: namespace: e2e-tests-pods-kfvpz, resource: bindings, ignored listing per whitelist
Jan 15 22:59:37.973: INFO: namespace e2e-tests-pods-kfvpz deletion completed in 7.399322732s

• [SLOW TEST:14.727 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be submitted and removed [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:148
------------------------------
SSS
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:35
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:37.973: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:35
STEP: Creating configMap with name configmap-test-volume-c319f125-fa47-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 22:59:38.132: INFO: Waiting up to 5m0s for pod "pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-cbkz7" to be "success or failure"
Jan 15 22:59:38.147: INFO: Pod "pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.924297ms
Jan 15 22:59:40.162: INFO: Pod "pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030670168s
STEP: Saw pod success
Jan 15 22:59:40.162: INFO: Pod "pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:59:40.178: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 22:59:40.221: INFO: Waiting for pod pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:59:40.236: INFO: Pod pod-configmaps-c31c625b-fa47-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:59:40.236: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cbkz7" for this suite.
Jan 15 22:59:47.264: INFO: namespace: e2e-tests-configmap-cbkz7, resource: bindings, ignored listing per whitelist
Jan 15 22:59:47.646: INFO: namespace e2e-tests-configmap-cbkz7 deletion completed in 7.377202519s

• [SLOW TEST:9.673 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:35
------------------------------
SSS
------------------------------
[k8s.io] Downward API volume 
  should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:165
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:47.646: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:165
STEP: Creating a pod to test downward API volume plugin
Jan 15 22:59:47.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-sfdxc" to be "success or failure"
Jan 15 22:59:47.781: INFO: Pod "downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.236028ms
Jan 15 22:59:49.796: INFO: Pod "downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030509717s
STEP: Saw pod success
Jan 15 22:59:49.796: INFO: Pod "downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 22:59:49.811: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 22:59:49.853: INFO: Waiting for pod downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504 to disappear
Jan 15 22:59:49.868: INFO: Pod downwardapi-volume-c8daab35-fa47-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 22:59:49.868: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sfdxc" for this suite.
Jan 15 22:59:56.858: INFO: namespace: e2e-tests-downward-api-sfdxc, resource: bindings, ignored listing per whitelist
Jan 15 22:59:57.290: INFO: namespace e2e-tests-downward-api-sfdxc deletion completed in 7.394128638s

• [SLOW TEST:9.644 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:165
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:210
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 22:59:57.290: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:210
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-lbpmw
Jan 15 22:59:59.469: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-lbpmw
STEP: checking the pod's current state and verifying that restartCount is present
Jan 15 22:59:59.484: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:02:00.440: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-lbpmw" for this suite.
Jan 15 23:02:07.687: INFO: namespace: e2e-tests-container-probe-lbpmw, resource: bindings, ignored listing per whitelist
Jan 15 23:02:07.870: INFO: namespace e2e-tests-container-probe-lbpmw deletion completed in 7.401573652s

• [SLOW TEST:130.580 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should *not* be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:210
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1322
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:02:07.870: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
[It] should create a deployment from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1322
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 23:02:08.012: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-v8ttb'
Jan 15 23:02:08.940: INFO: stderr: ""
Jan 15 23:02:08.940: INFO: stdout: "deployment \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
Jan 15 23:02:10.972: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-v8ttb'
Jan 15 23:02:14.392: INFO: stderr: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:02:14.392: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v8ttb" for this suite.
Jan 15 23:02:21.697: INFO: namespace: e2e-tests-kubectl-v8ttb, resource: bindings, ignored listing per whitelist
Jan 15 23:02:21.816: INFO: namespace e2e-tests-kubectl-v8ttb deletion completed in 7.393438296s

• [SLOW TEST:13.945 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create a deployment from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1322
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:131
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:02:21.816: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:131
STEP: creating pod
Jan 15 23:02:24.031: INFO: Pod pod-hostip-24c1612e-fa48-11e7-b6d5-0e5e74815504 has hostIP: 10.142.0.4
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:02:24.031: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nltc2" for this suite.
Jan 15 23:02:46.954: INFO: namespace: e2e-tests-pods-nltc2, resource: bindings, ignored listing per whitelist
Jan 15 23:02:47.455: INFO: namespace e2e-tests-pods-nltc2 deletion completed in 23.39539077s

• [SLOW TEST:25.640 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should get a host IP [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:131
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:816
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:02:47.456: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should check if Kubernetes master services is included in cluster-info [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:816
STEP: validating cluster-info
Jan 15 23:02:47.535: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig cluster-info'
Jan 15 23:02:47.699: INFO: stderr: ""
Jan 15 23:02:47.699: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://internal-api.prtest-cc63063-250.origin-ci-int-gce.dev.rhcloud.com:8443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:02:47.699: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d8rqb" for this suite.
Jan 15 23:02:54.619: INFO: namespace: e2e-tests-kubectl-d8rqb, resource: bindings, ignored listing per whitelist
Jan 15 23:02:55.129: INFO: namespace e2e-tests-kubectl-d8rqb deletion completed in 7.40074503s

• [SLOW TEST:7.673 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should check if Kubernetes master services is included in cluster-info [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:816
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:02:55.129: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[BeforeEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
[It] should create an rc from an image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
STEP: running the image gcr.io/google-containers/nginx-slim-amd64:0.20
Jan 15 23:02:55.227: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=gcr.io/google-containers/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-5zp24'
Jan 15 23:02:55.966: INFO: stderr: ""
Jan 15 23:02:55.966: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 15 23:02:55.997: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-mdkcp]
Jan 15 23:02:55.997: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-mdkcp" in namespace "e2e-tests-kubectl-5zp24" to be "running and ready"
Jan 15 23:02:56.013: INFO: Pod "e2e-test-nginx-rc-mdkcp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.814266ms
Jan 15 23:02:58.028: INFO: Pod "e2e-test-nginx-rc-mdkcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.031326517s
Jan 15 23:02:58.028: INFO: Pod "e2e-test-nginx-rc-mdkcp" satisfied condition "running and ready"
Jan 15 23:02:58.028: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-mdkcp]
Jan 15 23:02:58.028: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5zp24'
Jan 15 23:02:58.299: INFO: stderr: ""
[AfterEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
Jan 15 23:02:58.299: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5zp24'
Jan 15 23:02:58.590: INFO: stderr: ""
Jan 15 23:02:58.590: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:02:58.590: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5zp24" for this suite.
Jan 15 23:03:21.615: INFO: namespace: e2e-tests-kubectl-5zp24, resource: bindings, ignored listing per whitelist
Jan 15 23:03:22.017: INFO: namespace e2e-tests-kubectl-5zp24 deletion completed in 23.396819424s

• [SLOW TEST:26.888 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should create an rc from an image [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
------------------------------
SSSSS
------------------------------
[k8s.io] Downward API volume 
  should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:174
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:03:22.017: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:174
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:03:22.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-dnb45" to be "success or failure"
Jan 15 23:03:22.165: INFO: Pod "downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.861943ms
Jan 15 23:03:24.180: INFO: Pod "downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030420102s
STEP: Saw pod success
Jan 15 23:03:24.180: INFO: Pod "downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:03:24.195: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:03:24.246: INFO: Waiting for pod downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:03:24.261: INFO: Pod downwardapi-volume-48a2f819-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:03:24.261: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dnb45" for this suite.
Jan 15 23:03:31.447: INFO: namespace: e2e-tests-downward-api-dnb45, resource: bindings, ignored listing per whitelist
Jan 15 23:03:31.687: INFO: namespace e2e-tests-downward-api-dnb45 deletion completed in 7.396629065s

• [SLOW TEST:9.670 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's cpu request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:174
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:73
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:03:31.687: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:73
STEP: Creating a pod to test substitution in container's command
Jan 15 23:03:31.801: INFO: Waiting up to 5m0s for pod "var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-var-expansion-jgqm2" to be "success or failure"
Jan 15 23:03:31.816: INFO: Pod "var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.828587ms
Jan 15 23:03:33.831: INFO: Pod "var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030587066s
Jan 15 23:03:35.847: INFO: Pod "var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046324247s
STEP: Saw pod success
Jan 15 23:03:35.847: INFO: Pod "var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:03:35.866: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 23:03:35.911: INFO: Waiting for pod var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:03:35.926: INFO: Pod var-expansion-4e6313cf-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:03:35.926: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jgqm2" for this suite.
Jan 15 23:03:42.888: INFO: namespace: e2e-tests-var-expansion-jgqm2, resource: bindings, ignored listing per whitelist
Jan 15 23:03:43.364: INFO: namespace e2e-tests-var-expansion-jgqm2 deletion completed in 7.410133329s

• [SLOW TEST:11.678 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should allow substituting values in a container's command [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:73
------------------------------
[k8s.io] Projected 
  should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:800
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:03:43.365: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:800
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:03:43.485: INFO: Waiting up to 5m0s for pod "downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-l7kpr" to be "success or failure"
Jan 15 23:03:43.500: INFO: Pod "downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.997968ms
Jan 15 23:03:45.515: INFO: Pod "downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030576995s
STEP: Saw pod success
Jan 15 23:03:45.515: INFO: Pod "downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:03:45.530: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:03:45.594: INFO: Waiting for pod downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:03:45.610: INFO: Pod downwardapi-volume-555a050a-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:03:45.610: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l7kpr" for this suite.
Jan 15 23:03:52.365: INFO: namespace: e2e-tests-projected-l7kpr, resource: bindings, ignored listing per whitelist
Jan 15 23:03:53.031: INFO: namespace e2e-tests-projected-l7kpr deletion completed in 7.392547363s

• [SLOW TEST:9.666 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:800
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:73
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:03:53.031: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:49
[It] should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:73
STEP: creating service endpoint-test2 in namespace e2e-tests-services-mdwz7
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-mdwz7 to expose endpoints map[]
Jan 15 23:03:53.249: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mdwz7 exposes endpoints map[] (28.285621ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-mdwz7
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-mdwz7 to expose endpoints map[pod1:[80]]
Jan 15 23:03:54.344: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mdwz7 exposes endpoints map[pod1:[80]] (1.060678341s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-mdwz7
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-mdwz7 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 15 23:03:56.511: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mdwz7 exposes endpoints map[pod1:[80] pod2:[80]] (2.136542456s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-mdwz7
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-mdwz7 to expose endpoints map[pod2:[80]]
Jan 15 23:03:56.559: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mdwz7 exposes endpoints map[pod2:[80]] (32.051703ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-mdwz7
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-mdwz7 to expose endpoints map[]
Jan 15 23:03:56.591: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mdwz7 exposes endpoints map[] (15.222345ms elapsed)
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:03:56.622: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-mdwz7" for this suite.
Jan 15 23:04:19.533: INFO: namespace: e2e-tests-services-mdwz7, resource: bindings, ignored listing per whitelist
Jan 15 23:04:20.033: INFO: namespace e2e-tests-services-mdwz7 deletion completed in 23.381996935s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:54

• [SLOW TEST:27.002 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:73
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:56
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:04:20.033: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:56
STEP: Creating secret with name secret-test-map-6b393d5f-fa48-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:04:20.214: INFO: Waiting up to 5m0s for pod "pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-cjzd7" to be "success or failure"
Jan 15 23:04:20.230: INFO: Pod "pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.45238ms
Jan 15 23:04:22.246: INFO: Pod "pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032046769s
STEP: Saw pod success
Jan 15 23:04:22.246: INFO: Pod "pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:04:22.261: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 23:04:22.302: INFO: Waiting for pod pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:04:22.317: INFO: Pod pod-secrets-6b3ea5d2-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:04:22.317: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-cjzd7" for this suite.
Jan 15 23:04:29.015: INFO: namespace: e2e-tests-secrets-cjzd7, resource: bindings, ignored listing per whitelist
Jan 15 23:04:29.749: INFO: namespace e2e-tests-secrets-cjzd7 deletion completed in 7.403438044s

• [SLOW TEST:9.717 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:56
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:103
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:04:29.750: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:103
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 15 23:04:29.916: INFO: Waiting up to 5m0s for pod "pod-71074c3b-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-j7fdq" to be "success or failure"
Jan 15 23:04:29.931: INFO: Pod "pod-71074c3b-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.023839ms
Jan 15 23:04:31.947: INFO: Pod "pod-71074c3b-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030460866s
STEP: Saw pod success
Jan 15 23:04:31.947: INFO: Pod "pod-71074c3b-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:04:31.962: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-71074c3b-fa48-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 23:04:32.003: INFO: Waiting for pod pod-71074c3b-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:04:32.018: INFO: Pod pod-71074c3b-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:04:32.018: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j7fdq" for this suite.
Jan 15 23:04:39.020: INFO: namespace: e2e-tests-emptydir-j7fdq, resource: bindings, ignored listing per whitelist
Jan 15 23:04:39.442: INFO: namespace e2e-tests-emptydir-j7fdq deletion completed in 7.394653553s

• [SLOW TEST:9.692 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (root,0644,default) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:103
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:54
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:04:39.442: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:54
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-787pz
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 15 23:04:39.539: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jan 15 23:04:57.877: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.6.52 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-787pz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:04:57.877: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:04:59.086: INFO: Found all expected endpoints: [netserver-0]
Jan 15 23:04:59.102: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.4.54 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-787pz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:04:59.102: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:05:00.277: INFO: Found all expected endpoints: [netserver-1]
Jan 15 23:05:00.293: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.2.59 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-787pz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:05:00.293: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:05:01.481: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:05:01.481: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-787pz" for this suite.
Jan 15 23:05:24.111: INFO: namespace: e2e-tests-pod-network-test-787pz, resource: bindings, ignored listing per whitelist
Jan 15 23:05:24.919: INFO: namespace e2e-tests-pod-network-test-787pz deletion completed in 23.409047595s

• [SLOW TEST:45.478 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:54
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:66
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:05:24.919: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:66
Jan 15 23:05:25.101: INFO: (0) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 22.18727ms)
Jan 15 23:05:25.118: INFO: (1) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.180119ms)
Jan 15 23:05:25.134: INFO: (2) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.337941ms)
Jan 15 23:05:25.150: INFO: (3) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.37112ms)
Jan 15 23:05:25.167: INFO: (4) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.905157ms)
Jan 15 23:05:25.184: INFO: (5) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.616977ms)
Jan 15 23:05:25.201: INFO: (6) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.555362ms)
Jan 15 23:05:25.217: INFO: (7) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.570925ms)
Jan 15 23:05:25.234: INFO: (8) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.485478ms)
Jan 15 23:05:25.250: INFO: (9) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.483753ms)
Jan 15 23:05:25.266: INFO: (10) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.018769ms)
Jan 15 23:05:25.282: INFO: (11) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.122294ms)
Jan 15 23:05:25.299: INFO: (12) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.232325ms)
Jan 15 23:05:25.315: INFO: (13) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.070182ms)
Jan 15 23:05:25.331: INFO: (14) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.195696ms)
Jan 15 23:05:25.347: INFO: (15) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 15.991824ms)
Jan 15 23:05:25.363: INFO: (16) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.119625ms)
Jan 15 23:05:25.380: INFO: (17) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.412429ms)
Jan 15 23:05:25.396: INFO: (18) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.317742ms)
Jan 15 23:05:25.412: INFO: (19) /api/v1/proxy/nodes/ci-prtest-cc63063-250-ig-n-prvd/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.32138ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:05:25.412: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-cwdc6" for this suite.
Jan 15 23:05:32.364: INFO: namespace: e2e-tests-proxy-cwdc6, resource: bindings, ignored listing per whitelist
Jan 15 23:05:32.841: INFO: namespace e2e-tests-proxy-cwdc6 deletion completed in 7.399604317s

• [SLOW TEST:7.922 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:66
------------------------------
[k8s.io] ConfigMap 
  should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:330
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:05:32.841: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:330
STEP: Creating configMap e2e-tests-configmap-pp8wm/configmap-test-96a116d4-fa48-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 23:05:33.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-pp8wm" to be "success or failure"
Jan 15 23:05:33.032: INFO: Pod "pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.910421ms
Jan 15 23:05:35.049: INFO: Pod "pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031339122s
Jan 15 23:05:37.064: INFO: Pod "pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046821602s
STEP: Saw pod success
Jan 15 23:05:37.064: INFO: Pod "pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:05:37.080: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504 container env-test: <nil>
STEP: delete the pod
Jan 15 23:05:37.126: INFO: Waiting for pod pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:05:37.141: INFO: Pod pod-configmaps-96a377ef-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:05:37.141: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pp8wm" for this suite.
Jan 15 23:05:44.222: INFO: namespace: e2e-tests-configmap-pp8wm, resource: bindings, ignored listing per whitelist
Jan 15 23:05:44.563: INFO: namespace e2e-tests-configmap-pp8wm deletion completed in 7.392619707s

• [SLOW TEST:11.722 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable via environment variable [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:330
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:153
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:05:44.563: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:153
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-wxcjn
Jan 15 23:05:46.713: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-wxcjn
STEP: checking the pod's current state and verifying that restartCount is present
Jan 15 23:05:46.728: INFO: Initial restart count of pod liveness-http is 0
Jan 15 23:06:06.899: INFO: Restart count of pod e2e-tests-container-probe-wxcjn/liveness-http is now 1 (20.170792643s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:06:06.918: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wxcjn" for this suite.
Jan 15 23:06:14.031: INFO: namespace: e2e-tests-container-probe-wxcjn, resource: bindings, ignored listing per whitelist
Jan 15 23:06:14.339: INFO: namespace e2e-tests-container-probe-wxcjn deletion completed in 7.391782115s

• [SLOW TEST:29.776 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be restarted with a /healthz http liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:153
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:384
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:06:14.339: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:384
STEP: Creating configMap with name projected-configmap-test-volume-af58b4a8-fa48-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 23:06:14.483: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-b5gdh" to be "success or failure"
Jan 15 23:06:14.498: INFO: Pod "pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.907438ms
Jan 15 23:06:16.513: INFO: Pod "pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030209762s
STEP: Saw pod success
Jan 15 23:06:16.513: INFO: Pod "pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:06:16.528: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 23:06:16.570: INFO: Waiting for pod pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:06:16.585: INFO: Pod pod-projected-configmaps-af5b1508-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:06:16.585: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b5gdh" for this suite.
Jan 15 23:06:23.288: INFO: namespace: e2e-tests-projected-b5gdh, resource: bindings, ignored listing per whitelist
Jan 15 23:06:23.995: INFO: namespace e2e-tests-projected-b5gdh deletion completed in 7.381984042s

• [SLOW TEST:9.656 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume as non-root [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:384
------------------------------
SSSSS
------------------------------
[k8s.io] Projected 
  should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:922
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:06:23.995: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:922
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:06:24.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-svsnx" to be "success or failure"
Jan 15 23:06:24.138: INFO: Pod "downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.053529ms
Jan 15 23:06:26.154: INFO: Pod "downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030691563s
STEP: Saw pod success
Jan 15 23:06:26.154: INFO: Pod "downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:06:26.170: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:06:26.223: INFO: Waiting for pod downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:06:26.239: INFO: Pod downwardapi-volume-b519f604-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:06:26.239: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-svsnx" for this suite.
Jan 15 23:06:32.921: INFO: namespace: e2e-tests-projected-svsnx, resource: bindings, ignored listing per whitelist
Jan 15 23:06:33.666: INFO: namespace e2e-tests-projected-svsnx deletion completed in 7.398479577s

• [SLOW TEST:9.671 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's memory request [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:922
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:319
[BeforeEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:06:33.666: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:319
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-jcrmg;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-jcrmg;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-jcrmg.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 7.250.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.250.7_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 7.250.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.250.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-jcrmg;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-jcrmg;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-jcrmg.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-jcrmg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-jcrmg.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 7.250.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.250.7_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 7.250.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.250.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 15 23:06:54.359: INFO: DNS probes using dns-test-bae2cd90-fa48-11e7-b6d5-0e5e74815504 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:06:54.464: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-jcrmg" for this suite.
Jan 15 23:07:01.700: INFO: namespace: e2e-tests-dns-jcrmg, resource: bindings, ignored listing per whitelist
Jan 15 23:07:01.892: INFO: namespace e2e-tests-dns-jcrmg deletion completed in 7.398950714s

• [SLOW TEST:28.226 seconds]
[sig-network] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:319
------------------------------
S
------------------------------
[k8s.io] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:938
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:01.892: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:938
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:07:02.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-htb6m" to be "success or failure"
Jan 15 23:07:02.018: INFO: Pod "downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.741455ms
Jan 15 23:07:04.033: INFO: Pod "downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032420335s
STEP: Saw pod success
Jan 15 23:07:04.033: INFO: Pod "downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:07:04.049: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:07:04.091: INFO: Waiting for pod downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:07:04.106: INFO: Pod downwardapi-volume-cbada840-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:04.106: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-htb6m" for this suite.
Jan 15 23:07:10.831: INFO: namespace: e2e-tests-projected-htb6m, resource: bindings, ignored listing per whitelist
Jan 15 23:07:11.528: INFO: namespace e2e-tests-projected-htb6m deletion completed in 7.393929415s

• [SLOW TEST:9.636 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:938
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:61
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:11.528: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:61
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:07:11.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-6b6cp" to be "success or failure"
Jan 15 23:07:11.670: INFO: Pod "downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.97098ms
Jan 15 23:07:13.685: INFO: Pod "downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030699642s
STEP: Saw pod success
Jan 15 23:07:13.685: INFO: Pod "downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:07:13.701: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:07:13.745: INFO: Waiting for pod downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:07:13.760: INFO: Pod downwardapi-volume-d16eb9c0-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:13.760: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6b6cp" for this suite.
Jan 15 23:07:20.605: INFO: namespace: e2e-tests-downward-api-6b6cp, resource: bindings, ignored listing per whitelist
Jan 15 23:07:21.185: INFO: namespace e2e-tests-downward-api-6b6cp deletion completed in 7.396182175s

• [SLOW TEST:9.656 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should set mode on item file [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:61
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:32
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:21.185: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:32
STEP: Creating a pod to test use defaults
Jan 15 23:07:21.309: INFO: Waiting up to 5m0s for pod "client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-containers-9lkqd" to be "success or failure"
Jan 15 23:07:21.324: INFO: Pod "client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.690896ms
Jan 15 23:07:23.339: INFO: Pod "client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030028537s
STEP: Saw pod success
Jan 15 23:07:23.339: INFO: Pod "client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:07:23.354: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 23:07:23.393: INFO: Waiting for pod client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:07:23.409: INFO: Pod client-containers-d72fce23-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:23.409: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-9lkqd" for this suite.
Jan 15 23:07:30.478: INFO: namespace: e2e-tests-containers-9lkqd, resource: bindings, ignored listing per whitelist
Jan 15 23:07:30.835: INFO: namespace e2e-tests-containers-9lkqd deletion completed in 7.397135201s

• [SLOW TEST:9.650 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should use the image defaults if command and args are blank [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/docker_containers.go:32
------------------------------
SS
------------------------------
[sig-network] Services 
  should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:68
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:30.835: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:49
[It] should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:68
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:30.970: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-2tb9b" for this suite.
Jan 15 23:07:38.345: INFO: namespace: e2e-tests-services-2tb9b, resource: bindings, ignored listing per whitelist
Jan 15 23:07:38.404: INFO: namespace e2e-tests-services-2tb9b deletion completed in 7.40555185s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:54

• [SLOW TEST:7.569 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:68
------------------------------
S
------------------------------
[k8s.io] Projected 
  should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:904
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:38.404: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:904
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:07:38.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-t7rff" to be "success or failure"
Jan 15 23:07:38.533: INFO: Pod "downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.025958ms
Jan 15 23:07:40.549: INFO: Pod "downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03099768s
STEP: Saw pod success
Jan 15 23:07:40.549: INFO: Pod "downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:07:40.564: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:07:40.605: INFO: Waiting for pod downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:07:40.620: INFO: Pod downwardapi-volume-e171884e-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:40.620: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t7rff" for this suite.
Jan 15 23:07:48.012: INFO: namespace: e2e-tests-projected-t7rff, resource: bindings, ignored listing per whitelist
Jan 15 23:07:48.058: INFO: namespace e2e-tests-projected-t7rff deletion completed in 7.40766266s

• [SLOW TEST:9.653 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's memory limit [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:904
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:07:48.058: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:161
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jan 15 23:07:48.716: INFO: Waiting up to 5m0s for pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk" in namespace "e2e-tests-svcaccounts-vft96" to be "success or failure"
Jan 15 23:07:48.731: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999435ms
Jan 15 23:07:50.746: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030719974s
STEP: Saw pod success
Jan 15 23:07:50.746: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk" satisfied condition "success or failure"
Jan 15 23:07:50.762: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk container token-test: <nil>
STEP: delete the pod
Jan 15 23:07:50.805: INFO: Waiting for pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk to disappear
Jan 15 23:07:50.820: INFO: Pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-lsdfk no longer exists
STEP: Creating a pod to test consume service account root CA
Jan 15 23:07:50.838: INFO: Waiting up to 5m0s for pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p" in namespace "e2e-tests-svcaccounts-vft96" to be "success or failure"
Jan 15 23:07:50.852: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p": Phase="Pending", Reason="", readiness=false. Elapsed: 14.605482ms
Jan 15 23:07:52.868: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03013189s
Jan 15 23:07:54.884: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045730217s
STEP: Saw pod success
Jan 15 23:07:54.884: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p" satisfied condition "success or failure"
Jan 15 23:07:54.899: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p container root-ca-test: <nil>
STEP: delete the pod
Jan 15 23:07:54.943: INFO: Waiting for pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p to disappear
Jan 15 23:07:54.958: INFO: Pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-xw75p no longer exists
STEP: Creating a pod to test consume service account namespace
Jan 15 23:07:54.975: INFO: Waiting up to 5m0s for pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8" in namespace "e2e-tests-svcaccounts-vft96" to be "success or failure"
Jan 15 23:07:54.990: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.217368ms
Jan 15 23:07:57.006: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030761505s
STEP: Saw pod success
Jan 15 23:07:57.006: INFO: Pod "pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8" satisfied condition "success or failure"
Jan 15 23:07:57.021: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8 container namespace-test: <nil>
STEP: delete the pod
Jan 15 23:07:57.070: INFO: Waiting for pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8 to disappear
Jan 15 23:07:57.086: INFO: Pod pod-service-account-e7839058-fa48-11e7-b6d5-0e5e74815504-674n8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:07:57.086: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-vft96" for this suite.
Jan 15 23:08:03.833: INFO: namespace: e2e-tests-svcaccounts-vft96, resource: bindings, ignored listing per whitelist
Jan 15 23:08:04.516: INFO: namespace e2e-tests-svcaccounts-vft96 deletion completed in 7.400036554s

• [SLOW TEST:16.459 seconds]
[sig-auth] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:161
------------------------------
S
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:42
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:08:04.516: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:42
STEP: Creating projection with secret that has name projected-secret-test-f10a55e7-fa48-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:08:04.703: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-49jbw" to be "success or failure"
Jan 15 23:08:04.719: INFO: Pod "pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.87445ms
Jan 15 23:08:06.735: INFO: Pod "pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031543317s
STEP: Saw pod success
Jan 15 23:08:06.735: INFO: Pod "pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:08:06.750: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 15 23:08:06.800: INFO: Waiting for pod pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:08:06.815: INFO: Pod pod-projected-secrets-f10cf5bc-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:08:06.815: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-49jbw" for this suite.
Jan 15 23:08:14.131: INFO: namespace: e2e-tests-projected-49jbw, resource: bindings, ignored listing per whitelist
Jan 15 23:08:14.237: INFO: namespace e2e-tests-projected-49jbw deletion completed in 7.393122354s

• [SLOW TEST:9.721 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with defaultMode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:42
------------------------------
SS
------------------------------
[k8s.io] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:61
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:08:14.237: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:61
STEP: Creating configMap with name configmap-test-volume-map-f6cf55bb-fa48-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 23:08:14.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-x87fd" to be "success or failure"
Jan 15 23:08:14.396: INFO: Pod "pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.132627ms
Jan 15 23:08:16.412: INFO: Pod "pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031784494s
STEP: Saw pod success
Jan 15 23:08:16.412: INFO: Pod "pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:08:16.427: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 23:08:16.469: INFO: Waiting for pod pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:08:16.484: INFO: Pod pod-configmaps-f6d1c8a3-fa48-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:08:16.484: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x87fd" for this suite.
Jan 15 23:08:23.772: INFO: namespace: e2e-tests-configmap-x87fd, resource: bindings, ignored listing per whitelist
Jan 15 23:08:23.925: INFO: namespace e2e-tests-configmap-x87fd deletion completed in 7.412353951s

• [SLOW TEST:9.688 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings and Item mode set[Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:61
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:36
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:08:23.925: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:36
Jan 15 23:08:24.046: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:08:24.227: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-94zxf" for this suite.
Jan 15 23:08:31.301: INFO: namespace: e2e-tests-custom-resource-definition-94zxf, resource: bindings, ignored listing per whitelist
Jan 15 23:08:31.661: INFO: namespace e2e-tests-custom-resource-definition-94zxf deletion completed in 7.404421389s

• [SLOW TEST:7.735 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:36
------------------------------
SS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:58
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:08:31.661: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:58
STEP: Creating projection with secret that has name projected-secret-test-map-01301318-fa49-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:08:31.791: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-qxlpr" to be "success or failure"
Jan 15 23:08:31.808: INFO: Pod "pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 16.911223ms
Jan 15 23:08:33.823: INFO: Pod "pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032527494s
STEP: Saw pod success
Jan 15 23:08:33.823: INFO: Pod "pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:08:33.839: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 15 23:08:33.882: INFO: Waiting for pod pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:08:33.898: INFO: Pod pod-projected-secrets-01327772-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:08:33.898: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qxlpr" for this suite.
Jan 15 23:08:40.689: INFO: namespace: e2e-tests-projected-qxlpr, resource: bindings, ignored listing per whitelist
Jan 15 23:08:41.337: INFO: namespace e2e-tests-projected-qxlpr deletion completed in 7.41029459s

• [SLOW TEST:9.676 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings and Item Mode set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:58
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:126
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:08:41.337: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:126
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-h2t5p
Jan 15 23:08:45.530: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-h2t5p
STEP: checking the pod's current state and verifying that restartCount is present
Jan 15 23:08:45.545: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:10:46.505: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-h2t5p" for this suite.
Jan 15 23:10:53.329: INFO: namespace: e2e-tests-container-probe-h2t5p, resource: bindings, ignored listing per whitelist
Jan 15 23:10:53.945: INFO: namespace e2e-tests-container-probe-h2t5p deletion completed in 7.4110211s

• [SLOW TEST:132.608 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:126
------------------------------
SS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:10:53.945: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
STEP: Creating secret with name secret-test-5601ef46-fa49-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:10:54.096: INFO: Waiting up to 5m0s for pod "pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-dfl45" to be "success or failure"
Jan 15 23:10:54.111: INFO: Pod "pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.024685ms
Jan 15 23:10:56.127: INFO: Pod "pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030661542s
STEP: Saw pod success
Jan 15 23:10:56.127: INFO: Pod "pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:10:56.142: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 23:10:56.186: INFO: Waiting for pod pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:10:56.201: INFO: Pod pod-secrets-56047117-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:10:56.201: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dfl45" for this suite.
Jan 15 23:11:03.077: INFO: namespace: e2e-tests-secrets-dfl45, resource: bindings, ignored listing per whitelist
Jan 15 23:11:03.639: INFO: namespace e2e-tests-secrets-dfl45 deletion completed in 7.409332671s

• [SLOW TEST:9.694 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
------------------------------
SSSSSSS
------------------------------
[k8s.io] ConfigMap 
  updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:74
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:11:03.639: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:74
Jan 15 23:11:03.820: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-5bd3188b-fa49-11e7-b6d5-0e5e74815504
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5bd3188b-fa49-11e7-b6d5-0e5e74815504
STEP: waiting to observe update in volume
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:12:28.859: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-chzcj" for this suite.
Jan 15 23:12:51.742: INFO: namespace: e2e-tests-configmap-chzcj, resource: bindings, ignored listing per whitelist
Jan 15 23:12:52.292: INFO: namespace e2e-tests-configmap-chzcj deletion completed in 23.40317764s

• [SLOW TEST:108.652 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  updates should be reflected in volume [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:74
------------------------------
[k8s.io] ConfigMap 
  should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:413
[BeforeEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:12:52.292: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:413
STEP: Creating configMap with name configmap-test-volume-9c8a92ab-fa49-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 23:12:52.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-configmap-7npg8" to be "success or failure"
Jan 15 23:12:52.445: INFO: Pod "pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.002223ms
Jan 15 23:12:54.461: INFO: Pod "pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03078921s
STEP: Saw pod success
Jan 15 23:12:54.461: INFO: Pod "pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:12:54.476: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 23:12:54.520: INFO: Waiting for pod pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:12:54.535: INFO: Pod pod-configmaps-9c8cf3a1-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:12:54.535: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7npg8" for this suite.
Jan 15 23:13:01.805: INFO: namespace: e2e-tests-configmap-7npg8, resource: bindings, ignored listing per whitelist
Jan 15 23:13:01.970: INFO: namespace e2e-tests-configmap-7npg8 deletion completed in 7.406850291s

• [SLOW TEST:9.678 seconds]
[k8s.io] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable in multiple volumes in the same pod [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:413
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:40
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:13:01.970: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:40
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-d4k57
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 15 23:13:02.068: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jan 15 23:13:22.414: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.69:8080/dial?request=hostName&protocol=udp&host=172.16.4.61&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-d4k57 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:13:22.414: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:13:22.631: INFO: Waiting for endpoints: map[]
Jan 15 23:13:22.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.69:8080/dial?request=hostName&protocol=udp&host=172.16.2.68&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-d4k57 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:13:22.646: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:13:22.835: INFO: Waiting for endpoints: map[]
Jan 15 23:13:22.850: INFO: ExecWithOptions {Command:[/bin/sh -c curl -q -s 'http://172.16.2.69:8080/dial?request=hostName&protocol=udp&host=172.16.6.59&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-d4k57 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 15 23:13:22.850: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jan 15 23:13:23.037: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:13:23.037: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-d4k57" for this suite.
Jan 15 23:13:46.338: INFO: namespace: e2e-tests-pod-network-test-d4k57, resource: bindings, ignored listing per whitelist
Jan 15 23:13:46.474: INFO: namespace e2e-tests-pod-network-test-d4k57 deletion completed in 23.407777873s

• [SLOW TEST:44.503 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:40
------------------------------
[k8s.io] Downward API 
  should provide host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:89
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:13:46.474: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:89
STEP: Creating a pod to test downward api env vars
Jan 15 23:13:46.670: INFO: Waiting up to 5m0s for pod "downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-jk748" to be "success or failure"
Jan 15 23:13:46.685: INFO: Pod "downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.046421ms
Jan 15 23:13:48.701: INFO: Pod "downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030634564s
Jan 15 23:13:50.717: INFO: Pod "downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046435795s
STEP: Saw pod success
Jan 15 23:13:50.717: INFO: Pod "downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:13:50.732: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 23:13:50.777: INFO: Waiting for pod downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:13:50.792: INFO: Pod downward-api-bce0c8a0-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:13:50.792: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jk748" for this suite.
Jan 15 23:13:57.545: INFO: namespace: e2e-tests-downward-api-jk748, resource: bindings, ignored listing per whitelist
Jan 15 23:13:58.242: INFO: namespace e2e-tests-downward-api-jk748 deletion completed in 7.419929909s

• [SLOW TEST:11.769 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide host IP as an env var [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:89
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:91
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:13:58.243: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:91
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 15 23:13:58.380: INFO: Waiting up to 5m0s for pod "pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-v5dpf" to be "success or failure"
Jan 15 23:13:58.395: INFO: Pod "pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.096014ms
Jan 15 23:14:00.411: INFO: Pod "pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031122961s
STEP: Saw pod success
Jan 15 23:14:00.411: INFO: Pod "pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:14:00.427: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 23:14:00.468: INFO: Waiting for pod pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:14:00.483: INFO: Pod pod-c3dbd9ef-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:14:00.483: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-v5dpf" for this suite.
Jan 15 23:14:07.752: INFO: namespace: e2e-tests-emptydir-v5dpf, resource: bindings, ignored listing per whitelist
Jan 15 23:14:07.914: INFO: namespace e2e-tests-emptydir-v5dpf deletion completed in 7.401683806s

• [SLOW TEST:9.672 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should support (non-root,0666,tmpfs) [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:91
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:99
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:14:07.914: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:99
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-gg6c4
Jan 15 23:14:12.055: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-gg6c4
STEP: checking the pod's current state and verifying that restartCount is present
Jan 15 23:14:12.070: INFO: Initial restart count of pod liveness-exec is 0
Jan 15 23:15:00.463: INFO: Restart count of pod e2e-tests-container-probe-gg6c4/liveness-exec is now 1 (48.392529826s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:00.483: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gg6c4" for this suite.
Jan 15 23:15:07.347: INFO: namespace: e2e-tests-container-probe-gg6c4, resource: bindings, ignored listing per whitelist
Jan 15 23:15:07.941: INFO: namespace e2e-tests-container-probe-gg6c4 deletion completed in 7.429600357s

• [SLOW TEST:60.027 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:99
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:111
[BeforeEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:15:07.941: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:111
STEP: Creating a pod to test downward api env vars
Jan 15 23:15:08.077: INFO: Waiting up to 5m0s for pod "downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-jb6mb" to be "success or failure"
Jan 15 23:15:08.092: INFO: Pod "downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.217937ms
Jan 15 23:15:10.108: INFO: Pod "downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031130507s
Jan 15 23:15:12.124: INFO: Pod "downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047354602s
STEP: Saw pod success
Jan 15 23:15:12.124: INFO: Pod "downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:15:12.140: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504 container dapi-container: <nil>
STEP: delete the pod
Jan 15 23:15:12.185: INFO: Waiting for pod downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:15:12.201: INFO: Pod downward-api-ed6631d6-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:12.201: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jb6mb" for this suite.
Jan 15 23:15:19.422: INFO: namespace: e2e-tests-downward-api-jb6mb, resource: bindings, ignored listing per whitelist
Jan 15 23:15:19.633: INFO: namespace e2e-tests-downward-api-jb6mb deletion completed in 7.402640513s

• [SLOW TEST:11.691 seconds]
[k8s.io] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:111
------------------------------
SS
------------------------------
[k8s.io] Secrets 
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:371
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:15:19.633: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:371
STEP: creating secret e2e-tests-secrets-m9wbl/secret-test-f45ce70a-fa49-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:15:19.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-m9wbl" to be "success or failure"
Jan 15 23:15:19.788: INFO: Pod "pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.767271ms
Jan 15 23:15:21.803: INFO: Pod "pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030133892s
Jan 15 23:15:23.819: INFO: Pod "pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045799945s
STEP: Saw pod success
Jan 15 23:15:23.819: INFO: Pod "pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:15:23.834: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504 container env-test: <nil>
STEP: delete the pod
Jan 15 23:15:23.879: INFO: Waiting for pod pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:15:23.895: INFO: Pod pod-configmaps-f45f4931-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:23.895: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m9wbl" for this suite.
Jan 15 23:15:31.215: INFO: namespace: e2e-tests-secrets-m9wbl, resource: bindings, ignored listing per whitelist
Jan 15 23:15:31.336: INFO: namespace e2e-tests-secrets-m9wbl deletion completed in 7.412513329s

• [SLOW TEST:11.703 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable via the environment [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:371
------------------------------
S
------------------------------
[k8s.io] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:199
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:15:31.336: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:199
STEP: Creating a pod to test downward API volume plugin
Jan 15 23:15:31.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-downward-api-lzn87" to be "success or failure"
Jan 15 23:15:31.472: INFO: Pod "downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.491578ms
Jan 15 23:15:33.488: INFO: Pod "downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03150375s
STEP: Saw pod success
Jan 15 23:15:33.488: INFO: Pod "downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:15:33.503: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504 container client-container: <nil>
STEP: delete the pod
Jan 15 23:15:33.547: INFO: Waiting for pod downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:15:33.562: INFO: Pod downwardapi-volume-fb565ac2-fa49-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:33.562: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lzn87" for this suite.
Jan 15 23:15:40.852: INFO: namespace: e2e-tests-downward-api-lzn87, resource: bindings, ignored listing per whitelist
Jan 15 23:15:41.002: INFO: namespace e2e-tests-downward-api-lzn87 deletion completed in 7.410836555s

• [SLOW TEST:9.666 seconds]
[k8s.io] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:199
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:833
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:15:41.002: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:283
[It] should check if kubectl describe prints relevant information for rc and pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:833
Jan 15 23:15:41.164: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version --client'
Jan 15 23:15:41.251: INFO: stderr: ""
Jan 15 23:15:41.251: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.7-beta.0.34+b30876a5539f09\", GitCommit:\"b30876a5539f09684ff9fde266fda10b37738c9c\", GitTreeState:\"clean\", BuildDate:\"2018-01-15T22:11:35Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jan 15 23:15:41.265: INFO: Not supported for server versions before "1.8.7-beta.0.34+b30876a5539f09"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:41.266: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rz6vb" for this suite.
Jan 15 23:15:48.014: INFO: namespace: e2e-tests-kubectl-rz6vb, resource: bindings, ignored listing per whitelist
Jan 15 23:15:48.706: INFO: namespace e2e-tests-kubectl-rz6vb deletion completed in 7.411060846s

S [SKIPPING] [7.703 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
    should check if kubectl describe prints relevant information for rc and pods [Conformance] [It]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:833

    Jan 15 23:15:41.265: Not supported for server versions before "1.8.7-beta.0.34+b30876a5539f09"

    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:310
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:327
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:15:48.706: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:327
STEP: Creating secret with name secret-test-05b283ba-fa4a-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:15:48.858: INFO: Waiting up to 5m0s for pod "pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-4ncc2" to be "success or failure"
Jan 15 23:15:48.873: INFO: Pod "pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.048649ms
Jan 15 23:15:50.888: INFO: Pod "pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030564367s
Jan 15 23:15:52.904: INFO: Pod "pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046218009s
STEP: Saw pod success
Jan 15 23:15:52.904: INFO: Pod "pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:15:52.919: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-v6l4 pod pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504 container secret-env-test: <nil>
STEP: delete the pod
Jan 15 23:15:52.960: INFO: Waiting for pod pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:15:52.975: INFO: Pod pod-secrets-05b4ed67-fa4a-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:15:52.976: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4ncc2" for this suite.
Jan 15 23:16:00.113: INFO: namespace: e2e-tests-secrets-4ncc2, resource: bindings, ignored listing per whitelist
Jan 15 23:16:00.415: INFO: namespace e2e-tests-secrets-4ncc2 deletion completed in 7.410685475s

• [SLOW TEST:11.709 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in env vars [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:327
------------------------------
SSS
------------------------------
[k8s.io] EmptyDir volumes 
  volume on tmpfs should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:71
[BeforeEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:16:00.415: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:71
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 15 23:16:00.559: INFO: Waiting up to 5m0s for pod "pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-emptydir-4fwnl" to be "success or failure"
Jan 15 23:16:00.574: INFO: Pod "pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.018308ms
Jan 15 23:16:02.590: INFO: Pod "pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03122848s
STEP: Saw pod success
Jan 15 23:16:02.590: INFO: Pod "pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:16:02.606: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504 container test-container: <nil>
STEP: delete the pod
Jan 15 23:16:02.651: INFO: Waiting for pod pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:16:02.666: INFO: Pod pod-0caefad3-fa4a-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:16:02.666: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4fwnl" for this suite.
Jan 15 23:16:09.582: INFO: namespace: e2e-tests-emptydir-4fwnl, resource: bindings, ignored listing per whitelist
Jan 15 23:16:10.109: INFO: namespace e2e-tests-emptydir-4fwnl deletion completed in 7.414065028s

• [SLOW TEST:9.694 seconds]
[k8s.io] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  volume on tmpfs should have the correct mode [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:71
------------------------------
SSS
------------------------------
[k8s.io] Secrets 
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:52
[BeforeEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:16:10.109: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:52
STEP: Creating secret with name secret-test-map-12725f08-fa4a-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume secrets
Jan 15 23:16:10.247: INFO: Waiting up to 5m0s for pod "pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-secrets-cz4zs" to be "success or failure"
Jan 15 23:16:10.262: INFO: Pod "pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 15.6138ms
Jan 15 23:16:12.278: INFO: Pod "pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031221398s
STEP: Saw pod success
Jan 15 23:16:12.278: INFO: Pod "pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:16:12.293: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-prvd pod pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504 container secret-volume-test: <nil>
STEP: delete the pod
Jan 15 23:16:12.335: INFO: Waiting for pod pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:16:12.350: INFO: Pod pod-secrets-12752301-fa4a-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:16:12.350: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-cz4zs" for this suite.
Jan 15 23:16:19.164: INFO: namespace: e2e-tests-secrets-cz4zs, resource: bindings, ignored listing per whitelist
Jan 15 23:16:19.771: INFO: namespace e2e-tests-secrets-cz4zs deletion completed in 7.391925768s

• [SLOW TEST:9.662 seconds]
[k8s.io] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:52
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/replica_set.go:81
[BeforeEach] [sig-apps] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:16:19.772: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/replica_set.go:81
Jan 15 23:16:19.859: INFO: Creating ReplicaSet my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504
Jan 15 23:16:19.893: INFO: Pod name my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504: Found 1 pods out of 1
Jan 15 23:16:19.893: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504" is running
Jan 15 23:16:21.925: INFO: Pod "my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504-49pt7" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-01-15 23:16:19 +0000 UTC Reason: Message:}])
Jan 15 23:16:21.925: INFO: Trying to dial the pod
Jan 15 23:16:26.988: INFO: Controller my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504: Got expected result from replica 1 [my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504-49pt7]: "my-hostname-basic-1832b993-fa4a-11e7-b6d5-0e5e74815504-49pt7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:16:26.988: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-hcmv4" for this suite.
Jan 15 23:16:34.033: INFO: namespace: e2e-tests-replicaset-hcmv4, resource: bindings, ignored listing per whitelist
Jan 15 23:16:34.418: INFO: namespace e2e-tests-replicaset-hcmv4 deletion completed in 7.401148651s

• [SLOW TEST:14.647 seconds]
[sig-apps] ReplicaSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/replica_set.go:81
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Projected 
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
STEP: Creating a kubernetes client
Jan 15 23:16:34.419: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:777
[It] should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
STEP: Creating configMap with name projected-configmap-test-volume-map-20eec9a8-fa4a-11e7-b6d5-0e5e74815504
STEP: Creating a pod to test consume configMaps
Jan 15 23:16:34.546: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504" in namespace "e2e-tests-projected-x6b7g" to be "success or failure"
Jan 15 23:16:34.560: INFO: Pod "pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504": Phase="Pending", Reason="", readiness=false. Elapsed: 14.642839ms
Jan 15 23:16:36.576: INFO: Pod "pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03006908s
STEP: Saw pod success
Jan 15 23:16:36.576: INFO: Pod "pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504" satisfied condition "success or failure"
Jan 15 23:16:36.591: INFO: Trying to get logs from node ci-prtest-cc63063-250-ig-n-r6ln pod pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 15 23:16:36.633: INFO: Waiting for pod pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504 to disappear
Jan 15 23:16:36.648: INFO: Pod pod-projected-configmaps-20f12a86-fa4a-11e7-b6d5-0e5e74815504 no longer exists
[AfterEach] [k8s.io] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:135
Jan 15 23:16:36.648: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x6b7g" for this suite.
Jan 15 23:16:43.527: INFO: namespace: e2e-tests-projected-x6b7g, resource: bindings, ignored listing per whitelist
Jan 15 23:16:44.080: INFO: namespace e2e-tests-projected-x6b7g deletion completed in 7.403172066s

• [SLOW TEST:9.661 seconds]
[k8s.io] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:635
  should be consumable from pods in volume with mappings [Conformance] [sig-storage]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:392
------------------------------
SSSSSSSSSSSSJan 15 23:16:44.080: INFO: Running AfterSuite actions on all node
Jan 15 23:16:44.080: INFO: Running AfterSuite actions on node 1
Jan 15 23:16:44.080: INFO: Dumping logs locally to: /data/src/github.com/openshift/origin/_output/scripts/init/artifacts
Jan 15 23:16:44.080: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 147 of 699 Specs in 3643.443 seconds
SUCCESS! -- 147 Passed | 0 Failed | 0 Pending | 552 Skipped PASS
